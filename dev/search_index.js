var documenterSearchIndex = {"docs":
[{"location":"Manuals/quadrature_methods/#Quadrature-Methods","page":"Quadrature Methods","title":"Quadrature Methods","text":"A crucial part of transport-map applications is selecting a suitable quadrature method. Quadrature is used in map optimization, in particular when evaluating the Kullback–Leibler divergence\n\nmathcalD_mathrmKLleft(T_ rho  piright)=int Biglog rho(boldsymbolz)-log pi(T(boldsymbola boldsymbolz))-log operatornamedet nabla T(boldsymbola boldsymbolz) Big rho(boldsymbolz)  mathrmd boldsymbolz\n\nHere, boldsymbolz denotes the variable in the reference space with density rho(boldsymbolz), pi(boldsymbolx) is the target density, and T(boldsymbola boldsymbolz) is the transport map parameterized by boldsymbola.\n\nIn practice we approximate the integral by a quadrature sum:\n\nsum_i=1^N w_qiBig-logpibigl(T(boldsymbolaboldsymbolz_qi)bigr)-log detnabla T(boldsymbolaboldsymbolz_qi) Big\n\nThe quadrature points boldsymbolz_qi and weights w_qi must be chosen so the sum approximates expectations with respect to the reference measure rho(boldsymbolz), which is typically the standard Gaussian.\n\nEspecially in Bayesian inference, where evaluating the target density pi(boldsymbolx) can be expensive, using efficient quadrature methods is important to reduce the number of target evaluations [4].","category":"section"},{"location":"Manuals/quadrature_methods/#Monte-Carlo","page":"Quadrature Methods","title":"Monte Carlo","text":"Monte Carlo estimates an expectation Ef(Z) by sampling Z_i sim mathcalN(0 mathbfI) and forming the sample average. It is simple and robust; weights are uniform. Convergence is stochastic (O(n^-12)) but independent of dimension.\n\nWe start by loading the necessary packages:\n\nusing TransportMaps\nusing Plots\n\nusing Random # hide\nRandom.seed!(42) # hide\nmc = MonteCarloWeights(500, 2)\np_mc = scatter(mc.points[:, 1], mc.points[:, 2], ms=3,\n    label=\"MC samples\", title=\"Monte Carlo (500 pts)\", aspect_ratio=1)\nsavefig(\"quadrature_mc.svg\"); nothing # hide\n\n(Image: Monte Carlo samples)","category":"section"},{"location":"Manuals/quadrature_methods/#Latin-Hypercube-Sampling-(LHS)","page":"Quadrature Methods","title":"Latin Hypercube Sampling (LHS)","text":"Latin Hypercube is a stratified sampling design that improves space-filling over plain Monte Carlo; weights remain uniform. It often reduces variance for low to moderate dimensions.\n\nlhs = LatinHypercubeWeights(500, 2)\np_lhs = scatter(lhs.points[:, 1], lhs.points[:, 2], ms=3,\n    label=\"LHS samples\", title=\"Latin Hypercube (500 pts)\", aspect_ratio=1)\nsavefig(\"quadrature_lhs.svg\"); nothing # hide\n\n(Image: Latin Hypercube samples)","category":"section"},{"location":"Manuals/quadrature_methods/#Tensor-product-Gauss–Hermite","page":"Quadrature Methods","title":"Tensor-product Gauss–Hermite","text":"Gauss–Hermite quadrature provides nodes boldsymbolz_q i and weights w_q i such that for smooth integrands the integral with respect to the Gaussian density is approximated very accurately. Tensor-product rules take the 1D rule in each coordinate and form all combinations, producing N=n^d nodes for n points per dimension.\n\nhermite = GaussHermiteWeights(5, 2)\np_hermite = scatter(hermite.points[:, 1], hermite.points[:, 2],\n    ms=4, label=\"Gauss–Hermite\", title=\"Tensor Gauss–Hermite (5 × 5)\", aspect_ratio=1)\nsavefig(\"quadrature_hermite.svg\"); nothing # hide\n\n(Image: Gauss-Hermite tensor product sample)","category":"section"},{"location":"Manuals/quadrature_methods/#Sparse-Smolyak-Gauss–Hermite","page":"Quadrature Methods","title":"Sparse Smolyak Gauss–Hermite","text":"Sparse Smolyak grids combine 1D quadrature rules across dimensions with carefully chosen coefficients to cancel redundant high-order interactions. The result is substantially fewer nodes than the full tensor product while retaining high accuracy for mixed smooth functions. Note that Smolyak quadrature can produce negative weights; this is expected for higher-order correction terms.\n\nsparse = SparseSmolyakWeights(2, 2)\np_sparse = scatter(sparse.points[:, 1], sparse.points[:, 2], ms=6,\n    label=\"Smolyak\", title=\"Sparse Smolyak (level 2)\", aspect_ratio=1)\nsavefig(\"quadrature_smolyak.svg\"); nothing # hide\n\n(Image: Sparse Smolyak sample)","category":"section"},{"location":"Manuals/quadrature_methods/#Quick-usage-notes","page":"Quadrature Methods","title":"Quick usage notes","text":"Use Monte Carlo or LHS for high-dimensional problems where deterministic quadrature is infeasible.\nUse tensor Gauss–Hermite in very low dimensions when the integrand is smooth and high accuracy is required.\nUse Sparse Smolyak for intermediate dimensions (e.g. d leq 6) to get higher accuracy at much lower cost than the full tensor rule.\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"references/#References","page":"References","title":"References","text":"Y. Marzouk, T. Moselhy, M. Parno and A. Spantini. Sampling via Measure Transport: An Introduction. In: Handbook of Uncertainty Quantification, edited by R. Ghanem, D. Higdon and H. Owhadi (Springer International Publishing, Cham, 2016); pp. 1–41.\n\n\n\nM. Ramgraber, D. Sharp, M. L. Provost and Y. Marzouk. A Friendly Introduction to Triangular Transport (Mar 2025), arXiv:2503.21673 [stat].\n\n\n\nR. Baptista, Y. Marzouk and O. Zahm. On the Representation and Learning of Monotone Triangular Transport Maps. Foundations of Computational Mathematics (2023).\n\n\n\nJ. Grashorn, M. Broggi, L. Chamoin and M. Beer. Efficiency Comparison of MCMC and Transport Map Bayesian Posterior Estimation for Structural Health Monitoring. Mechanical Systems and Signal Processing 216, 111440 (2024).\n\n\n\nM. Rosenblatt. Remarks on a multivariate transformation. The annals of mathematical statistics 23, 470–472 (1952).\n\n\n\nH. Knothe. Contributions to the theory of convex bodies. The Michigan Mathematical Journal 4, 39–52 (1957).\n\n\n\nM. Parno, P.-B. Rubio, D. Sharp, M. Brennan, R. Baptista, H. Bonart and Y. Marzouk. MParT: Monotone Parameterization Toolkit. Journal of Open Source Software 7, 4843 (2022).\n\n\n\nB. Zanger, O. Zahm, T. Cui and M. Schreiber. Sequential Transport Maps Using SoS Density Estimation and alpha-Divergences (Oct 2024), arXiv:2402.17943 [stat].\n\n\n\nB. Sudret. Global Sensitivity Analysis Using Polynomial Chaos Expansions. Reliability Engineering & System Safety 93, 964–979 (2008).\n\n\n\nD. Xiu and G. E. Karniadakis. The Wiener–Askey Polynomial Chaos for Stochastic Differential Equations. SIAM Journal on Scientific Computing 24, 619–644 (2002).\n\n\n\nN. Lüthen, S. Marelli and B. Sudret. Sparse Polynomial Chaos Expansions: Literature Survey and Benchmark. SIAM/ASA Journal on Uncertainty Quantification 9, 593–649 (2021).\n\n\n\nA. B. Sullivan, D. M. Snyder and S. A. Rounds. Controls on biochemical oxygen demand in the upper Klamath River, Oregon. Chemical Geology 269, 12–21 (2010).\n\n\n\n","category":"section"},{"location":"Manuals/optimization/#Optimization-of-the-Map-Coefficients","page":"Optimization","title":"Optimization of the Map Coefficients","text":"A crucial step in constructing transport maps is the optimization of the map coefficients, which determine how well the map represents the target distribution. This process can be approached in two distinct ways, depending on the available information about the target distribution [1].","category":"section"},{"location":"Manuals/optimization/#Map-from-density","page":"Optimization","title":"Map-from-density","text":"One way to construct a transport map is to directly optimize its parameters based on the (unnormalized) target density, as shown in Banana: Map from Density. This approach requires access to the target density function and uses quadrature schemes to approximate integrals, as introduced in Quadrature Methods.\n\nFormally, we define the following optimization problem to determine the coefficients boldsymbola of the parameterized map T:\n\nmin_boldsymbola sum_i=1^N w_qiBig-logpibigl(T(boldsymbolaboldsymbolz_qi)bigr)-log detnabla T(boldsymbolaboldsymbolz_qi) Big\n\nAs noted by [1], this optimization problem is generally non-convex. Specifically, it is only convex when the target density pi(boldsymbolx) is log-concave. Especially in Bayesian inference, where the target density represents the posterior density, the function is not log-concave, resulting in a non-convex optimization problem.\n\nIn this package, map optimization is performed with the help of Optim.jl, and support a wide range of optimizers and options (such as convergence criteria and printing preferences). Specifically, we can pass our optimize! function the desired optimizer and options. For a full overview of available options, see the Optim.jl configuration documentation.\n\nTo perform the optimization of the map coefficients, we call:\n\noptimize!(M::PolynomialMap, target_density::Function, quadrature::AbstractQuadratureWeights;\n  optimizer::Optim.AbstractOptimizer = LBFGS(), options::Optim.Options = Optim.Options())\n\nWe have to provide the polynomial map M, the target density function, and a quadrature scheme. Optionally, we can specify the optimizer (default is LBFGS()) and options.\n\nnote: Set initial coefficients\nAs the starting point of the optimization, the map coefficients can be set using setcoefficients!(M, coeffs), where coeffs is a vector of coefficients.","category":"section"},{"location":"Manuals/optimization/#Usage","page":"Optimization","title":"Usage","text":"First we load the packages:\n\nusing TransportMaps\nusing Optim\nusing Distributions\nusing Plots\n\nThen, define the target density and quadrature scheme. Here, we use the same banana-shaped density as in Banana: Map from Density:\n\nbanana_density(x) = logpdf(Normal(), x[1]) + logpdf(Normal(), x[2] - x[1]^2)\ntarget = MapTargetDensity(banana_density, :auto_diff)\nquadrature = GaussHermiteWeights(10, 2)\nnothing #hide\n\nSet optimization options to print the trace every 20 iterations:\n\nopts_trace = Optim.Options(iterations=200, show_trace=true, show_every=20, store_trace=true)\n\nWe will try the following optimizers from Optim.jl, ordered from simplest to most sophisticated:","category":"section"},{"location":"Manuals/optimization/#Gradient-Descent","page":"Optimization","title":"Gradient Descent","text":"The most basic optimization algorithm, Gradient Descent iteratively moves in the direction of the negative gradient. It is simple and robust, but can be slow to converge, especially for ill-conditioned problems.\n\nM_gd = PolynomialMap(2, 2)\nres_gd = optimize!(M_gd, target, quadrature; optimizer=GradientDescent(), options=opts_trace)\nprintln(res_gd)","category":"section"},{"location":"Manuals/optimization/#Conjugate-Gradient","page":"Optimization","title":"Conjugate Gradient","text":"Conjugate Gradient improves upon basic gradient descent by using conjugate directions, which can accelerate convergence for large-scale or quadratic problems. It requires gradient information but not the Hessian.\n\nM_cg = PolynomialMap(2, 2)\nres_cg = optimize!(M_cg, target, quadrature; optimizer=ConjugateGradient(), options=opts_trace)\nprintln(res_cg)","category":"section"},{"location":"Manuals/optimization/#Nelder-Mead","page":"Optimization","title":"Nelder-Mead","text":"Nelder-Mead is a derivative-free optimizer that uses a simplex of points to search for the minimum. It is useful when gradients are unavailable or unreliable, but may be less efficient for high-dimensional or smooth problems.\n\nM_nm = PolynomialMap(2, 2)\nres_nm = optimize!(M_nm, target, quadrature; optimizer=NelderMead(), options=opts_trace)\nprintln(res_nm)","category":"section"},{"location":"Manuals/optimization/#BFGS","page":"Optimization","title":"BFGS","text":"BFGS is a quasi-Newton method that builds up an approximation to the Hessian matrix using gradient evaluations. It is generally faster and more robust than gradient descent and conjugate gradient for smooth problems.\n\nM_bfgs = PolynomialMap(2, 2)\nres_bfgs = optimize!(M_bfgs, target, quadrature; optimizer=BFGS(), options=opts_trace)\nprintln(res_bfgs)","category":"section"},{"location":"Manuals/optimization/#LBFGS","page":"Optimization","title":"LBFGS","text":"LBFGS is a limited-memory version of BFGS, making it suitable for large-scale problems where storing the full Hessian approximation is impractical. It is the default optimizer in many scientific computing packages due to its efficiency and reliability.\n\nM_lbfgs = PolynomialMap(2, 2)\nres_lbfgs = optimize!(M_lbfgs, target, quadrature; optimizer=LBFGS(), options=opts_trace)\nprintln(res_lbfgs)\n\nFinally, we can compare the results by means of variance diagnostic:\n\nsamples_z = randn(1000, 2)\nv_gd = variance_diagnostic(M_gd, target, samples_z)\nv_cg = variance_diagnostic(M_cg, target, samples_z)\nv_nm = variance_diagnostic(M_nm, target, samples_z)\nv_bfgs = variance_diagnostic(M_bfgs, target, samples_z)\nv_lbfgs = variance_diagnostic(M_lbfgs, target, samples_z)\n\nprintln(\"Variance diagnostic GradientDescent:   \", v_gd)\nprintln(\"Variance diagnostic ConjugateGradient: \", v_cg)\nprintln(\"Variance diagnostic NelderMead:        \", v_nm)\nprintln(\"Variance diagnostic BFGS:              \", v_bfgs)\nprintln(\"Variance diagnostic LBFGS:             \", v_lbfgs)\n\nWe can visualize the convergence of all optimizers:\n\nplot([res_gd.trace[i].iteration for i in 1:length(res_gd.trace)], lw=2,\n    [res_gd.trace[i].g_norm for i in 1:length(res_gd.trace)], label=\"GradientDescent\")\nplot!([res_cg.trace[i].iteration for i in 1:length(res_cg.trace)], lw=2,\n    [res_cg.trace[i].g_norm for i in 1:length(res_cg.trace)], label=\"ConjugateGradient\")\nplot!([res_nm.trace[i].iteration for i in 1:length(res_nm.trace)], lw=2,\n    [res_nm.trace[i].g_norm for i in 1:length(res_nm.trace)], label=\"NelderMead\")\nplot!([res_bfgs.trace[i].iteration for i in 1:length(res_bfgs.trace)], lw=2,\n    [res_bfgs.trace[i].g_norm for i in 1:length(res_bfgs.trace)], label=\"BFGS\")\nplot!([res_lbfgs.trace[i].iteration for i in 1:length(res_lbfgs.trace)], lw=2,\n    [res_lbfgs.trace[i].g_norm for i in 1:length(res_lbfgs.trace)], label=\"LBFGS\")\nplot!(xaxis=:log, yaxis=:log, xlabel=\"Iteration\", ylabel=\"Gradient norm\",\n    title=\"Convergence of different optimizers\", xlims=(1, 200),\n    legend=:bottomleft)\nsavefig(\"optimization-conv.svg\"); nothing # hide\n\n(Image: Optimization Convergence)\n\nIt becomes clear, that LBFGS and BFGS are the most efficient optimizers in this case, while Nelder-Mead struggles to keep up.","category":"section"},{"location":"Manuals/optimization/#Map-from-samples","page":"Optimization","title":"Map-from-samples","text":"Another strategy of constructing a transport map is to use samples of the target density, as seen in Banana: Map from Samples. The formulation of transport map estimation in this way has the benefit to transform the problem into a convex optimization problem, when reference density is log-concave [1]. Since we can choose the reference density, we can leverage this property to simplify the optimization process.\n\nWhen the map is constructed from samples, the optimization problem is formulated by minimizing the Kullback-Leibler divergence between the pushforward of the reference density and the empirical distribution of the samples. We denote the transport map by S, which pushes forward the target distribution to the reference distribution. This leads to the following optimization problem:\n\nmin_boldsymbola -frac1M sum_i=1^M log rholeft(S(boldsymbola boldsymbolx_i)right) - log leftdet nabla S(boldsymbola boldsymbolx_i)right\n\nwhere boldsymbolx_i_i=1^M are samples from the target distribution, and rho(cdot) is the density of the reference distribution.\n\nTo perform the optimization, we can use the same optimize! function as before, but now we pass samples instead of a target density and quadrature scheme. Similarly, we can specify the optimizer and options:\n\noptimize!(M::PolynomialMap, samples::AbstractArray{<:Real};\n  optimizer::Optim.AbstractOptimizer = LBFGS(), options::Optim.Options = Optim.Options())\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"Manuals/basis_functions/#Basis-Functions","page":"Basis Functions","title":"Basis Functions","text":"In order to construct a parameterized map, we first need to define a suitable basis.\n\nThis manual describes the different one-dimensional basis families currently implemented in TransportMaps.jl. We focus on variants of the (probabilists') Hermite polynomials and recently proposed edge-controlled versions [3], [2].","category":"section"},{"location":"Manuals/basis_functions/#Probabilistic-Hermite-Basis","page":"Basis Functions","title":"Probabilistic Hermite Basis","text":"The probabilistic Hermite polynomials form an orthonormal basis with respect to the standard normal density\n\nphi(z) = frac1sqrt2 pi expleft(-fracz^22right)\n\nThey satisfy the three-term recurrence operatornameHe_n+1(z)=z operatornameHe_n(z)-n operatornameHe_n-1(z)  with (operatornameHe_0(z)=1) and (operatornameHe_1(z)=z).\n\nOrthonormal polynomial bases such as the Hermite family are useful for several reasons. Orthogonality with respect to a reference measure makes coefficient estimation stable: projections onto the basis are simple inner products and a truncated series gives the best L^2-approximation under that measure. These properties are the same reasons orthogonal polynomials are used in Polynomial Chaos Expansions (PCE) for surrogate modelling and uncertainty quantification (see Sudret [9] for a concise introduction). In the Gaussian case the Hermite polynomials are the natural choice (Wiener–Askey scheme, see [10]).\n\nWe want to visualize the Hermite polynomials, first we load the necessary packages:\n\nusing Distributions\nusing Plots\nusing TransportMaps\n\nThen we construct the basis via HermiteBasis() or HermiteBasis(:none) (:none means no edge control):\n\nbasis = HermiteBasis()\nz = -3:0.01:3\n\np1 = plot(xlabel=\"z\", ylabel=\"Basis function\", title=\"Standard Hermite Basis\")\nfor degree in 0:4\n    plot!(p1, z, map(x -> basisfunction(basis, degree, x), z), label=\"degree $degree\")\nend\nsavefig(\"hermite_basis_standard.svg\"); nothing # hide\n\n(Image: Standard Hermite Basis)\n\nIf we zoom out, we can see that the tails grow quickly for large z:\n\nz = -7:0.1:7\n\np2 = plot(xlabel=\"z\", ylabel=\"Basis function\", title=\"Standard Hermite Basis\")\nfor degree in 0:4\n    plot!(p2, z, map(x -> basisfunction(basis, degree, x), z), label=\"degree $degree\")\nend\nsavefig(\"hermite_basis_standard_zoom.svg\"); nothing # hide\n\n(Image: Standard Hermite Basis)","category":"section"},{"location":"Manuals/basis_functions/#Linearized-Hermite-Basis","page":"Basis Functions","title":"Linearized Hermite Basis","text":"Linearized Hermite polynomials (edge-linearized basis) were introduced in [3] to control growth for large z by replacing the polynomial with a tangent line outside data-dependent bounds z^lz^u:\n\nmathcalH^mathrmLin_j(z)=frac1sqrtZ_alpha_j\nbegincases\nmathrmHe_j(z^l)+mathrmHe_j(z^l)(z-z^l)  z z^l \nmathrmHe_j(z)  z^lle z le z^u \nmathrmHe_j(z^u)+mathrmHe_j(z^u)(z-z^u)  z z^u\nendcases\n\nThe bounds are chosen here as the 0.01 and 0.99 empirical quantiles of (reference) samples. Alternatively, they can be chosen as the quantiles of the respective reference density. The normalization constant Z_alpha_j follows the definition in the paper: Z_alpha_j=alpha_j for jk and Z_alpha_k=(alpha_k+1).\n\nbasis = LinearizedHermiteBasis(Normal(), 4, 1)\nprintln(\"Linearization bounds: \", basis.linearizationbounds)\n\np3 = plot(xlabel=\"z\", ylabel=\"Basis function\", title=\"Linearized Hermite Basis\")\nfor degree in 0:4\n    plot!(p3, z, map(x -> basisfunction(basis, degree, x), z), label=\"degree $degree\")\nend\nsavefig(\"hermite_basis_linearized.svg\"); nothing # hide\n\n(Image: Linearized Hermite Basis)","category":"section"},{"location":"Manuals/basis_functions/#Edge-Controlled-(Weighted)-Hermite-Basis:-Gaussian-Weight","page":"Basis Functions","title":"Edge-Controlled (Weighted) Hermite Basis: Gaussian Weight","text":"Edge control modifies each Hermite polynomial with a decaying weight to reduce growth in the tails [2]. Using a Gaussian weight gives:\n\nmathcalH_j^textGauss(z)=mathrmHe_j(z)expleft(-tfracz^24right)\n\nbasis = GaussianWeightedHermiteBasis()\n\np4 = plot(xlabel=\"z\", ylabel=\"Basis function\", title=\"Gaussian-Weighted Hermite Basis\")\nfor degree in 0:4\n    plot!(p4, z, map(x -> basisfunction(basis, degree, x), z), label=\"degree $degree\")\nend\nsavefig(\"hermite_basis_gaussian.svg\"); nothing # hide\n\n(Image: Gaussian Weighted Hermite Basis)\n\nnote: Note\nIn order to preserve some extrapolation properties, the weights are only applied to polynomials of degree j geq 2, as noted in [2].","category":"section"},{"location":"Manuals/basis_functions/#Edge-Controlled-Hermite-Basis:-Cubic-Spline-Weight","page":"Basis Functions","title":"Edge-Controlled Hermite Basis: Cubic Spline Weight","text":"A cubic spline weight smoothly damps the polynomials outside a radius r, also introduced in [2]. In this implementation, we define r based on the 0.01 and 0.99 quantile values z^l z^u:\n\nmathcalH_j^mathrmCub(z)=operatornameHe_j(z)left(2 u^3-3 u^2+1right)qquad u=minleft(1fraczrright) r=2max(z^lz^u)\n\nbasis = CubicSplineHermiteBasis(Normal())\n\np5 = plot(xlabel=\"z\", ylabel=\"Basis function\", title=\"Cubic Spline Weighted Hermite Basis\")\nfor degree in 0:4\n    plot!(p5, z, map(x -> basisfunction(basis, degree, x), z), label=\"degree $degree\")\nend\nsavefig(\"hermite_basis_cubic.svg\"); nothing # hide\n\n(Image: Cubic Spline Weighted Hermite Basis)","category":"section"},{"location":"Manuals/basis_functions/#Summary","page":"Basis Functions","title":"Summary","text":"We showcased four basis variants:\n\nStandard (orthonormal) Hermite\nLinearized Hermite (piecewise linear tails)\nGaussian-weighted Hermite (exponential damping)\nCubic-spline-weighted Hermite (compact-style smooth damping)\n\nThese can be supplied when constructing polynomial transport maps to tune stability, tail behavior, and sparsity characteristics.\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"Examples/banana_adaptive/#Banana:-Adaptive-Transport-Map-from-Samples","page":"Banana: Adaptive Transport Map from Samples","title":"Banana: Adaptive Transport Map from Samples","text":"This implements the ATM algorithm from [3] to approximate the banana distribution using an adaptive polynomial transport map.\n\nAs an example, we once again consider the banana distribution defined by the density:\n\np(x) = phi(x_1) cdot phi(x_2 - x_1^2)\n\nwhere phi is the standard normal PDF.\n\nusing Random # hide\nRandom.seed!(123) # hide\nnothing # hide\n\nWe start with the necessary packages:\n\nusing TransportMaps\nusing Distributions\nusing LinearAlgebra\nusing Plots","category":"section"},{"location":"Examples/banana_adaptive/#Generating-Target-Samples","page":"Banana: Adaptive Transport Map from Samples","title":"Generating Target Samples","text":"The generation of samples from the banana distribution is done in the same way as in the example Banana: Map from Samples. Here, we use N = 500 samples for the ATM learning.\n\nbanana_density(x) = pdf(Normal(), x[1]) * pdf(Normal(), x[2] - x[1]^2)\n\nnum_samples = 500\n\nfunction generate_banana_samples(n_samples::Int)\n    samples = Matrix{Float64}(undef, n_samples, 2)\n\n    count = 0\n    while count < n_samples\n        x1 = randn() * 2\n        x2 = randn() * 3 + x1^2\n\n        if rand() < banana_density([x1, x2]) / 0.4\n            count += 1\n            samples[count, :] = [x1, x2]\n        end\n    end\n\n    return samples\nend\nnothing #hide\n\nprintln(\"Generating samples from banana distribution...\")\ntarget_samples = generate_banana_samples(num_samples)\nprintln(\"Generated $(size(target_samples, 1)) samples\")","category":"section"},{"location":"Examples/banana_adaptive/#Creating-the-Transport-Map","page":"Banana: Adaptive Transport Map from Samples","title":"Creating the Transport Map","text":"First, create a linear transport map as a starting point. This is also the default behavior of the optimize_adaptive_transportmap function, which standardizes the samples using a linear map based on their marginal mean and standard deviation when no explicit linear map is provided.\n\nL = LinearMap(target_samples)\n\nThen, we perform adaptive transport map learning with k-fold cross-validation. We use the setup as in [3]: We select a Hermite basis with linearization, use the modified Softplus rectifier with parameter beta = 2 and k = 5 folds. We set the maximum number of terms to 3 and 6 for the two components, respectively.\n\nCalling the optimize_adaptive_transportmap function performs the adaptive learning of map terms and returns the learned map, optimization results, selected terms, and selected folds. We can provide the initial linear map L to standardize the samples before learning the ATM, and also specify options, such as the rectifier, basis and options for the optimization process. The function returns the ComposedMap of the linear map and the learned adaptive transport map.\n\nM, results, selected_terms, selected_folds = optimize_adaptive_transportmap(\n    target_samples, [3, 6], 5, L, Softplus(2.))\nnothing # hide\n\nThe adaptive map learning prints out information about the learning process, including the selected terms and the training and test objectives for each fold.\n\nWe see, that in the first component, the ATM selected 2 terms (out of the maximum 3), and in the second component, it selected all 5 out of the maximum 6 terms. The number of selected terms is chosen based on the test objective across the folds.\n\nTo visualized which terms were selected, we can plot the multi-index sets of the learned ATM:\n\nind_atm = getmultiindexsets(M.polynomialmap[2])\n\ndim = scatter(ind_atm[:, 1], ind_atm[:, 2], ms=30, legend=false)\nplot!(xlims=(-0.5, maximum(ind_atm[:, 1]) + 0.5), ylims=(-0.5, maximum(ind_atm[:, 2]) + 0.5),\n    aspect_ratio=1, xlabel=\"Multi-index α₁\", ylabel=\"Multi-index α₂\")\nxticks!(0:maximum(ind_atm[:, 1]))\nyticks!(0:maximum(ind_atm[:, 2]))\nsavefig(\"atm-indices.svg\"); nothing # hide\n\n(Image: Learned Multi-Index) We see that the ATM algorithm selected terms in an L-shape pattern, indicating that lower-order interactions were prioritized.","category":"section"},{"location":"Examples/banana_adaptive/#Testing-the-Map","page":"Banana: Adaptive Transport Map from Samples","title":"Testing the Map","text":"new_samples = generate_banana_samples(1000)\nnorm_samples = randn(1000, 2)\nnothing #hide\n\nmapped_banana_samples = inverse(M, norm_samples)\nnothing #hide","category":"section"},{"location":"Examples/banana_adaptive/#Visualizing-Results","page":"Banana: Adaptive Transport Map from Samples","title":"Visualizing Results","text":"Let's create a scatter plot comparing the original samples with the mapped samples to see how well our transport map learned the distribution:\n\np11 = scatter(new_samples[:, 1], new_samples[:, 2],\n    label=\"Original Samples\", alpha=0.5, color=1,\n    title=\"Original Banana Distribution Samples\",\n    xlabel=\"x₁\", ylabel=\"x₂\")\n\nscatter!(p11, mapped_banana_samples[:, 1], mapped_banana_samples[:, 2],\n    label=\"Mapped Samples\", alpha=0.5, color=2,\n    title=\"Transport Map Generated Samples\",\n    xlabel=\"x₁\", ylabel=\"x₂\")\n\nplot(p11, size=(600, 400))\nsavefig(\"atm-comparison-target.svg\"); nothing # hide\n\n(Image: Sample Comparison)","category":"section"},{"location":"Examples/banana_adaptive/#Density-Comparison","page":"Banana: Adaptive Transport Map from Samples","title":"Density Comparison","text":"Similarly, we can compare the true banana density with the learned density obtained via the pullback of the composed map:\n\nx₁ = range(-3, 3, length=100)\nx₂ = range(-2.5, 4.0, length=100)\n\ntrue_density = [banana_density([x1, x2]) for x2 in x₂, x1 in x₁]\nlearned_density = [pullback(M, [x1, x2]) for x2 in x₂, x1 in x₁]\n\np3 = contour(x₁, x₂, true_density,\n    title=\"True Banana Density\",\n    xlabel=\"x₁\", ylabel=\"x₂\",\n    colormap=:viridis, levels=10)\n\np4 = contour(x₁, x₂, learned_density,\n    title=\"Learned Density (Pullback)\",\n    xlabel=\"x₁\", ylabel=\"x₂\",\n    colormap=:viridis, levels=10)\n\nplot(p3, p4, layout=(1, 2), size=(800, 400))\nsavefig(\"atm-density-comparison.svg\"); nothing # hide\n\n(Image: Density Comparison)","category":"section"},{"location":"Examples/banana_adaptive/#Plot-iterations","page":"Banana: Adaptive Transport Map from Samples","title":"Plot iterations","text":"Additionally, we can visualize the selected terms and optimization objectives over the iterations for the second component of the ATM. First, we retrieve the best fold based on the selected folds during learning:\n\nmap_index = 2  # Choose 2nd component\nbest_fold = selected_folds[map_index]\nres_best = results[map_index][best_fold]\nnothing #hide\n\nThen, we can plot the selected terms at each iteration:\n\nmax_1 = maximum(res_best.terms[end][:, 1])\nmax_2 = maximum(res_best.terms[end][:, 2])\n\np = plot(layout=(2, 3), xlims=(-0.5, max_1 + 0.5), ylims=(-0.5, max_2 + 0.5),\n    aspect_ratio=1, xlabel=\"Multi-index α₁\", ylabel=\"Multi-index α₂\", legend=false,)\n\nfor (i, term) in enumerate(res_best.terms)\n    scatter!(p, term[:, 1], term[:, 2], ms=20, title=\"Iteration $i\", subplot=i)\nend\n\nxticks!(0:max_1)\nyticks!(0:max_2)\nplot!(p, size=(800, 600))\nsavefig(\"iterations.svg\"); nothing # hide\n\n(Image: Iterations)\n\nWe can see that in the last iteration, the interaction term (1 1) was added. However, in the end, this term was not selected in the final map based on the cross-validation. To better understand the selection, we can also plot the training and test objectives over the iterations:\n\nCompare optimization objectives\n\nplot(res_best.train_objectives, label=\"Train Objective\", lw=2, ls=:dash, marker=:o)\nplot!(res_best.test_objectives, label=\"Test Objective\", lw=2, ls=:dash, marker=:o)\nplot!(xlabel=\"Iteration\", ylabel=\"Objective Value\",\n    title=\"Training and Test Objectives over Iterations\")\nplot!(size=(600, 400))\nsavefig(\"objectives.svg\"); nothing # hide\n\n(Image: Objectives)\n\nHere, we see that the test objective increased in the last iteration, indicating overfitting, which is why the last added term was not selected in the final adaptive transport map.\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"api/bases/#Bases","page":"Bases","title":"Bases","text":"","category":"section"},{"location":"api/bases/#Index","page":"Bases","title":"Index","text":"Pages = [\"bases.md\"]","category":"section"},{"location":"api/bases/#Types","page":"Bases","title":"Types","text":"","category":"section"},{"location":"api/bases/#Functions","page":"Bases","title":"Functions","text":"","category":"section"},{"location":"api/bases/#TransportMaps.HermiteBasis","page":"Bases","title":"TransportMaps.HermiteBasis","text":"HermiteBasis\n\nProbabilist Hermite polynomial basis.\n\n\n\n\n\n","category":"type"},{"location":"api/bases/#TransportMaps.LinearizedHermiteBasis","page":"Bases","title":"TransportMaps.LinearizedHermiteBasis","text":"LinearizedHermiteBasis\n\nProbabilist Hermite polynomial basis with linearization outside specified bounds.\n\nFields\n\nlinearizationbounds::Vector{Float64}: lower and upper bounds for linearization.\nnormalization::Vector{Float64}: normalization factors for each degree.\n\nConstructors\n\nLinearizedHermiteBasis(; lower=-Inf, upper=Inf, normalization=Float64[]): Explicit constructor with specified bounds and normalization.\nLinearizedHermiteBasis(max_degree::Int): Construct with default normalization for given maximum degree.\nLinearizedHermiteBasis(samples::Vector{<:Real}, max_degree::Int, k::Int): Construct bounds from 1st and 99th percentile of samples.\nLinearizedHermiteBasis(density::Distributions.UnivariateDistribution, max_degree::Int, k::Int): Construct bounds from 1st and 99th percentile of reference density.\n\n\n\n\n\n","category":"type"},{"location":"api/bases/#TransportMaps.CubicSplineHermiteBasis","page":"Bases","title":"TransportMaps.CubicSplineHermiteBasis","text":"CubicSplineHermiteBasis\n\nProbabilist Hermite polynomial basis with cubic spline edge control.\n\nFields\n\nradius::Float64: radius of the spline for edge control.\n\nConstructors\n\nCubicSplineHermiteBasis(radius::Float64=3.0): Explicit constructor with radius=3.0\nCubicSplineHermiteBasis(samples::Vector{<:Real}): Construct radius from from 1st and 99th percentile of samples.\nCubicSplineHermiteBasis(density::Distributions.UnivariateDistribution): Construct radius from 1st and 99th percentile of reference density.\n\n\n\n\n\n","category":"type"},{"location":"api/bases/#TransportMaps.GaussianWeightedHermiteBasis","page":"Bases","title":"TransportMaps.GaussianWeightedHermiteBasis","text":"GaussianWeightedHermiteBasis\n\nProbabilist Hermite polynomial basis with Gaussian weight for edge control.\n\n\n\n\n\n","category":"type"},{"location":"api/bases/#TransportMaps.MultivariateBasis","page":"Bases","title":"TransportMaps.MultivariateBasis","text":"MultivariateBasis{T<:AbstractPolynomialBasis}\n\nA multivariate polynomial basis function constructed as a tensor product of univariate polynomial bases.\n\nFields\n\nmultiindexset::Vector{Int}: Multi-index representing the polynomial degrees for each dimension\nunivariatebases::Vector{T}: Vector of univariate basis functions, one for each dimension\n\nConstructors\n\nMultivariateBasis(multiindexset::Vector{Int}, ::Type{T}) where T<:AbstractPolynomialBasis: Create a multivariate basis with the specified multi-index and basis type.\nMultivariateBasis(multiindexset::Vector{Int}, basistype::AbstractPolynomialBasis): Constructor that accepts a basis instance instead of a type.\n\n\n\n\n\n","category":"type"},{"location":"api/bases/#TransportMaps.basisfunction","page":"Bases","title":"TransportMaps.basisfunction","text":"basisfunction(basis::HermiteBasis, αᵢ::Real, zᵢ::Real)\n\nEvaluate HermiteBasis with degree αᵢ at zᵢ.\n\n\n\n\n\nbasisfunction(basis::LinearizedHermiteBasis, αᵢ::Real, zᵢ::Real)\n\nEvaluate LinearizedHermiteBasis with degree αᵢ at zᵢ.\n\n\n\n\n\nbasisfunction(basis::CubicSplineHermiteBasis, αᵢ::Real, zᵢ::Real)\n\nEvaluate CubicSplineHermiteBasis with degree αᵢ at zᵢ.\n\n\n\n\n\nbasisfunction(basis::GaussianWeightedHermiteBasis, αᵢ::Real, zᵢ::Real)\n\nEvaluate GaussianWeightedHermiteBasis with degree αᵢ at zᵢ.\n\n\n\n\n\n","category":"function"},{"location":"api/bases/#TransportMaps.basisfunction_derivative","page":"Bases","title":"TransportMaps.basisfunction_derivative","text":"basisfunction_derivative(basis::HermiteBasis, αᵢ::Real, zᵢ::Real)\n\nEvaluate derivative of HermiteBasis with degree αᵢ at zᵢ.\n\n\n\n\n\nbasisfunction_derivative(basis::LinearizedHermiteBasis, αᵢ::Real, zᵢ::Real)\n\nEvaluate derivative of LinearizedHermiteBasis with degree αᵢ at zᵢ.\n\n\n\n\n\nbasisfunction_derivative(basis::CubicSplineHermiteBasis, αᵢ::Real, zᵢ::Real)\n\nEvaluate derivative of CubicSplineHermiteBasis with degree αᵢ at zᵢ.\n\n\n\n\n\nbasisfunction_derivative(basis::GaussianWeightedHermiteBasis, αᵢ::Real, zᵢ::Real)\n\nEvaluate derivative of GaussianWeightedHermiteBasis with degree αᵢ at zᵢ.\n\n\n\n\n\n","category":"function"},{"location":"Examples/bod_bayesianinference/#Bayesian-Inference:-Biochemical-Oxygen-Demand-(BOD)-Example","page":"Bayesian Inference: BOD","title":"Bayesian Inference: Biochemical Oxygen Demand (BOD) Example","text":"This example demonstrates Bayesian parameter estimation for a biochemical oxygen demand model using transport maps. The problem comes from environmental engineering and was originally presented in [12] and later used as a benchmark in transport map applications [1].\n\nThe model describes the evolution of biochemical oxygen demand (BOD) in a river system using an exponential growth model with two uncertain parameters controlling growth and decay rates.\n\nusing TransportMaps\nusing Plots\nusing Distributions","category":"section"},{"location":"Examples/bod_bayesianinference/#The-Forward-Model","page":"Bayesian Inference: BOD","title":"The Forward Model","text":"The BOD model is given by:\n\nmathcalB(t) = A(1-exp(-Bt))+ varepsilon\n\nwhere the parameters A and B unknown material parameters and varepsilon sim mathcalN(0 10^-3) represents measurement noise.\n\nThe parameters A and B follow the prior distributions\n\nA sim mathcalU(04 12) quad B sim mathcalU(001 031)\n\nIn order to describe the input parameters in an unbounded space, we transform them into a space with standard normal prior distributions p(theta_i) sim mathcalN(0 1). We write A and B as functions of the new variables theta_1 and theta_2 with the help of a density transformation with the standard normal CDF Phi(theta_i):\n\nbeginaligned\nA = 04 + (12 - 04) cdot Phi(theta_1) \nB = 001 + (031 - 001) cdot Phi(theta_2)\nendaligned\n\nusing Random # hide\nRandom.seed!(123) # hide\nnothing # hide\n\nWe define the forward model as a function of theta and t in Julia:\n\nfunction forward_model(t, θ)\n    A = 0.4 + (1.2 - 0.4) * cdf(Normal(), θ[1])\n    B = 0.01 + (0.31 - 0.01) * cdf(Normal(), θ[2])\n    return A * (1 - exp(-B * t))\nend\nnothing #hide","category":"section"},{"location":"Examples/bod_bayesianinference/#Experimental-Data","page":"Bayesian Inference: BOD","title":"Experimental Data","text":"We have BOD measurements at five time points:\n\nt = [1, 2, 3, 4, 5]\nD = [0.18, 0.32, 0.42, 0.49, 0.54]\nσ = sqrt(1e-3)\nnothing #hide\n\nLet's visualize the data along with model predictions for different parameter values:\n\ns = scatter(t, D, label=\"Data\", xlabel=\"Time (t)\", ylabel=\"Biochemical Oxygen Demand (D)\",\n    size=(600, 400), legend=:topleft)\n# Plot model output for some parameter values\nt_values = range(0, 5, length=100)\nfor θ₁ in [-0.5, 0, 0.5]\n    for θ₂ in [-0.5, 0, 0.5]\n        plot!(t_values, [forward_model(ti, [θ₁, θ₂]) for ti in t_values],\n            label=\"(θ₁ = $θ₁, θ₂ = $θ₂)\", linestyle=:dash)\n    end\nend\nsavefig(\"realizations-bod.svg\"); nothing # hide\n\n(Image: BOD Realizations)","category":"section"},{"location":"Examples/bod_bayesianinference/#Bayesian-Inference-Setup","page":"Bayesian Inference: BOD","title":"Bayesian Inference Setup","text":"We define the posterior distribution using a standard normal prior on both parameters and a Gaussian likelihood for the observations:\n\npi(mathbfyboldsymboltheta) = prod_t=1^5 frac1sqrt2pisigma^2expleft(-frac12sigma^2(y_t - mathcalB(t))^2right)\n\nfunction posterior(θ)\n    # Calculate the likelihood\n    likelihood = prod([pdf(Normal(forward_model(t[k], θ), σ), D[k]) for k in 1:5])\n    # Calculate the prior\n    prior = pdf(Normal(), θ[1]) * pdf(Normal(), θ[2])\n    return prior * likelihood\nend\n\ntarget = MapTargetDensity(x -> log(posterior(x)), :auto_diff)","category":"section"},{"location":"Examples/bod_bayesianinference/#Creating-and-Optimizing-the-Transport-Map","page":"Bayesian Inference: BOD","title":"Creating and Optimizing the Transport Map","text":"We use a 2-dimensional polynomial transport map with degree 3 and Softplus rectifier:\n\nM = PolynomialMap(2, 3, :normal, Softplus(), LinearizedHermiteBasis())\n\nSet up Gauss-Hermite quadrature for optimization:\n\nquadrature = GaussHermiteWeights(10, 2)\n\nOptimize the map coefficients:\n\nres = optimize!(M, target, quadrature)\nprintln(\"Optimization result: \", res)","category":"section"},{"location":"Examples/bod_bayesianinference/#Generating-Posterior-Samples","page":"Bayesian Inference: BOD","title":"Generating Posterior Samples","text":"Generate samples from the standard normal distribution and map them to the posterior:\n\nsamples_z = randn(1000, 2)\n\nMap the samples through our transport map:\n\nmapped_samples = evaluate(M, samples_z)","category":"section"},{"location":"Examples/bod_bayesianinference/#Quality-Assessment","page":"Bayesian Inference: BOD","title":"Quality Assessment","text":"Compute the variance diagnostic to assess the quality of our approximation:\n\nvar_diag = variance_diagnostic(M, target, samples_z)\nprintln(\"Variance Diagnostic: \", var_diag)","category":"section"},{"location":"Examples/bod_bayesianinference/#Visualization","page":"Bayesian Inference: BOD","title":"Visualization","text":"Plot the mapped samples along with contours of the true posterior density:\n\nθ₁ = range(-0.5, 1.5, length=100)\nθ₂ = range(-0.5, 3, length=100)\n\nposterior_values = [posterior([θ₁, θ₂]) for θ₂ in θ₂, θ₁ in θ₁]\n\nscatter(mapped_samples[:, 1], mapped_samples[:, 2],\n    label=\"Mapped Samples\", alpha=0.5, color=1,\n    xlabel=\"θ₁\", ylabel=\"θ₂\", title=\"Posterior Density and Mapped Samples\")\ncontour!(θ₁, θ₂, posterior_values, colormap=:viridis, label=\"Posterior Density\")\nsavefig(\"samples-bod.svg\"); nothing # hide\n\n(Image: BOD Samples)\n\nFinally, we can compute the pullback density to visualize how well our transport map approximates the posterior:\n\nposterior_pullback = [pullback(M, [θ₁, θ₂]) for θ₂ in θ₂, θ₁ in θ₁]\n\ncontour(θ₁, θ₂, posterior_values ./ maximum(posterior_values);\n    levels=5, colormap=:viridis, colorbar=false,\n    label=\"Target\", xlabel=\"θ₁\", ylabel=\"θ₂\")\ncontour!(θ₁, θ₂, posterior_pullback ./ maximum(posterior_pullback);\n    levels=5, colormap=:viridis, linestyle=:dash,\n    label=\"Pullback\")\nsavefig(\"pullback-bod.svg\"); nothing # hide\n\n(Image: BOD Pullback Density)\n\nWe can also visually observe a good agreement between the true posterior and the TM approximation.","category":"section"},{"location":"Examples/bod_bayesianinference/#Conditional-Density-and-Samples","page":"Bayesian Inference: BOD","title":"Conditional Density and Samples","text":"We can compute conditional densities and generate conditional samples using the transport map. Therefore, we make use of the factorization of the structure of triangular maps given by the Knothe-Rosenblatt rearrangement [1], [2]:\n\npi(mathrmx)=underbracepileft(x_1right)_T_1^-1left(z_1right) underbracepileft(x_2 mid x_1right)_T_2^-1left(z_2  x_1right) underbracepileft(x_3 mid x_1 x_2right)_T_3^-1left(z_3  x_1 x_2right) cdots underbracepileft(x_K mid x_1 ldots x_K-1right)_T_K^-1left(z_K  x_1 ldots x_K-1right)\n\nThis allows us to compute the conditional density pi(theta_2  theta_1) and generate samples from this conditional distribution efficiently. We only need to invert the second component of the map to obtain theta_2 given a fixed value of theta_1 and then push forward samples from the reference distribution.\n\nWe can use the conditional_sample function to generate samples from the conditional distribution. Therefore, we samples from the standard normal distribution for z_2 and push them through the conditional map. We use the previously generated samples for z_2 and fix theta_1.\n\nθ₁ = 0.\nconditional_samples = conditional_sample(M, θ₁, randn(10_000))\nnothing #hide\n\nThen, we compute the conditional density of theta_2 given theta_1 first analytically by integrating out theta_1 from the joint posterior. We use numerical integration for this purpose and evaluate the conditional density on a grid.\n\nθ_range = 0:0.01:2\nint_analytical = gaussquadrature(ξ -> posterior([θ₁, ξ]), 1000, -10., 10.)\nposterior_conditional(θ₂) = posterior([θ₁, θ₂]) / int_analytical\nconditional_analytical = posterior_conditional.(θ_range)\nnothing #hide\n\nThen we compute the conditional density using the transport map, which is given as:\n\npi(theta_2  theta_1) = rho_2left(T^2(theta_1 theta_2)^-1right) leftfracpartial T^2(theta_1 theta_2)^-1partial theta_2right\n\nconditional_mapped = conditional_density(M, θ_range, θ₁)\nnothing #hide\n\nFinally, we plot the results:\n\nhistogram(conditional_samples, bins=50, normalize=:pdf, α=0.5,\n    label=\"Conditional Samples\", xlabel=\"θ₂\", ylabel=\"π(θ₂ | θ₁=$θ₁)\")\nplot!(θ_range, conditional_analytical, lw=2, label=\"Analytical Conditional PDF\")\nplot!(θ_range, conditional_mapped, lw=2, label=\"TM Conditional PDF\")\nsavefig(\"conditional-bod.svg\"); nothing # hide\n\n(Image: BOD Conditional Density)\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"Examples/banana_mapfromsamples/#Banana:-Map-from-Samples","page":"Banana: Map from Samples","title":"Banana: Map from Samples","text":"This example demonstrates how to use TransportMaps.jl to approximate a \"banana\" distribution using polynomial transport maps when only samples from the target distribution are available.\n\nUnlike the density-based approach, this method learns the transport map directly from sample data using optimization techniques. This is particularly useful when the target density is unknown or difficult to evaluate [1].\n\nWe start with the necessary packages:\n\nusing TransportMaps\nusing Distributions\nusing LinearAlgebra\nusing Plots\n\nusing Random # hide\nRandom.seed!(123) # hide\nnothing # hide","category":"section"},{"location":"Examples/banana_mapfromsamples/#Generating-Target-Samples","page":"Banana: Map from Samples","title":"Generating Target Samples","text":"The banana distribution has the density:\n\np(x) = phi(x_1) cdot phi(x_2 - x_1^2)\n\nwhere phi is the standard normal PDF.\n\nbanana_density(x) = pdf(Normal(), x[1]) * pdf(Normal(), x[2] - x[1]^2)\n\nSet up the log-target function for sampling:\n\nnum_samples = 1000\nnothing #hide\n\nGenerate samples using rejection sampling (no external dependencies)\n\nfunction generate_banana_samples(n_samples::Int)\n    samples = Matrix{Float64}(undef, n_samples, 2)\n\n    count = 0\n    while count < n_samples\n        x1 = randn() * 2\n        x2 = randn() * 3 + x1^2\n\n        if rand() < banana_density([x1, x2]) / 0.4\n            count += 1\n            samples[count, :] = [x1, x2]\n        end\n    end\n\n    return samples\nend\n\nprintln(\"Generating samples from banana distribution...\")\ntarget_samples = generate_banana_samples(num_samples)\nprintln(\"Generated $(size(target_samples, 1)) samples\")","category":"section"},{"location":"Examples/banana_mapfromsamples/#Creating-the-Transport-Map","page":"Banana: Map from Samples","title":"Creating the Transport Map","text":"First, we create a linear map to standardize the samples:\n\nL = LinearMap(target_samples)\n\nWe create a 2-dimensional polynomial transport map with degree 2. For sample-based optimization, we typically start with lower degrees and can increase complexity as needed.\n\nM = PolynomialMap(2, 2, :normal, Softplus())","category":"section"},{"location":"Examples/banana_mapfromsamples/#Optimizing-from-Samples","page":"Banana: Map from Samples","title":"Optimizing from Samples","text":"The key difference from density-based optimization is that we optimize directly from the sample data without requiring the density function. Inside the optimization the map is arranged s.t. the \"forward\" direction is from the (unknown) target distribution to the standard normal distribution. Also, we give the linear map to standardize the samples before optimization.\n\nres = optimize!(M, target_samples, L)\nnothing # hide\n\nWe can check the optimization results of the first component:\n\nres.optimization_results[1]\n\nWe can also check the optimization results of the second component:\n\nres.optimization_results[2]\n\nFinally, we construct a composed map that combines the linear and polynomial maps:\n\nC = ComposedMap(L, M)","category":"section"},{"location":"Examples/banana_mapfromsamples/#Testing-the-Map","page":"Banana: Map from Samples","title":"Testing the Map","text":"Let's generate new samples from the banana density and standard normal samples and map them through our optimized transport map to verify the learned distribution:\n\nnew_samples = generate_banana_samples(1000)\nnorm_samples = randn(1000, 2)\nnothing #hide\n\nMap the samples through our transport map. Note that evaluate now transports from reference to target, i.e. mapped_samples should be standard normal samples:\n\nmapped_samples = evaluate(C, new_samples)\nnothing #hide\n\nwhile pushing from the standard normal samples to the target distribution generates new samples from the banana distribution:\n\nmapped_banana_samples = inverse(C, norm_samples)\nnothing #hide","category":"section"},{"location":"Examples/banana_mapfromsamples/#Visualizing-Results","page":"Banana: Map from Samples","title":"Visualizing Results","text":"Let's create a scatter plot comparing the original samples with the mapped samples to see how well our transport map learned the distribution:\n\np11 = scatter(new_samples[:, 1], new_samples[:, 2],\n    label=\"Original Samples\", alpha=0.5, color=1,\n    title=\"Original Banana Distribution Samples\",\n    xlabel=\"x₁\", ylabel=\"x₂\")\n\nscatter!(p11, mapped_banana_samples[:, 1], mapped_banana_samples[:, 2],\n    label=\"Mapped Samples\", alpha=0.5, color=2,\n    title=\"Transport Map Generated Samples\",\n    xlabel=\"x₁\", ylabel=\"x₂\")\n\nplot(p11, size=(600, 400))\nsavefig(\"samples-comparison-target.svg\"); nothing # hide\n\n(Image: Sample Comparison)\n\nand the resulting samples in standard normal space:\n\np12 = scatter(norm_samples[:, 1], norm_samples[:, 2],\n    label=\"Original Samples\", alpha=0.5, color=1,\n    title=\"Original Banana Distribution Samples\",\n    xlabel=\"x₁\", ylabel=\"x₂\")\n\nscatter!(p12, mapped_samples[:, 1], mapped_samples[:, 2],\n    label=\"Mapped Samples\", alpha=0.5, color=2,\n    title=\"Transport Map Generated Samples\",\n    xlabel=\"x₁\", ylabel=\"x₂\")\n\nplot(p12, size=(600, 400), aspect_ratio=1)\nsavefig(\"samples-comparison-reference.svg\"); nothing # hide\n\n(Image: Sample Comparison)","category":"section"},{"location":"Examples/banana_mapfromsamples/#Density-Comparison","page":"Banana: Map from Samples","title":"Density Comparison","text":"We can also compare the learned density (via pullback) with the true density:\n\nx₁ = range(-3, 3, length=100)\nx₂ = range(-2.5, 4.0, length=100)\n\nTrue banana density values:\n\ntrue_density = [banana_density([x1, x2]) for x2 in x₂, x1 in x₁]\nnothing # hide\n\nLearned density via pullback through the transport map. Note that \"pullback\" computes the density of the mapped samples in the standard normal space:\n\nlearned_density = [pullback(C, [x1, x2]) for x2 in x₂, x1 in x₁]\nnothing # hide\n\nCreate contour plots for comparison:\n\np3 = contour(x₁, x₂, true_density,\n    title=\"True Banana Density\",\n    xlabel=\"x₁\", ylabel=\"x₂\",\n    colormap=:viridis, levels=10)\n\np4 = contour(x₁, x₂, learned_density,\n    title=\"Learned Density (Pullback)\",\n    xlabel=\"x₁\", ylabel=\"x₂\",\n    colormap=:viridis, levels=10)\n\nplot(p3, p4, layout=(1, 2), size=(800, 400))\nsavefig(\"density-comparison.svg\"); nothing # hide\n\n(Image: Density Comparison)","category":"section"},{"location":"Examples/banana_mapfromsamples/#Combined-Visualization","page":"Banana: Map from Samples","title":"Combined Visualization","text":"Finally, let's create a combined plot showing both the original samples and the density contours:\n\nscatter(target_samples[:, 1], target_samples[:, 2],\n    label=\"Original Samples\", alpha=0.3, color=1,\n    xlabel=\"x₁\", ylabel=\"x₂\",\n    title=\"Banana Distribution: Samples and Learned Density\")\n\ncontour!(x₁, x₂, learned_density ./ maximum(learned_density),\n    levels=5, colormap=:viridis, alpha=0.8,\n    label=\"Learned Density Contours\")\n\nxlims!(-3, 3)\nylims!(-2.5, 4.0)\nsavefig(\"combined-result.svg\"); nothing # hide\n\n(Image: Combined Result)","category":"section"},{"location":"Examples/banana_mapfromsamples/#Quality-Assessment","page":"Banana: Map from Samples","title":"Quality Assessment","text":"We can assess the quality of our sample-based approximation by comparing statistics of the original and mapped samples:\n\nprintln(\"Sample Statistics Comparison:\")\nprintln(\"Original samples - Mean: \", Distributions.mean(target_samples, dims=1))\nprintln(\"Original samples - Std:  \", Distributions.std(target_samples, dims=1))\nprintln(\"Mapped samples - Mean:   \", Distributions.mean(mapped_banana_samples, dims=1))\nprintln(\"Mapped samples - Std:    \", Distributions.std(mapped_banana_samples, dims=1))","category":"section"},{"location":"Examples/banana_mapfromsamples/#Interpretation","page":"Banana: Map from Samples","title":"Interpretation","text":"The sample-based approach learns the transport map by fitting to the empirical distribution of the samples. This method is particularly useful when:\n\nThe target density is unknown or expensive to evaluate\nOnly sample data is available from experiments or simulations\nThe distribution is complex and difficult to express analytically\n\nThe quality of the approximation depends on:\n\nThe number and quality of the original samples\nThe polynomial degree of the transport map\nThe optimization algorithm and convergence criteria","category":"section"},{"location":"Examples/banana_mapfromsamples/#Further-Experiments","page":"Banana: Map from Samples","title":"Further Experiments","text":"You can experiment with:\n\nDifferent polynomial degrees for more complex distributions\nDifferent rectifier functions (Softplus(), ShiftedELU())\nMore sophisticated MCMC sampling strategies\nCross-validation techniques to assess generalization\nDifferent sample sizes to study convergence behavior\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"api/optimization/#Optimization","page":"Optimization","title":"Optimization","text":"","category":"section"},{"location":"api/optimization/#Index","page":"Optimization","title":"Index","text":"Pages = [\"optimization.md\"]","category":"section"},{"location":"api/optimization/#Types","page":"Optimization","title":"Types","text":"","category":"section"},{"location":"api/optimization/#Functions","page":"Optimization","title":"Functions","text":"","category":"section"},{"location":"api/optimization/#TransportMaps.OptimizationHistory","page":"Optimization","title":"TransportMaps.OptimizationHistory","text":"OptimizationHistory\n\nA data structure to store the iteration history of adaptive transport map optimization from samples.\n\nFields\n\nterms::Vector{Matrix{Int64}}: Multi-index sets at each iteration\ntrain_objectives::Vector{Float64}: Normalized training objectives at each iteration\ntest_objectives::Vector{Float64}: Normalized test objectives at each iteration (empty if no test split)\ngradients::Vector{Vector{Float64}}: Gradient values for candidate terms at each iteration\n\n\n\n\n\n","category":"type"},{"location":"api/optimization/#TransportMaps.OptimizationResult","page":"Optimization","title":"TransportMaps.OptimizationResult","text":"OptimizationResult\n\nA data structure to store the optimization results for each component of a transport map when optimizing the map from samples.\n\nFields\n\ntrain_objectives::Vector{Float64}: Train objective for each component\ntest_objectives::Vector{Float64}: Test objective for each component\noptimization_results::Vector{Optim.MultivariateOptimizationResults}: Result from optimization with Optim.jl\n\n\n\n\n\n","category":"type"},{"location":"api/optimization/#TransportMaps.MapOptimizationResult","page":"Optimization","title":"TransportMaps.MapOptimizationResult","text":"MapOptimizationResult\n\nA data struct to store the iteration history of adaptive transport map optimization from density.\n\nFields\n\nmaps::Vector{PolynomialMap}: Maps for each iteration\ntrain_objectives::Vector{Float64}: Train objectives for each iteration\ntest_objectives::Vector{Float64}: Test objectives for each iteration\ngradients::Vector{Vector{Float64}}: Gradients of train objectives\noptimization_results::Vector{Optim.MultivariateOptimizationResults}: Results of optimization with Optim.jl\n\n\n\n\n\n","category":"type"},{"location":"api/optimization/#TransportMaps.optimize!","page":"Optimization","title":"TransportMaps.optimize!","text":"optimize!(M::PolynomialMap, target::AbstractMapDensity, quadrature::AbstractQuadratureWeights;\n          optimizer, options)\n\nOptimize polynomial map coefficients to minimize KL divergence to a target density.\n\nArguments\n\nM::PolynomialMap: The polynomial map to optimize.\ntarget::AbstractMapDensity: Target map density object (provides the target density π(x) and any needed operations).\nquadrature::AbstractQuadratureWeights: Quadrature points and weights used for numerical integration.\n\nKeyword Arguments\n\noptimizer: Optimizer from Optim.jl to use (default: LBFGS()).\noptions: Options passed to the optimizer (default: Optim.Options()).\n\nReturns\n\nOptimization result from Optim.jl. The optimized coefficients are written back into M.\n\n\n\n\n\noptimize!(M::PolynomialMap, samples::Matrix{Float64}, lm::AbstractLinearMap=LinearMap();\n          optimizer::Optim.AbstractOptimizer = LBFGS(),\n          options::Optim.Options = Optim.Options(),\n          test_fraction::Float64 = 0.0)\n\nOptimize polynomial map coefficients to minimize KL divergence to a target density.\n\nArguments\n\nM::PolynomialMap: The polynomial map to optimize.\nsamples::Matrix{Float64}: A matrix of sample data used to initialize and fit the map. Columns are interpreted as components/dimensions and rows as individual sample points.\nlm::AbstractLinearMap: A linear map used to standardize the samples before optimization (default: LinearMap(), identity map).\n\nKeyword Arguments\n\noptimizer::Optim.AbstractOptimizer: Optimizer from Optim.jl to use (default: LBFGS()).\noptions::Optim.Options: Options passed to the optimizer (default: Optim.Options()).\ntest_fraction::Float64: Fraction of samples to hold out for testing/validation (default: 0.0, i.e. no test split).\n\nReturns\n\nOptimizationResult: Optimization results containing training and test objectives for each component. The optimized coefficients are written back into M.\n\n\n\n\n\n","category":"function"},{"location":"api/optimization/#TransportMaps.optimize_adaptive_transportmap","page":"Optimization","title":"TransportMaps.optimize_adaptive_transportmap","text":"optimize_adaptive_transportmap(samples, maxterms; kwargs...)\n\nAdaptively optimize a triangular transport map by greedily enriching the multi-index set.\n\nArguments\n\nsamples::Matrix{Float64}: Sample data where rows are samples and columns are dimensions\nmaxterms::Vector{Int64}: Maximum number of terms for each component\n\nKeyword Arguments\n\nlm::AbstractLinearMap=LinearMap(samples): Linear map to standardize samples\nrectifier::AbstractRectifierFunction=Softplus(): Rectifier function to use\nbasis::AbstractPolynomialBasis=LinearizedHermiteBasis(): Polynomial basis\noptimizer::Optim.AbstractOptimizer=LBFGS(): Optimization algorithm\noptions::Optim.Options=Optim.Options(): Optimizer options\ntest_fraction::Float64=0.0: Fraction of samples to use for testing (validation)\n\nReturns\n\nC::ComposedMap{LinearMap, PolynomialMap}: The optimized triangular transport map\niteration_histories::Vector{OptimizationHistory}: History of optimization for each component\n\n\n\n\n\noptimize_adaptive_transportmap(samples, maxterms, k_folds, lm, rectifier, basis; optimizer, options)\n\nAdaptively optimize a triangular transport map using k-fold cross-validation to select the number of terms per component.\n\nArguments\n\nsamples::Matrix{Float64}: Sample data where rows are samples and columns are dimensions\nmaxterms::Vector{Int64}: Upper bound on the number of terms to consider for each component\nk_folds::Int: Number of folds to use for cross-validation\nlm::AbstractLinearMap: Linear map to standardize samples (default: LinearMap(samples))\nrectifier::AbstractRectifierFunction: Rectifier function to use (default: Softplus())\nbasis::AbstractPolynomialBasis: Polynomial basis (default: LinearizedHermiteBasis())\n\nKeyword Arguments\n\noptimizer::Optim.AbstractOptimizer=LBFGS(): Optimization algorithm\noptions::Optim.Options=Optim.Options(): Optimizer options\n\nReturns\n\nComposedMap{LinearMap, PolynomialMap}: The optimized triangular transport map trained on all samples\nfold_histories::Vector{Vector{OptimizationHistory}}: Optimization history for each component and fold\nselected_terms::Vector{Int}: Number of terms selected for each component\nselected_fold::Vector{Int}: Index of the best fold for each component\n\n\n\n\n\noptimize_adaptive_transportmap(\n    target::AbstractMapDensity,\n    quadrature::AbstractQuadratureWeights,\n    maxterms::Int;\n    kwargs...\n)\n\nAdaptively optimize a triangular transport map from a target density by greedily enriching the multi-index set across all components simultaneously.\n\nArguments\n\ntarget::AbstractMapDensity: Target density to approximate\nquadrature::AbstractQuadratureWeights: Quadrature points and weights for integration\nmaxterms::Int: Maximum total number of terms to add across all components\n\nKeyword Arguments\n\nrectifier::AbstractRectifierFunction=Softplus(): Rectifier function to use\nbasis::AbstractPolynomialBasis=LinearizedHermiteBasis(): Polynomial basis\noptimizer::Optim.AbstractOptimizer=LBFGS(): Optimization algorithm\noptions::Optim.Options=Optim.Options(): Optimizer options\nvalidation_samples::Matrix{Float64}=Matrix{Float64}(undef,0,0): Samples for variance diagnostic validation\n\nReturns\n\nM::PolynomialMap: The optimized triangular transport map (with best validation variance diagnostic)\nhistory::OptimizationHistory: History of optimization iterations\n\n\n\n\n\n","category":"function"},{"location":"api/optimization/#TransportMaps.variance_diagnostic","page":"Optimization","title":"TransportMaps.variance_diagnostic","text":"variance_diagnostic(M::PolynomialMap, target::MapTargetDensity, Z::AbstractArray{<:Real})\n\nCompute a variance-based diagnostic for assessing the quality of a transport map.\n\nThe diagnostic measures the variance of the log-ratio between the pushforward density and the reference density. A smaller variance indicates a better approximation of the target density by the transport map.\n\nArguments\n\nM::PolynomialMap: The transport map to be evaluated\ntarget::MapTargetDensity: The target density that the map should approximate\nZ::AbstractArray{<:Real}: Sample points from the reference distribution, where each row is a sample and columns correspond to dimensions\n\nReturns\n\nFloat64: The computed variance diagnostic\n\n\n\n\n\n","category":"function"},{"location":"Manuals/map_parameterization/#Choosing-a-Map-Parameterization","page":"Map Parameterization","title":"Choosing a Map Parameterization","text":"A crucial aspect of constructing transport maps is the choice of map parameterization. In addition to the choice of the univariate basis functions, the construction of the multi-index set that defines the multi-dimensional polynomial expansion for each component can significantly influence the performance of the map.\n\nThe k-th component of a triangular transport map is defined as T^k mathbbR^k to mathbbR [3]\n\nT^k(z_1 z_2dotsz_k boldsymbola) = f(z_1 z_2dotsz_k-10boldsymbola) + int_0^z_k g(partial_k f(z_1 z_2dotsz_k-1xiboldsymbola))  mathrmd xi\n\nHere, g is a monotone increasing function that ensures the monotonicity of the map, which is a necessary property for transport maps. A common choice for g is the softplus function, defined as g(x) = log(1 + e^x).\n\nFurther, f is a multivariate polynomial function parameterized by coefficients boldsymbola. The polynomial function f can be expressed as a linear combination of multivariate basis functions:\n\nf(z_1 z_2dotsz_k boldsymbola) = sum_alpha in mathcalA_k a_alpha Psi_alpha(z_1 z_2dotsz_k)\n\nHere, Psi_alpha(z_1 z_2dotsz_k) = prod_i=1^k psi_alpha_i(z_i) are multivariate basis functions constructed from univariate basis functions psi_alpha_i(z_i), and mathcalA_k is the multi-index set that determines which polynomial terms are included in the expansion.\n\nBy selecting an appropriate multi-index set mathcalA_k, the structure of the map itself can be tailored to the problem at hand. This includes the ability to create sparse maps, which can be particularly beneficial in high-dimensional settings or when the underlying relationships are known to be simpler. This feature is similar to the concept of sparse polynomial chaos expansions used in (forward) uncertainty propagation [11].\n\nIn TransportMaps.jl, three types of map structures outlined in [1] are implemented:\n\nTotal Order Map: Includes all terms up to a specified total degree p for d dimensions. In order to maintain the triangular structure, the k-th component of the map only depends on the first k variables. Thus, the multivariate basis for the k-th component is constructed from the first k univariate bases. The multi-index set alpha satisfies:\n\nmathcalA_k^TO = boldsymbolalpha boldsymbolalpha_1  leq p  wedge  alpha_i = 0  forall  i  k\n\nNo-Mixed Terms Map: Excludes mixed terms, retaining only pure terms for each variable. The multi-index set alpha satisfies:\n\nmathcalA_k^NM = boldsymbolalpha boldsymbolalpha_1  leq p  wedge  alpha_i alpha_j = 0   forall  i neq j  wedge  alpha_i = 0   forall  i  k\n\nDiagonal Map: Includes only diagonal terms, where each variable is independent of the others. The multi-index set alpha for the k-th component satisfies:\n\nmathcalA_k^D = boldsymbolalpha boldsymbolalpha_1  leq p  wedge  alpha_i = 0  forall  i neq k\n\nFor an better overview, the multi-index sets for each map type will be visualized later.","category":"section"},{"location":"Manuals/map_parameterization/#Constructing-Maps-with-Different-Parameterizations","page":"Map Parameterization","title":"Constructing Maps with Different Parameterizations","text":"We will now demonstrate how to construct and compare the three types of map parameterizations using TransportMaps.jl. We start by importing the necessary packages:\n\nusing TransportMaps\nusing Plots\nusing Distributions\n\nThen we define the three types of (sparse) maps. We consider a two-dimensional map with polynomial degree p=3, softplus as the rectifier function, and Hermite polynomials as the univariate basis functions.","category":"section"},{"location":"Manuals/map_parameterization/#Total-Order-Map","page":"Map Parameterization","title":"Total Order Map","text":"M_to = PolynomialMap(2, 3, :normal, Softplus(), HermiteBasis())\n# alternative: PolynomialMap(2, 3, :normal, Softplus(), HermiteBasis(), :total)","category":"section"},{"location":"Manuals/map_parameterization/#No-mixed-terms-Map","page":"Map Parameterization","title":"No-mixed-terms Map","text":"We can use the NoMixedMap constructor for this map type:\n\nM_nm = NoMixedMap(2, 3, :normal, Softplus(), HermiteBasis())\n# alternative: PolynomialMap(2, 3, :normal, Softplus(), HermiteBasis(), :no_mixed)","category":"section"},{"location":"Manuals/map_parameterization/#Diagonal-Map","page":"Map Parameterization","title":"Diagonal Map","text":"For the diagonal map, we can use the DiagonalMap constructor:\n\nM_d = DiagonalMap(2, 3, :normal, Softplus(), HermiteBasis())\n# alternative: PolynomialMap(2, 3, :normal, Softplus(), HermiteBasis(), :diagonal)","category":"section"},{"location":"Manuals/map_parameterization/#Visualizing-Multi-Index-Sets","page":"Map Parameterization","title":"Visualizing Multi-Index Sets","text":"The multi-index set determines the terms included in the polynomial expansion. We extract the multi-index sets of the second component of each map for visualization:\n\nExtract the multi-index sets of the second component of each map for visualization:\n\nind_to = getmultiindexsets(M_to.components[2])\nind_nm = getmultiindexsets(M_nm.components[2])\nind_d = getmultiindexsets(M_d.components[2])\nnothing #hide\n\nFinally, we plot the multi-index sets for each map type. This allows us to reproduce the comparison of multi-index sets as shown in Figure 1 in [1]:\n\nscatter(ind_to[:, 1], ind_to[:, 2], ms=20, label=\"Total Order\",)\nscatter!(ind_nm[:, 1], ind_nm[:, 2], ms=12, label=\"No Mixed\")\nscatter!(ind_d[:, 1], ind_d[:, 2], ms=6, label=\"Diagonal\")\nscatter!(xlim=(-0.5, 3.5), ylim=(-0.5, 3.5), aspect_ratio=1, legend=:topright,\n    xlabel=\"Multi-index α₁\", ylabel=\"Multi-index α₂\",\n    title=\"Multi-index Set of Map Component M²\")\nsavefig(\"multi_indices.svg\"); nothing # hide\n\n(Image: Multi Index Sets)","category":"section"},{"location":"Manuals/map_parameterization/#Example:-Comparing-Map-Parameterizations","page":"Map Parameterization","title":"Example: Comparing Map Parameterizations","text":"We optimize the coefficients of each map to match a cubic target density. The target density is defined as:\n\np(x) = phi(x_1) cdot phi(x_2 - x_1^3 - x_1^2)\n\nwhere phi is the standard normal PDF.\n\nDefine the cubic target density and visualize it:\n\ncubic_density(x) = logpdf(Normal(), x[1]) + logpdf(Normal(), x[2] - x[1]^3 - x[1]^2)\n\nx₁ = range(-3, 3, length=1000)\nx₂ = range(-10, 20, length=1000)\n\ntrue_density = [exp(cubic_density([x1, x2])) for x2 in x₂, x1 in x₁]\ncontour(x₁, x₂, true_density;\n    label=\"x₁\", ylabel=\"x₂\", colormap=:viridis, levels=10)\n\nWe create the MapTargetDensity and quadrature weights for optimization:\n\ntarget = MapTargetDensity(cubic_density, :auto_diff)\nquadrature = SparseSmolyakWeights(3, 2)\nnothing #hide\n\nGenerate samples in the standard normal space boldsymbolZ for the variance diagnostic:\n\nsamples_z = randn(1000, 2)\nnothing #hide\n\nNow, we optimize each map type, compute the variance diagnostic and visualize the mapped samples.","category":"section"},{"location":"Manuals/map_parameterization/#Total-Order-Map-2","page":"Map Parameterization","title":"Total Order Map","text":"We start with the total order map:\n\noptimize!(M_to, target, quadrature)\n\nmapped_samples = evaluate(M_to, samples_z)\nvar_diag = variance_diagnostic(M_to, target, samples_z)\nprintln(\"Variance Diagnostic: \", var_diag)\n\nscatter(mapped_samples[:, 1], mapped_samples[:, 2],\n    ms=4, label=nothing, c=1, title=\"Total Order\",\n    xlabel=\"x₁\", ylabel=\"x₂\")\nsavefig(\"total_order.svg\"); nothing # hide\n\n(Image: Total Order)","category":"section"},{"location":"Manuals/map_parameterization/#No-Mixed-Terms-Map","page":"Map Parameterization","title":"No Mixed Terms Map","text":"Next, we optimize the no-mixed-terms map:\n\noptimize!(M_nm, target, quadrature)\n\nmapped_samples_no_mixed = evaluate(M_nm, samples_z)\nvar_diag_no_mixed = variance_diagnostic(M_nm, target, samples_z)\nprintln(\"Variance Diagnostic: \", var_diag_no_mixed)\n\nscatter(mapped_samples_no_mixed[:, 1], mapped_samples_no_mixed[:, 2],\n    ms=4, label=nothing, c=2, title=\"No Mixed\",\n    xlabel=\"x₁\", ylabel=\"x₂\")\nsavefig(\"no_mixed.svg\"); nothing # hide\n\n(Image: No Mixed)","category":"section"},{"location":"Manuals/map_parameterization/#Diagonal-Map-2","page":"Map Parameterization","title":"Diagonal Map","text":"Finally, we optimize the diagonal map:\n\noptimize!(M_d, target, quadrature)\n\nmapped_samples_diagonal = evaluate(M_d, samples_z)\nvar_diag_diagonal = variance_diagnostic(M_d, target, samples_z)\nprintln(\"Variance Diagnostic: \", var_diag_diagonal)\n\nscatter(mapped_samples_diagonal[:, 1], mapped_samples_diagonal[:, 2],\n    ms=4, label=nothing, c=3, title=\"Diagonal\",\n    xlabel=\"x₁\", ylabel=\"x₂\")\nsavefig(\"diagonal.svg\"); nothing # hide\n\n(Image: Diagonal)\n\nWe observe that the total order and no-mixed-terms maps achieve similar variance diagnostics, while the diagonal map performs significantly worse due to its inability to capture dependencies between variables.\n\nnote: Adaptive Map Construction\nThe choice of map parameterization can significantly impact the performance of transport maps. In practice, one might start with a simpler map structure, such as the diagonal or no-mixed-terms map, and then adaptively enrich the map based on the observed performance, as discussed in [3]. This adaptive approach allows for a balance between computational efficiency and approximation accuracy. For more information see the Adaptive Transport Maps manual and [3].\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"Examples/banana_mapfromdensity/#Banana:-Map-from-Density","page":"Banana: Map from Density","title":"Banana: Map from Density","text":"This example demonstrates how to use TransportMaps.jl to approximate a \"banana\" distribution using polynomial transport maps.\n\nThe banana distribution is a common test case in transport map literature [1], defined as a standard normal in the first dimension and a normal distribution centered at x_1^2 in the second dimension. This example showcases the effectiveness of triangular transport maps for capturing nonlinear dependencies [3].\n\nWe start with the necessary packages:\n\nusing TransportMaps\nusing Distributions\nusing Plots\n\nusing Random # hide\nRandom.seed!(123) # hide\nnothing # hide","category":"section"},{"location":"Examples/banana_mapfromdensity/#Creating-the-Transport-Map","page":"Banana: Map from Density","title":"Creating the Transport Map","text":"We start by creating a 2-dimensional polynomial transport map with degree 2 and a Softplus rectifier function.\n\nM = PolynomialMap(2, 2, Normal(), Softplus())","category":"section"},{"location":"Examples/banana_mapfromdensity/#Setting-up-Quadrature","page":"Banana: Map from Density","title":"Setting up Quadrature","text":"For optimization, we need to specify quadrature weights. Here we use a sparse Smolyak grid with level 2:\n\nquadrature = SparseSmolyakWeights(2, 2)","category":"section"},{"location":"Examples/banana_mapfromdensity/#Defining-the-Target-Density","page":"Banana: Map from Density","title":"Defining the Target Density","text":"The banana distribution has the density:\n\np(x) = phi(x_1) cdot phi(x_2 - x_1^2)\n\nwhere phi is the standard normal PDF.\n\nWe pass the logpdf as an input for the map construction\n\ntarget_density(x) = logpdf(Normal(), x[1]) + logpdf(Normal(), x[2] - x[1]^2)\nnothing #hide\n\nCreate a MapTargetDensity object for optimization\n\ntarget = MapTargetDensity(target_density, :auto_diff)","category":"section"},{"location":"Examples/banana_mapfromdensity/#Optimizing-the-Map","page":"Banana: Map from Density","title":"Optimizing the Map","text":"Now we optimize the map coefficients to approximate the target density:\n\nres = optimize!(M, target, quadrature)\nprintln(\"Optimization result: \", res)","category":"section"},{"location":"Examples/banana_mapfromdensity/#Testing-the-Map","page":"Banana: Map from Density","title":"Testing the Map","text":"Let's generate some samples from the standard normal distribution and map them through our optimized transport map:\n\nsamples_z = randn(1000, 2)\n\nMap the samples through our transport map:\n\nmapped_samples = evaluate(M, samples_z)","category":"section"},{"location":"Examples/banana_mapfromdensity/#Visualizing-Results","page":"Banana: Map from Density","title":"Visualizing Results","text":"Let's create a scatter plot of the mapped samples to see how well our transport map approximates the banana distribution:\n\nscatter(mapped_samples[:, 1], mapped_samples[:, 2],\n    label=\"Mapped Samples\", alpha=0.5, color=2,\n    title=\"Transport Map Approximation of Banana Distribution\",\n    xlabel=\"x₁\", ylabel=\"x₂\")\nsavefig(\"samples-banana.svg\"); nothing # hide\n\n(Image: Banana Samples)","category":"section"},{"location":"Examples/banana_mapfromdensity/#Quality-Assessment","page":"Banana: Map from Density","title":"Quality Assessment","text":"We can assess the quality of our approximation using the variance diagnostic:\n\nvar_diag = variance_diagnostic(M, target, samples_z)\nprintln(\"Variance Diagnostic: \", var_diag)","category":"section"},{"location":"Examples/banana_mapfromdensity/#Interpretation","page":"Banana: Map from Density","title":"Interpretation","text":"The variance diagnostic provides a measure of how well the transport map approximates the target distribution. Lower values indicate better approximation.\n\nThe scatter plot should show the characteristic \"banana\" shape, with samples curved according to the relationship x_2 propto x_1^2.","category":"section"},{"location":"Examples/banana_mapfromdensity/#Further-Experiments","page":"Banana: Map from Density","title":"Further Experiments","text":"You can experiment with:\n\nDifferent polynomial degrees (see [3] for monotone map theory)\nDifferent rectifier functions (IdentityRectifier(), ShiftedELU())\nDifferent quadrature methods (MonteCarloWeights, LatinHypercubeWeights, GaussHermiteWeights)\nMore quadrature points for higher accuracy\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"api/maps/#Maps","page":"Maps","title":"Maps","text":"","category":"section"},{"location":"api/maps/#Index","page":"Maps","title":"Index","text":"Pages = [\"maps.md\"]","category":"section"},{"location":"api/maps/#Types","page":"Maps","title":"Types","text":"","category":"section"},{"location":"api/maps/#Functions","page":"Maps","title":"Functions","text":"","category":"section"},{"location":"api/maps/#TransportMaps.PolynomialMap","page":"Maps","title":"TransportMaps.PolynomialMap","text":"PolynomialMap <: AbstractTriangularMap\n\nTriangular transport map with polynomial basis.\n\nFields\n\ncomponents::Vector{PolynomialMapComponent{<:AbstractPolynomialBasis}}: Vector of map components\nreference::MapReferenceDensity: Reference density of the map\nforwarddirection::Symbol: :target for map from density, :reference for map from samples\n\nConstructors\n\nPolynomialMap(dimension::Int,degree::Int, referencetype::Symbol=:normal, rectifier::AbstractRectifierFunction=Softplus(), basis::AbstractPolynomialBasis=LinearizedHermiteBasis(), map_type::Symbol=:total): Initialize polynomial map for map from density.\nDiagonalMap(dimension::Int, degree::Int, referencetype::Symbol=:normal, rectifier::AbstractRectifierFunction=Softplus(), basis::AbstractPolynomialBasis=LinearizedHermiteBasis()): Initialize diagonal map with diagonal structure.\nNoMixedMap(dimension::Int, degree::Int, referencetype::Symbol=:normal, rectifier::AbstractRectifierFunction=Softplus(), basis::AbstractPolynomialBasis=LinearizedHermiteBasis()): Initialize diagonal map without mixed terms.\n\n\n\n\n\n","category":"type"},{"location":"api/maps/#TransportMaps.PolynomialMapComponent","page":"Maps","title":"TransportMaps.PolynomialMapComponent","text":"PolynomialMapComponent{T<:AbstractPolynomialBasis} <: AbstractMapComponent\n\nA single component Mᵏ of a triangular transport map with polynomial basis.\n\nFields\n\nbasisfunctions::Vector{MultivariateBasis{T}}: Vector of multivariate basis functions\ncoefficients::Vector{Float64}: Coefficients for the basis functions\nrectifier::AbstractRectifierFunction: Rectifier function applied to partial derivatives\nindex::Int: Index k indicating the component dimension\n\nConstructors\n\nPolynomialMapComponent(index::Int, degree::Int, rectifier::AbstractRectifierFunction=Softplus(), basis::AbstractPolynomialBasis=HermiteBasis(), map_type::Symbol=:total): Initialize component with specified degree and basis type.\nPolynomialMapComponent(index::Int, degree::Int, rectifier::AbstractRectifierFunction, basis::AbstractPolynomialBasis, density::Distributions.UnivariateDistribution, map_type::Symbol=:total): Initialize component using analytical reference density.\nPolynomialMapComponent(index::Int, degree::Int, rectifier::AbstractRectifierFunction, basis::AbstractPolynomialBasis, samples::AbstractMatrix{Float64}, map_type::Symbol=:total): Initialize component using sample data.\nPolynomialMapComponent(multi_indices::Vector{Vector{Int}}, rectifier::AbstractRectifierFunction, basis::AbstractPolynomialBasis, samples::AbstractMatrix{Float64}): Initialize component with custom multi-index sets from samples.\nPolynomialMapComponent(multi_indices::Vector{Vector{Int}}, rectifier::AbstractRectifierFunction, basis::AbstractPolynomialBasis, reference_density::Distributions.UnivariateDistribution): Initialize component with custom multi-index sets from density.\n\n\n\n\n\n","category":"type"},{"location":"api/maps/#TransportMaps.LinearMap","page":"Maps","title":"TransportMaps.LinearMap","text":"LinearMap <: AbstractLinearMap\n\nA linear transformation map that standardizes data using mean and standard deviation.\n\nFields\n\nμ::Vector{Float64}: Mean vector for each dimension\nσ::Vector{Float64}: Standard deviation vector for each dimension\n\nConstructors\n\nLinearMap(samples::Matrix{Float64}): Compute empirical mean and standard deviation from samples.\nLinearMap(μ::Vector{Float64}, σ::Vector{Float64}): Construct linear map with explicit mean and standard deviation.\n\n\n\n\n\n","category":"type"},{"location":"api/maps/#TransportMaps.LaplaceMap","page":"Maps","title":"TransportMaps.LaplaceMap","text":"LaplaceMap <: AbstractLinearMap\n\nA linear transport map based on the Laplace approximation of a target density.\n\nThe Laplace approximation assumes the target density is approximately Gaussian around its mode. The map is defined by a location parameter (mode) and a scale parameter (Cholesky factor of the covariance).\n\nFields\n\nmode::Vector{Float64}: Mode or mean vector of the approximation\nchol::Matrix{Float64}: Lower Cholesky factor L of the covariance matrix (Σ = L * L')\n\nConstructors\n\nLaplaceMap(samples::Matrix{Float64}): Constructs a LaplaceMap from sample data by\n\ncomputing the empirical mean and covariance.\n\nLaplaceMap(density::MapTargetDensity, x0::Vector{Float64}; hessian_type::Symbol = :auto_diff, optimizer::Optim.AbstractOptimizer = LBFGS(), options::Optim.Options = Optim.Options()): Compute a Laplace approximation of a target density by finding the mode via optimization and computing the Hessian at the mode.\n\n\n\n\n\n","category":"type"},{"location":"api/maps/#TransportMaps.ComposedMap","page":"Maps","title":"TransportMaps.ComposedMap","text":"ComposedMap{T<:AbstractLinearMap} <: AbstractComposedMap\n\nA composed transport map consisting of a linear map followed by a polynomial map.\n\nThe composition is defined as S(x) = M(L(x)) where L is the linear map of type T and M is the polynomial map. The linear map can be a LinearMap or LaplaceMap.\n\nFields\n\nlinearmap<:T: The linear map component\npolynomialmap::PolynomialMap: The polynomial map component\n\nConstructors\n\nComposedMap(lm::AbstractLinearMap, pm::PolynomialMap)\n\n\n\n\n\n","category":"type"},{"location":"api/maps/#TransportMaps.evaluate","page":"Maps","title":"TransportMaps.evaluate","text":"evaluate(mvb::MultivariateBasis{T}, z::Vector{<:Real}) where T<:AbstractPolynomialBasis\n\nEvaluate the multivariate basis function at point z.\n\n\n\n\n\nevaluate(map_component::PolynomialMapComponent, z::AbstractVector{<:Real})\n\nEvaluate the polynomial map component Mᵏ(z) at point z.\n\nComputes Mᵏ(z) = f(z₁,...,zₖ₋₁,0) + ∫₀^{zₖ} g(∂f/∂xₖ) dxₖ, where g is the rectifier.\n\n\n\n\n\nevaluate(map_component::PolynomialMapComponent, Z::AbstractMatrix{<:Real})\n\nEvaluate the map component at multiple points (row-wise) using multithreading.\n\nReturns a vector where element i is Mᵏ(Z[i,:]).\n\n\n\n\n\nevaluate(M::PolynomialMap, z::AbstractVector{<:Real})\n\nEvaluate the polynomial map at point z.\n\nReturns a vector where each component i is Mⁱ(z₁,...,zᵢ).\n\n\n\n\n\nevaluate(M::PolynomialMap, Z::AbstractMatrix{<:Real})\n\nEvaluate the polynomial map at multiple points (row-wise) using multithreading.\n\nEach row of Z is a point to evaluate. Returns a matrix where row i contains M(Z[i,:]).\n\n\n\n\n\nevaluate(L::LinearMap, x::AbstractVector{<:Real})\n\nApply the linear transformation (x - μ) / σ to standardize the input.\n\n\n\n\n\nevaluate(L::LinearMap, X::AbstractMatrix{<:Real})\n\nApply the linear transformation to multiple points (row-wise).\n\n\n\n\n\nevaluate(L::LaplaceMap, x::AbstractVector{<:Real})\n\nApply the Laplace map transformation: L⁻¹(x - mode).\n\n\n\n\n\nevaluate(L::LaplaceMap, X::AbstractMatrix{<:Real})\n\nApply the Laplace map transformation to multiple points (row-wise).\n\n\n\n\n\nevaluate(C::ComposedMap, x::AbstractVector{<:Real})\n\nEvaluate the composed map: S(x) = M(L(x)).\n\n\n\n\n\nevaluate(C::ComposedMap, X::AbstractMatrix{<:Real})\n\nEvaluate the composed map for multiple points (row-wise).\n\n\n\n\n\n","category":"function"},{"location":"api/maps/#TransportMaps.inverse","page":"Maps","title":"TransportMaps.inverse","text":"inverse(map_component::PolynomialMapComponent, zₖ₋₁::AbstractVector{<:Real}, xₖ::Real)\n\nCompute the inverse of the map component using one-dimensional root finding.\n\nGiven z[1:k-1] and the target value xₖ, finds zₖ such that Mᵏ(z) = xₖ.\n\n\n\n\n\ninverse(M::PolynomialMap, x::AbstractVector{<:Real}, k::Int=numberdimensions(M))\n\nCompute the inverse of the first k components of the map at point x.\n\nReturns z such that M(z)[1:k] = x[1:k].\n\n\n\n\n\ninverse(M::PolynomialMap, X::AbstractMatrix{<:Real}, k::Int=numberdimensions(M))\n\nCompute the inverse at multiple points using multithreading.\n\nReturns a matrix where row i contains M⁻¹(X[i,:])[1:k].\n\n\n\n\n\ninverse(L::LinearMap, y::AbstractVector{<:Real})\n\nInvert the linear transformation: y * σ + μ to recover the original scale.\n\n\n\n\n\ninverse(L::LinearMap, Y::AbstractMatrix{<:Real})\n\nInvert the transformation for multiple points (row-wise).\n\n\n\n\n\ninverse(L::LaplaceMap, y::AbstractVector{<:Real})\n\nInvert the Laplace map: L * y + mode.\n\n\n\n\n\ninverse(L::LaplaceMap, Y::AbstractMatrix{<:Real})\n\nInvert the transformation for multiple points (row-wise).\n\n\n\n\n\ninverse(C::ComposedMap, z::AbstractVector{<:Real})\n\nInvert the composed map: S⁻¹(z) = L⁻¹(M⁻¹(z)).\n\n\n\n\n\ninverse(C::ComposedMap, Z::AbstractMatrix{<:Real})\n\nInvert the composed map for multiple points (row-wise).\n\n\n\n\n\n","category":"function"},{"location":"api/maps/#TransportMaps.pushforward","page":"Maps","title":"TransportMaps.pushforward","text":"pushforward(M::PolynomialMap, target::MapTargetDensity, z::AbstractVector{<:Real})\n\nCompute the pushforward density ρ(z) = π(M(z)) |det J_M(z)|.\n\nMaps the target density back through M to the reference space.\n\n\n\n\n\npushforward(M::PolynomialMap, target::MapTargetDensity, Z::AbstractMatrix{<:Real})\n\nCompute pushforward densities at multiple points using multithreading.\n\nReturns a vector where element i is the pushforward density at Z[i,:].\n\n\n\n\n\n","category":"function"},{"location":"api/maps/#TransportMaps.pullback","page":"Maps","title":"TransportMaps.pullback","text":"pullback(M::PolynomialMap, x::AbstractVector{<:Real})\n\nCompute the pullback density π̂(x) = ρ(M⁻¹(x)) |det J_{M⁻¹}(x)|.\n\nMaps the reference density through M to approximate the target density.\n\n\n\n\n\npullback(M::PolynomialMap, X::AbstractMatrix{<:Real})\n\nCompute pullback densities at multiple points using multithreading.\n\nReturns a vector where element i is the pullback density at X[i,:].\n\n\n\n\n\npullback(C::ComposedMap, x::AbstractVector{<:Real})\n\nCompute the pullback density: π(S(x)) * |det(∇S(x))|.\n\n\n\n\n\npullback(C::ComposedMap, X::AbstractMatrix{<:Real})\n\nCompute the pullback density for multiple points (row-wise).\n\n\n\n\n\n","category":"function"},{"location":"api/maps/#TransportMaps.setcoefficients!","page":"Maps","title":"TransportMaps.setcoefficients!","text":"setcoefficients!(map_component::PolynomialMapComponent, coefficients::AbstractVector{<:Real})\n\nSet the coefficients of the map component.\n\n\n\n\n\nsetcoefficients!(M::PolynomialMap, coefficients::AbstractVector{<:Real})\n\nSet all coefficients of the polynomial map.\n\nCoefficients are ordered by component: [c₁₁, c₁₂, ..., c₂₁, c₂₂, ..., cᵈₙ].\n\n\n\n\n\n","category":"function"},{"location":"api/maps/#TransportMaps.getcoefficients","page":"Maps","title":"TransportMaps.getcoefficients","text":"getcoefficients(map_component::PolynomialMapComponent)\n\nGet a copy of the coefficients from the map component.\n\n\n\n\n\ngetcoefficients(M::PolynomialMap)\n\nGet all coefficients from the polynomial map.\n\nReturns a vector with coefficients ordered by component.\n\n\n\n\n\n","category":"function"},{"location":"api/maps/#TransportMaps.setparameters!","page":"Maps","title":"TransportMaps.setparameters!","text":"setparameters!(map::LinearMap, μ::AbstractVector{<:Real}, σ::AbstractVector{<:Real})\n\nSet the mean and standard deviation parameters of the linear map.\n\n\n\n\n\n","category":"function"},{"location":"api/maps/#TransportMaps.numbercoefficients","page":"Maps","title":"TransportMaps.numbercoefficients","text":"numbercoefficients(map_component::PolynomialMapComponent)\n\nReturn the number of coefficients in the map component.\n\n\n\n\n\nnumbercoefficients(M::PolynomialMap)\n\nReturn the total number of coefficients in the polynomial map.\n\n\n\n\n\n","category":"function"},{"location":"api/maps/#TransportMaps.numberdimensions","page":"Maps","title":"TransportMaps.numberdimensions","text":"numberdimensions(M::PolynomialMap)\n\nReturn the number of dimensions (components) of the polynomial map.\n\n\n\n\n\nnumberdimensions(L::LinearMap)\n\nReturn the number of dimensions of the linear map.\n\n\n\n\n\nnumberdimensions(L::LaplaceMap)\n\nReturn the number of dimensions of the Laplace map.\n\n\n\n\n\nnumberdimensions(C::ComposedMap)\n\nReturn the number of dimensions of the composed map.\n\n\n\n\n\n","category":"function"},{"location":"api/maps/#TransportMaps.getmultiindexsets","page":"Maps","title":"TransportMaps.getmultiindexsets","text":"getmultiindexsets(map_component::PolynomialMapComponent)\n\nReturn the multi-indices as a matrix (one per row).\n\n\n\n\n\n","category":"function"},{"location":"api/maps/#TransportMaps.conditional_density","page":"Maps","title":"TransportMaps.conditional_density","text":"conditional_density(M::PolynomialMap, x_range::Real, x_given::AbstractVector{<:Real})\n\nCompute the conditional density π(xₖ | x₁, ..., xₖ₋₁) at xrange given xgiven.\n\n\n\n\n\nconditional_density(M::PolynomialMap, x_range::AbstractVector{<:Real}, x_given::AbstractVector{<:Real})\n\nCompute conditional densities at multiple x_range values using multithreading.\n\n\n\n\n\n","category":"function"},{"location":"api/maps/#TransportMaps.conditional_sample","page":"Maps","title":"TransportMaps.conditional_sample","text":"conditional_sample(M::PolynomialMap, x_given::AbstractVector{<:Real}, z_range::Real)\n\nGenerate a sample from π(xₖ | x₁, ..., xₖ₋₁) by pushing forward zrange ~ ρ(zrange).\n\n\n\n\n\nconditional_sample(M::PolynomialMap, x_given::AbstractVector{<:Real}, z_range::AbstractVector{<:Real})\n\nGenerate multiple samples from π(xₖ | x₁, ..., xₖ₋₁) using multithreading.\n\n\n\n\n\n","category":"function"},{"location":"api/maps/#TransportMaps.multivariate_conditional_density","page":"Maps","title":"TransportMaps.multivariate_conditional_density","text":"multivariate_conditional_density(M::PolynomialMap, x::AbstractVector{<:Real})\n\nCompute the multivariate conditional density π(xⱼ, xⱼ₊₁, ..., xₖ | x₁, ..., xⱼ₋₁) as ∏ᵢ₌ⱼᵏ π(xᵢ | x₁, ..., xᵢ₋₁).\n\n\n\n\n\nmultivariate_conditional_density(M::PolynomialMap, x_range::AbstractVector{<:Real}, x_given::AbstractVector{<:Real})\n\nCompute the conditional density π(xⱼ, ..., xₖ | x₁, ..., xⱼ₋₁) where x_range = [xⱼ, ..., xₖ].\n\n\n\n\n\n","category":"function"},{"location":"api/maps/#TransportMaps.multivariate_conditional_sample","page":"Maps","title":"TransportMaps.multivariate_conditional_sample","text":"multivariate_conditional_sample(M::PolynomialMap, x_given::AbstractVector{<:Real}, z_range::AbstractVector{<:Real})\n\nGenerate samples from π(xⱼ, ..., xₖ | x₁, ..., xⱼ₋₁) by sequentially pushing forward z_range values.\n\n\n\n\n\n","category":"function"},{"location":"Manuals/adaptive_transport_map/#Adaptive-Transport-Maps","page":"Adaptive Transport Maps","title":"Adaptive Transport Maps","text":"A key challenge in constructing transport maps is choosing the appropriate parameterization, specifically the multi-index set that defines which polynomial terms to include in the expansion. While fixed parameterizations like total order, no-mixed terms, or diagonal maps (see Choosing a Map Parameterization) can work well in many cases, they may not be optimal for all target distributions.\n\nAdaptive transport maps address this limitation by automatically selecting the most relevant polynomial terms through a greedy enrichment strategy. This approach is particularly useful when:\n\nThe structure of the target distribution is unknown a priori\nComputational resources are limited and a sparse representation is desired\nHigh-dimensional problems require careful selection of interaction terms","category":"section"},{"location":"Manuals/adaptive_transport_map/#Theory","page":"Adaptive Transport Maps","title":"Theory","text":"The adaptive transport map (ATM) algorithm was introduced by [3] and provides a principled approach to construct sparse, triangular transport maps by adaptively enriching the multi-index set based on gradient information.","category":"section"},{"location":"Manuals/adaptive_transport_map/#Greedy-Multi-Index-Selection","page":"Adaptive Transport Maps","title":"Greedy Multi-Index Selection","text":"Given a triangular transport map with components T^k mathbbR^k to mathbbR for k=1ldotsd, each component is parameterized by a polynomial expansion:\n\nT^k(z_1 ldots z_k boldsymbola) = f(z_1 ldots z_k-1 0 boldsymbola) + int_0^z_k gleft(partial_k f(z_1 ldots z_k-1 xi boldsymbola)right) dxi\n\nwhere f is a multivariate polynomial:\n\nf(z_1 ldots z_k boldsymbola) = sum_alpha in mathcalA_k a_alpha Psi_alpha(z_1 ldots z_k)\n\nHere, mathcalA_k is the multi-index set that determines which terms are included.\n\nThe ATM algorithm starts with a minimal multi-index set (typically containing only the constant term) and iteratively adds terms that maximize the improvement in the objective function. At each iteration t, given the current multi-index set mathcalA_t, the algorithm:\n\nIdentifies candidate terms from the reduced margin of mathcalA_t:\n\nmathcalA_mathrmRM(mathcalA_t) = alpha notin mathcalA_t  alpha - e_i in mathcalA_t text for all  i text with  alpha_i  0\n\nwhere e_i is the i-th standard basis vector.\n\nFor each candidate alpha in mathcalA_mathrmRM(mathcalA_t), evaluates the gradient of the objective with respect to the coefficient a_alpha (initialized to zero).\nSelects the candidate with the largest absolute gradient value:\n\nalpha^+ = argmax_alpha in mathcalA_mathrmRM(mathcalA_t) leftfracpartial Jpartial a_alpharight\n\nUpdates the multi-index set: mathcalA_t+1 = mathcalA_t cup alpha^+ and optimizes all coefficients.\n\nThis greedy selection strategy ensures that at each iteration, the term most likely to improve the objective function is added, leading to sparse and efficient representations.","category":"section"},{"location":"Manuals/adaptive_transport_map/#Cross-Validation-for-Model-Selection","page":"Adaptive Transport Maps","title":"Cross-Validation for Model Selection","text":"A critical question when using adaptive transport maps is: how many terms should be included? Including too few terms may result in underfitting, while including too many can lead to overfitting.\n\nTo address this, the ATM implementation supports k-fold cross-validation. The algorithm:\n\nSplits the data into k folds\nFor each fold, trains the map on k-1 folds and validates on the remaining fold\nTracks both training and validation objectives at each iteration\nSelects the number of terms that minimizes the average validation objective across folds\n\nThis approach provides a data-driven way to balance model complexity and generalization performance.","category":"section"},{"location":"Manuals/adaptive_transport_map/#Usage-in-TransportMaps.jl","page":"Adaptive Transport Maps","title":"Usage in TransportMaps.jl","text":"The optimize_adaptive_transportmap function provides interfaces for constructing adaptive transport maps from either samples or a known density function.","category":"section"},{"location":"Manuals/adaptive_transport_map/#Adaptive-Maps-from-Samples","page":"Adaptive Transport Maps","title":"Adaptive Maps from Samples","text":"When working with sample data, the simplest approach uses a fixed train-test split to monitor overfitting:\n\nM, histories = optimize_adaptive_transportmap(\n    samples,             # Matrix of samples (n_samples × d)\n    maxterms,            # Vector of maximum terms per component\n    lm,                  # Linear map for standardization (default: LinearMap(samples))\n    rectifier,           # Rectifier function (default: Softplus())\n    basis;               # Polynomial basis (default: LinearizedHermiteBasis())\n    optimizer = LBFGS(),\n    options = Optim.Options(),\n    test_fraction = 0.2  # Fraction of data for validation\n)\n\nFor automatic model selection, use k-fold cross-validation. This implementation is based on the original algorithm proposed in [3]:\n\nM, fold_histories, selected_terms, selected_folds = optimize_adaptive_transportmap(\n    samples,             # Matrix of samples (n_samples × d)\n    maxterms,            # Vector of maximum terms per component\n    k_folds,             # Number of folds for cross-validation\n    lm,                  # Linear map for standardization (default: LinearMap(samples))\n    rectifier,           # Rectifier function (default: Softplus())\n    basis;               # Polynomial basis (default: LinearizedHermiteBasis())\n    optimizer = LBFGS(),\n    options = Optim.Options()\n)\n\nThe k-fold version returns:\n\nM: The final composed transport map trained on all data with the selected number of terms\nfold_histories: Optimization histories for each component and fold\nselected_terms: Number of terms selected for each component based on cross-validation\nselected_folds: Which fold had the best performance for each component\n\nexample: Example from Samples\nThe usage is demonstrated in the example Banana: Adaptive Transport Map from Samples.","category":"section"},{"location":"Manuals/adaptive_transport_map/#Adaptive-Maps-from-Density","page":"Adaptive Transport Maps","title":"Adaptive Maps from Density","text":"When the target density function is known analytically, adaptive maps can be constructed directly without requiring samples. This approach uses quadrature methods for integration and adaptively enriches the multi-index set across all components simultaneously:\n\nM, history = optimize_adaptive_transportmap(\n    target,              # AbstractMapDensity: Target density to approximate\n    quadrature,          # AbstractQuadratureWeights: Quadrature points and weights\n    maxterms;            # Maximum total number of terms to add across all components\n    rectifier = Softplus(),\n    basis = LinearizedHermiteBasis(),\n    reference_density = Normal(),\n    optimizer = LBFGS(),\n    options = Optim.Options(),\n    validation = nothing  # Optional: validation quadrature for model selection\n)\n\nKey differences from the sample-based approach:\n\nUses a single global budget of terms (maxterms) shared across all components\nSelects which component to enrich at each iteration based on gradient information\nSupports optional validation using a separate quadrature rule\nReturns the map with the best validation KL divergence (if validation is provided)\n\nThe returned history contains:\n\nmaps: Array of maps at each iteration\ntrain_objectives: Training KL divergence values\ntest_objectives: Validation KL divergence values (if validation provided)\ngradients: Gradient metrics for all candidates at each iteration\n\nexample: Example from Density\nThe usage is demonstrated in the example Cubic: Adaptive Transport Map from Density.","category":"section"},{"location":"Manuals/adaptive_transport_map/#References","page":"Adaptive Transport Maps","title":"References","text":"The implementation of adaptive transport maps is based on the work by Baptista et al.:\n\nBaptista, R., Marzouk, Y., & Zahm, O. (2023). On the Representation and Learning of Monotone Triangular Transport Maps. Foundations of Computational Mathematics. https://doi.org/10.1007/s10208-023-09630-x\nMatlab implementation of the original ATM algorithm: https://github.com/baptistar/ATM\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"Examples/cubic_adaptive_fromdensity/#Cubic:-Adaptive-Transport-Map-from-Density","page":"Cubic: Adaptive Transport Map from Density","title":"Cubic: Adaptive Transport Map from Density","text":"This example demonstrates the adaptive map construction from a given density pi(x). The target is given by the unnormalized joint density\n\np(x) = phi(x_1) cdot phi(x_2 - x_1^3)\n\nThis density is similar to the banana density, however, it has a cubic instead of a quadratic term in the second term of the product.","category":"section"},{"location":"Examples/cubic_adaptive_fromdensity/#Key-Differences-from-Sample-Based-ATM","page":"Cubic: Adaptive Transport Map from Density","title":"Key Differences from Sample-Based ATM","text":"Unlike the sample-based adaptive transport map approach (see Banana: Adaptive Transport Map from Samples), this method constructs the map directly from the analytical density function without requiring samples. The key differences are:\n\nGlobal term budget: Instead of specifying maximum terms per component, we provide a single global budget that is shared across all components. The algorithm decides which component to enrich at each iteration.\nQuadrature-based optimization: Uses numerical integration via quadrature rules rather than empirical sample averages to evaluate the KL divergence.\nSimultaneous enrichment: At each iteration, the algorithm evaluates candidate terms from all components and selects the one with the largest gradient magnitude, leading to a more flexible enrichment strategy.\nNo cross-validation: The density-based approach uses a simple train-validation split rather than k-fold cross-validation, making it more computationally efficient.\n\nMore information is found in the manual page Adaptive Transport Maps.\n\nWe start with loading the necessary packages:\n\nusing TransportMaps\nusing Distributions\nusing LinearAlgebra\nusing Plots\n\nusing Random # hide\nRandom.seed!(123) # hide\nnothing # hide","category":"section"},{"location":"Examples/cubic_adaptive_fromdensity/#Defining-the-Target-Density","page":"Cubic: Adaptive Transport Map from Density","title":"Defining the Target Density","text":"We define the log-density function and wrap it in a MapTargetDensity object. The :ad argument indicates that automatic differentiation should be used for computing gradients.\n\ntarget_density(x) = logpdf(Normal(0, 0.5), x[1]) + logpdf(Normal(0, 0.1), x[2] - x[1]^3)\n\ntarget = MapTargetDensity(target_density, :ad)\nnothing # hide","category":"section"},{"location":"Examples/cubic_adaptive_fromdensity/#Choosing-a-Quadrature-Method","page":"Cubic: Adaptive Transport Map from Density","title":"Choosing a Quadrature Method","text":"For density-based ATM, we need to specify quadrature points and weights for numerical integration. Here, we use Smolyak sparse grid quadrature with level 3 in 2 dimensions, which provides a good balance between accuracy and computational cost.\n\nquadrature = SparseSmolyakWeights(3, 2)","category":"section"},{"location":"Examples/cubic_adaptive_fromdensity/#Adaptive-Map-Construction","page":"Cubic: Adaptive Transport Map from Density","title":"Adaptive Map Construction","text":"We construct the adaptive transport map with a global budget of 10 terms (including the initial constant terms). The algorithm will iteratively add terms to the components that maximize the reduction in KL divergence.\n\nFor model selection, we provide validation quadrature points using Latin hypercube sampling. The algorithm returns the map with the best validation KL divergence.\n\nT, hist = optimize_adaptive_transportmap(target, quadrature, 10;\n    validation = LatinHypercubeWeights(100, 2))\ndisplay(hist)\n\nThe function uses default parameters:\n\nrectifier = Softplus(): Monotonicity-enforcing function\nbasis = LinearizedHermiteBasis(): Polynomial basis (Hermite polynomials with linearization)\nreference_density = Normal(): Reference distribution (standard normal)","category":"section"},{"location":"Examples/cubic_adaptive_fromdensity/#Convergence-Analysis","page":"Cubic: Adaptive Transport Map from Density","title":"Convergence Analysis","text":"The optimization history allows us to analyze the convergence behavior and model selection. We can plot both the training and validation KL divergence over iterations, as well as the maximum gradient magnitude among all candidate terms at each iteration. The gradient plot shows the maximum absolute gradient value among all candidate terms at each iteration. This indicates how much improvement we expect from adding the best term.\n\nconvergence_kl = plot(hist.train_objectives, label=\"Train Objective\", xlabel=\"Iteration\",\n    ylabel=\"KL divergence\", title=\"Objective Value vs Iteration\", marker=:o)\nplot!(convergence_kl, hist.test_objectives, label=\"Test Objective\", marker=:o)\nyaxis!(:log10)\n\ngrad_norms = maximum.(hist.gradients[2:end])\n\nconvergence_grad = plot(2:length(hist.gradients), grad_norms; xlabel=\"Iteration\",\n    ylabel=\"Maximum Gradient\", label=nothing, marker=:o, title=\"Gradient\")\nyaxis!(:log10)\nxlims!(xlims(convergence_kl))\n\nplot(convergence_kl, convergence_grad, layout=(2, 1))\nsavefig(\"cubic-density-convergence.svg\"); nothing # hide\n\n(Image: Cubic density: Convergence)\n\nWe observe that:\n\nThe training objective decreases monotonically as expected\nThe validation objective reaches a minimum and then may increase, indicating potential overfitting\nThe gradient magnitudes decrease over iterations, showing diminishing returns from adding new terms\nThe algorithm automatically selects the model with the best validation performance. Here, it is the map fitted at iteration 7.\n\nGenerate samples and compare response\n\nsamples_z = randn(2000, 2)\nmapped_samples = evaluate(T, samples_z)\nnothing #hide\n\nWe compare the computed map with the target pdf\n\nx1 = -2:0.01:2\nx2 = -2:0.01:2\n\npdf_val = [pdf(target, [x₁, x₂]) for x₂ in x2, x₁ in x1]\n\ns = scatter(mapped_samples[:, 1], mapped_samples[:, 2],\n    label=\"Mapped Samples\", alpha=0.5, color=1,\n    xlabel=\"x₁\", ylabel=\"x₂\", title=\"Target Density and Mapped Samples\")\n\ncontour!(x1, x2, pdf_val)\nsavefig(\"cubic-density.svg\"); nothing # hide\n\n(Image: Cubic density: TM samples and density contour)","category":"section"},{"location":"Examples/cubic_adaptive_fromdensity/#Selected-Multi-Index-Sets","page":"Cubic: Adaptive Transport Map from Density","title":"Selected Multi-Index Sets","text":"We can visualize which polynomial terms were selected for each map component. The adaptive algorithm automatically determines the sparsity pattern based on the gradient information, potentially discovering non-trivial interaction structures.\n\nind_atm = getmultiindexsets(T[1])\nMIS1 = scatter(ind_atm[:, 1], zeros(length(ind_atm)), ms=30, legend=false)\nplot!(xlims=(-0.5, maximum(ind_atm[:, 1]) + 0.5), ylims=(-0.5, 0.5),\n    aspect_ratio=1, xlabel=\"Multi-index α₁\", ylabel=\"\", title=\"Multi-indices Component 1\")\nxticks!(0:maximum(ind_atm[:, 1]))\nyticks!(0:0)\n\nind_atm = getmultiindexsets(T[2])\nMIS2 = scatter(ind_atm[:, 1], ind_atm[:, 2], ms=30, legend=false)\nplot!(xlims=(-0.5, maximum(ind_atm[:, 1]) + 0.5), ylims=(-0.5, maximum(ind_atm[:, 2]) + 0.5),\n    aspect_ratio=1, xlabel=\"Multi-index α₁\", ylabel=\"Multi-index α₂\",\n    title=\"Multi-indices Component 2\")\nxticks!(0:maximum(ind_atm[:, 1]))\nyticks!(0:maximum(ind_atm[:, 2]))\n\nplot(MIS1, MIS2, layout=(2, 1))\nsavefig(\"cubic-density-terms.svg\"); nothing # hide\n\n(Image: Cubic density: Terms in the multi-index sets of the two map components)\n\nThe selected terms reveal the structure the algorithm discovered:\n\nComponent 1 only contains univariate terms due to the triangular structure, i.e., T^1(z_1)\nComponent 2 also only contains terms with no-mixed multivariate polynomials. This means, T^2 does depend on both z_1 and z_2, but there are no non-linear interaction terms.\nThe adaptive approach avoids unnecessary terms, resulting in a sparse representation. This is shown by the final form which contains 7 terms instead of the maximum possible 10 terms.\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"api/quadrature/#Quadrature","page":"Quadrature","title":"Quadrature","text":"","category":"section"},{"location":"api/quadrature/#Index","page":"Quadrature","title":"Index","text":"Pages = [\"quadrature.md\"]","category":"section"},{"location":"api/quadrature/#Types","page":"Quadrature","title":"Types","text":"","category":"section"},{"location":"api/quadrature/#Functions","page":"Quadrature","title":"Functions","text":"","category":"section"},{"location":"api/quadrature/#TransportMaps.GaussHermiteWeights","page":"Quadrature","title":"TransportMaps.GaussHermiteWeights","text":"GaussHermiteWeights\n\nTensor product Gauss-Hermite quadrature for numerical integration with Gaussian reference measure. Uses numberpoints points per dimension, resulting in numberpoints^dimension total quadrature points.\n\nFields\n\npoints::Matrix{Float64}: Quadrature points\nweights::Vector{Float64}: Quadrature weights\n\nConstructors\n\nGaussHermiteWeights(numberpoints::Int64, dimension::Int64): Construct weights for number of points per dimension and dimension.\nGaussHermiteWeights(numberpoints::Int64, map::AbstractTransportMap): Get number of dimensions from map.\n\n\n\n\n\n","category":"type"},{"location":"api/quadrature/#TransportMaps.MonteCarloWeights","page":"Quadrature","title":"TransportMaps.MonteCarloWeights","text":"MonteCarloWeights\n\nMonte Carlo quadrature using random samples from the reference distribution. All points receive uniform weights 1/numberpoints.\n\nFields\n\npoints::Matrix{Float64}: Quadrature points (random samples)\nweights::Vector{Float64}: Quadrature weights (uniform)\n\nConstructors\n\nMonteCarloWeights(numberpoints::Int64, dimension::Int64): Construct weights with random sampling.\nMonteCarloWeights(numberpoints::Int64, map::AbstractTransportMap): Get number of dimensions and reference density from map.\nMonteCarloWeights(points::Matrix{Float64}, weights::Vector{Float64}=Float64[]): Construct from custom points and weights.\n\n\n\n\n\n","category":"type"},{"location":"api/quadrature/#TransportMaps.LatinHypercubeWeights","page":"Quadrature","title":"TransportMaps.LatinHypercubeWeights","text":"LatinHypercubeWeights\n\nLatin Hypercube sampling for quasi-Monte Carlo integration. Provides better space-filling properties than pure Monte Carlo. All points receive uniform weights 1/n.\n\nFields\n\npoints::Matrix{Float64}: Quadrature points (Latin Hypercube samples)\nweights::Vector{Float64}: Quadrature weights (uniform)\n\nConstructors\n\nLatinHypercubeWeights(n::Int64, d::Int64): Construct Latin Hypercube samples for d dimensions.\nLatinHypercubeWeights(n::Int64, map::AbstractTransportMap): Get number of dimensions and reference density from map.\n\n\n\n\n\n","category":"type"},{"location":"api/quadrature/#TransportMaps.SparseSmolyakWeights","page":"Quadrature","title":"TransportMaps.SparseSmolyakWeights","text":"SparseSmolyakWeights\n\nSparse Smolyak quadrature using Gauss-Hermite rules. Reduces the curse of dimensionality by using a sparse grid construction. The level parameter controls accuracy (higher level = more points and higher accuracy).\n\nFields\n\npoints::Matrix{Float64}: Quadrature points (sparse grid)\nweights::Vector{Float64}: Quadrature weights\n\nConstructors\n\nSparseSmolyakWeights(level::Int64, dimension::Int64): Construct sparse Smolyak grid with specified level and dimension.\nSparseSmolyakWeights(level::Int64, map::AbstractTransportMap): Get number of dimensions from map.\n\n\n\n\n\n","category":"type"},{"location":"api/quadrature/#TransportMaps.gaussquadrature","page":"Quadrature","title":"TransportMaps.gaussquadrature","text":"gaussquadrature(fun::Function, n::Int, a::Real, b::Real)\n\nNumerically integrates the function fun over the interval [a, b] using the Gauss-Legendre quadrature rule with n points.\n\nArguments\n\nfun::Function: The function to integrate. Should accept a single Float64 argument.\nn::Int: The number of quadrature points to use.\na::Real: The lower bound of the integration interval.\nb::Real: The upper bound of the integration interval.\n\nReturns\n\nFloat64: The approximate value of the integral of fun over [a, b].\n\n\n\n\n\n","category":"function"},{"location":"api/rectifiers/#Rectifier-Functions","page":"Rectifiers","title":"Rectifier Functions","text":"","category":"section"},{"location":"api/rectifiers/#Index","page":"Rectifiers","title":"Index","text":"Pages = [\"rectifiers.md\"]","category":"section"},{"location":"api/rectifiers/#Types","page":"Rectifiers","title":"Types","text":"","category":"section"},{"location":"api/rectifiers/#TransportMaps.Softplus","page":"Rectifiers","title":"TransportMaps.Softplus","text":"Softplus(β::Float64=1.0)\n\nSmooth rectifier function: g(ξ) = log(1 + exp(βξ)) / β. Ensures monotonicity with smooth approximation to ReLU. Higher β values create sharper transitions.\n\n\n\n\n\n","category":"type"},{"location":"api/rectifiers/#TransportMaps.ShiftedELU","page":"Rectifiers","title":"TransportMaps.ShiftedELU","text":"ShiftedELU()\n\nRectifier combining exponential and linear behavior: g(ξ) = exp(ξ) for ξ ≤ 0, g(ξ) = ξ + 1 for ξ > 0. Ensures monotonicity with different behavior for negative and positive inputs.\n\n\n\n\n\n","category":"type"},{"location":"api/rectifiers/#TransportMaps.IdentityRectifier","page":"Rectifiers","title":"TransportMaps.IdentityRectifier","text":"IdentityRectifier()\n\nNo-op rectifier: g(ξ) = ξ. Does not ensure monotonicity! Only use when partial derivatives are guaranteed to be positive by other means.\n\n\n\n\n\n","category":"type"},{"location":"api/rectifiers/#TransportMaps.Exponential","page":"Rectifiers","title":"TransportMaps.Exponential","text":"Exponential()\n\nExponential rectifier: g(ξ) = exp(ξ). Ensures strict positivity and monotonicity. Can lead to extreme values for large |ξ|.\n\n\n\n\n\n","category":"type"},{"location":"Manuals/getting_started/#Getting-Started-with-TransportMaps.jl","page":"Getting Started","title":"Getting Started with TransportMaps.jl","text":"This guide will help you get started with TransportMaps.jl for constructing and using transport maps.","category":"section"},{"location":"Manuals/getting_started/#Basic-Concepts","page":"Getting Started","title":"Basic Concepts","text":"","category":"section"},{"location":"Manuals/getting_started/#What-is-a-Transport-Map?","page":"Getting Started","title":"What is a Transport Map?","text":"A transport map T boldsymbolZ mapsto boldsymbolX is a mapping from reference space boldsymbolZ sim rho(boldsymbolz) to the target space boldsymbolX sim pi(boldsymbolx) [1]. Hence, the inverse map T^-1 boldsymbolX mapsto boldsymbolZ maps from the target to the reference space.","category":"section"},{"location":"Manuals/getting_started/#Triangular-Maps","page":"Getting Started","title":"Triangular Maps","text":"TransportMaps.jl focuses on triangular transport maps [3], following the Knothe-Rosenblatt rearrangement [6]. This structure ensures that the map is invertible and the Jacobian determinant is easy to compute. A triangular map in n dimensions has the form:\n\nT(boldsymbolz) =\nleft(beginarrayc\nT_1(z_1) \nT_2(z_1 z_2) \nT_3(z_1 z_2 z_3) \nvdots \nT_n(z_1 z_2 dots z_n)\nendarray\nright)\n\nThe inverse map T^-1 can be computed sequentially by inverting each component.","category":"section"},{"location":"Manuals/getting_started/#First-Example:-A-Simple-2D-Transport-Map","page":"Getting Started","title":"First Example: A Simple 2D Transport Map","text":"using TransportMaps\nusing Distributions\nusing Random\nusing Plots\nusing LinearAlgebra\n\nLet's create a simple 2D transport map:\n\nSet random seed for reproducibility\n\nRandom.seed!(1234)\nnothing #hide\n\nCreate a 2D polynomial map with degree 2\n\nM = PolynomialMap(2, 2, Normal(), Softplus())\n\nThe map is initially identity (coefficients are zero)\n\nprintln(\"Initial coefficients: \", getcoefficients(M))","category":"section"},{"location":"Manuals/getting_started/#Defining-a-Target-Distribution","page":"Getting Started","title":"Defining a Target Distribution","text":"For optimization, you need to define your target probability density. Let's start with a simple correlated Gaussian:\n\nExample: Correlated Gaussian\n\nfunction correlated_gaussian(x; ρ=0.8)\n    Σ = [1.0 ρ; ρ 1.0]\n    return logpdf(MvNormal(zeros(2), Σ), x)\nend\nnothing #hide\n\nCreate a MapTargetDensity object for optimization\n\ntarget_density = MapTargetDensity(correlated_gaussian, :auto_diff)","category":"section"},{"location":"Manuals/getting_started/#Setting-up-Quadrature","page":"Getting Started","title":"Setting up Quadrature","text":"Choose an appropriate quadrature scheme for map optimization:\n\nGauss-Hermite quadrature (good for Gaussian-like targets)\n\nquadrature = GaussHermiteWeights(5, 2)  # 5 points per dimension, 2D\n# alternative options:\n# quadrature = MonteCarloWeights(1000, 2)  # 1000 samples, 2D\n# quadrature = LatinHypercubeWeights(1000, 2)\n# quadrature = SparseSmolyakWeights(3, 2)  # Level 3, 2D","category":"section"},{"location":"Manuals/getting_started/#Optimizing-the-Map","page":"Getting Started","title":"Optimizing the Map","text":"Fit the transport map to your target distribution:\n\nresult = optimize!(M, target_density, quadrature)","category":"section"},{"location":"Manuals/getting_started/#Generating-Samples","page":"Getting Started","title":"Generating Samples","text":"Once optimized, use the map to generate samples:\n\nGenerate reference samples (standard Gaussian)\n\nn_samples = 1000\nreference_samples = randn(n_samples, 2)\n\nTransform to target distribution\n\ntarget_samples = evaluate(M, reference_samples)","category":"section"},{"location":"Manuals/getting_started/#Visualizing-Results","page":"Getting Started","title":"Visualizing Results","text":"Let's plot both the reference and target samples:\n\np1 = scatter(reference_samples[:, 1], reference_samples[:, 2],\n    alpha=0.6, title=\"Reference Samples\",\n    xlabel=\"Z₁\", ylabel=\"Z₂\", legend=false, aspect_ratio=:equal)\n\np2 = scatter(target_samples[:, 1], target_samples[:, 2],\n    alpha=0.6, title=\"Target Samples\",\n    xlabel=\"X₁\", ylabel=\"X₂\", legend=false, aspect_ratio=:equal)\n\nplot(p1, p2, layout=(1, 2), size=(800, 400))\nsavefig(\"samples.svg\"); nothing # hide\n\n(Image: Transport Map Samples)","category":"section"},{"location":"Manuals/getting_started/#Evaluating-Map-Quality","page":"Getting Started","title":"Evaluating Map Quality","text":"Check how well your map approximates the target:\n\nVariance diagnostic (should be close to 1 for good maps)\n\nvar_diag = variance_diagnostic(M, target_density, reference_samples)\nprintln(\"Variance diagnostic: \", var_diag)\n\nYou can also check the Jacobian determinant\n\nsample_point = [0.0, 0.0]\njac = jacobian(M, sample_point)\ndet_jac = det(jac)\nprintln(\"Jacobian determinant at origin: \", det_jac)","category":"section"},{"location":"Manuals/getting_started/#Working-with-Different-Rectifiers","page":"Getting Started","title":"Working with Different Rectifiers","text":"The rectifier function affects the map's behavior. Let's compare different options:\n\nShiftedELU rectifier\n\nM_elu = PolynomialMap(2, 2, Normal(), ShiftedELU())\nresult_elu = optimize!(M_elu, target_density, quadrature)\nvar_diag_elu = variance_diagnostic(M_elu, target_density, reference_samples)\n\nprintln(\"Variance diagnostics:\")\nprintln(\"  Softplus: \", var_diag)\nprintln(\"  ShiftedELU: \", var_diag_elu)","category":"section"},{"location":"Manuals/getting_started/#More-Complex-Example:-Banana-Distribution","page":"Getting Started","title":"More Complex Example: Banana Distribution","text":"Now let's try a more challenging target - the banana distribution:\n\nDefine banana density\n\nbanana_density(x) = pdf(Normal(), x[1]) * pdf(Normal(), x[2] - x[1]^2)\ntarget_density_banana = MapTargetDensity(x -> log.(banana_density(x)), :auto_diff)\n\nCreate a new map for this target and optimize:\n\nM_banana = PolynomialMap(2, 2, Normal(), Softplus())\nresult_banana = optimize!(M_banana, target_density_banana, quadrature)\n\nGenerate samples\n\nbanana_samples = evaluate(M_banana, reference_samples)\n\nVisualize the banana distribution\n\nx1_grid = range(-3, 3, length=100)\nx2_grid = range(-3, 6, length=100)\nposterior_values = [banana_density([x₁, x₂]) for x₂ in x2_grid, x₁ in x1_grid]\n\nscatter(banana_samples[:, 1], banana_samples[:, 2],\n    alpha=0.6, title=\"Banana Distribution Samples\",\n    xlabel=\"X₁\", ylabel=\"X₂\", legend=false, aspect_ratio=:equal)\ncontour!(x1_grid, x2_grid, posterior_values, colormap=:viridis, label=\"Posterior Density\")\nsavefig(\"banana_samples.svg\"); nothing # hide\n\n(Image: Banana Distribution Samples)\n\nCheck quality\n\nvar_diag_banana = variance_diagnostic(M_banana, target_density_banana, reference_samples)\nprintln(\"Banana distribution variance diagnostic: \", var_diag_banana)\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"#TransportMaps.jl","page":"Home","title":"TransportMaps.jl","text":"This is an implementation of triangular transport maps in Julia based on the description in [1]. For a comprehensive introduction to transport maps, see [2]. The theoretical foundations for monotone triangular transport maps are detailed in [1], [3]. For practical applications in structural health monitoring and Bayesian inference, see [4].","category":"section"},{"location":"#What-are-Transport-Maps?","page":"Home","title":"What are Transport Maps?","text":"Transport maps are smooth, invertible functions that can transform one probability distribution into another [1]. The mathematical foundation builds on the Rosenblatt transformation [5] and the Knothe-Rosenblatt rearrangement [6]. They are particularly useful for:\n\nSampling: Generate samples from complex distributions by transforming samples from simple distributions\nVariational inference: Approximate complex posterior distributions in Bayesian updating problems [4]\nDensity estimation: Learn the structure of complex probability distributions","category":"section"},{"location":"#Key-Features","page":"Home","title":"Key Features","text":"Polynomial Maps: Triangular polynomial transport maps\nAdaptive Construction: Automatic selection of polynomial terms for efficient approximation\nMultiple Rectifiers: Support for different activation functions (Softplus, ShiftedELU, Identity)\nQuadrature Integration: Multiple quadrature schemes for map optimization\nOptimization: Built-in optimization routines for fitting maps to target densities\nMultithreaded evaluation for processing multiple points efficiently","category":"section"},{"location":"#Installation","page":"Home","title":"Installation","text":"using Pkg\nPkg.add(\"https://github.com/lukasfritsch/TransportMaps.jl\")","category":"section"},{"location":"#Quick-Start-Example","page":"Home","title":"Quick Start Example","text":"For a comprehensive introduction, see the page Getting Started with TransportMaps.jl, which demonstrates:\n\nCreating polynomial transport maps\nSetting up quadrature schemes\nOptimizing map parameters\nGenerating samples from the learned map\n\nAdditional examples are available in the Examples section:\n\nPages = [\"Examples/banana_mapfromdensity.md\", \"Examples/banana_mapfromsamples.md\", \"Examples/banana_adaptive.md\", \"Examples/cubic_adaptive_fromdensity.md\", \"Examples/bod_bayesianinference.md\"]\nDepth = 1\n\nFurther, the following manuals discuss the technical details of the implementation:\n\nPages = [\"Manuals/basis_functions.md\", \"Manuals/map_parameterization.md\", \"Manuals/quadrature_methods.md\", \"Manuals/optimization.md\", \"Manuals/conditional_densities.md\", \"Manuals/adaptive_transport_map.md\"]\nDepth = 1","category":"section"},{"location":"#Package-Architecture","page":"Home","title":"Package Architecture","text":"The package is organized around several key components:","category":"section"},{"location":"#Map-Components","page":"Home","title":"Map Components","text":"PolynomialMapComponent: Individual polynomial components of triangular maps\nwith different polynomial bases:\nHermiteBasis: Probabilists' Hermite polynomial basis\nLinearizedHermiteBasis: Linearized Hermite basis for improved conditioning\nCubicSplineHermiteBasis: Cubic spline basis with Hermite support\nGaussianWeightedHermiteBasis: Hermite basis with Gaussian weighting","category":"section"},{"location":"#Transport-Maps","page":"Home","title":"Transport Maps","text":"PolynomialMap: Main triangular polynomial transport map\nDiagonalMap: Convenience constructor for diagonal maps (no cross-terms)\nNoMixedMap: Maps without mixed polynomial terms\nLinearMap: Linear standardization map (mean/std)\nLaplaceMap: Laplace approximation-based map\nComposedMap: Composition of linear and polynomial maps","category":"section"},{"location":"#Rectifier-Functions","page":"Home","title":"Rectifier Functions","text":"IdentityRectifier: No transformation (linear)\nSoftplus: Smooth positive transformation (log-sum-exp)\nShiftedELU: Exponential linear unit variant\nExponential: Pure exponential transformation","category":"section"},{"location":"#Quadrature-Schemes","page":"Home","title":"Quadrature Schemes","text":"GaussHermiteWeights: Gauss-Hermite quadrature points and weights\nMonteCarloWeights: Monte Carlo integration\nLatinHypercubeWeights: Latin hypercube sampling\nSparseSmolyakWeights: Sparse grid quadrature for higher dimensions","category":"section"},{"location":"#Map-Optimization","page":"Home","title":"Map Optimization","text":"optimize!: Optimize map coefficients to minimize KL divergence\noptimize_adaptive_transportmap: Adaptive refinement for map from samples\noptimize_adaptive_transportmapcomponent: Adaptive refinement for map from density","category":"section"},{"location":"#Conditional-Densities","page":"Home","title":"Conditional Densities","text":"conditional_density: Compute conditional densities π(xₖ | x₁, ..., xₖ₋₁)\nconditional_sample: Sample from conditional distributions\nmultivariate_conditional_density: Multivariate conditional densities\nmultivariate_conditional_sample: Multivariate conditional sampling","category":"section"},{"location":"#API-Reference","page":"Home","title":"API Reference","text":"Pages = [\"api/bases.md\", \"api/rectifiers.md\", \"api/densities.md\", \"api/quadrature.md\", \"api/maps.md\", \"api/optimization.md\"]\nDepth = 1","category":"section"},{"location":"#Authors","page":"Home","title":"Authors","text":"Lukas Fritsch, Institute for Risk and Reliability, Leibniz University Hannover\nJan Grashorn, Chair for Engineering Materials and Building Preservation, Helmut-Schmidt-University Hamburg","category":"section"},{"location":"#Related-Implementation","page":"Home","title":"Related Implementation","text":"ATM: Matlab code for adaptive transport maps [3]\nMParT: C++-based library for transport maps [7]\nTransportBasedInference.jl: Julia implementation of adaptive transport maps (ATM) and Kalman filters\nSequentialMeasureTransport.jl: Julia implementation of transport maps from sum-of-squares densities [8]\nTriangular-Transport-Toolbox: Python code for the triangular transport tutorial paper [2]","category":"section"},{"location":"api/densities/#Reference-and-Target-Densities","page":"Reference and Target Densities","title":"Reference and Target Densities","text":"","category":"section"},{"location":"api/densities/#Index","page":"Reference and Target Densities","title":"Index","text":"Pages = [\"densities.md\"]","category":"section"},{"location":"api/densities/#Types","page":"Reference and Target Densities","title":"Types","text":"","category":"section"},{"location":"api/densities/#Functions","page":"Reference and Target Densities","title":"Functions","text":"","category":"section"},{"location":"api/densities/#TransportMaps.MapTargetDensity","page":"Reference and Target Densities","title":"TransportMaps.MapTargetDensity","text":"MapTargetDensity\n\nWrapper for target density functions used in transport map optimization. Stores the log-density function and its gradient, with support for automatic differentiation, finite differences, or analytical gradients.\n\nFields\n\nlogdensity::F: Function computing log-density log π(x)\ngradient_type::Symbol: Type of gradient computation (:analytical, :auto_diff, or :finite_difference)\ngrad_logdensity::G: Function computing gradient ∇ log π(x)\n\nConstructors\n\nMapTargetDensity(logdensity, grad_logdensity): Provide both log-density and analytical gradient.\nMapTargetDensity(logdensity, :analytical, grad_logdensity): Explicitly specify analytical gradient.\nMapTargetDensity(logdensity, :auto_diff): Use automatic differentiation for gradient.\nMapTargetDensity(logdensity, :finite_difference): Use finite differences for gradient.\n\n\n\n\n\n","category":"type"},{"location":"api/densities/#TransportMaps.MapReferenceDensity","page":"Reference and Target Densities","title":"TransportMaps.MapReferenceDensity","text":"MapReferenceDensity\n\nWrapper for reference density (typically standard Gaussian) used in transport maps. The reference density defines the space from which samples are drawn and mapped to the target distribution.\n\nFields\n\nlogdensity::F: Function computing log-density log ρ(z)\ngradient_type::Symbol: Type of gradient computation (always :auto_diff)\ngrad_logdensity::G: Function computing gradient ∇ log ρ(z)\ndensitytype::Distributions.UnivariateDistribution: Univariate density type (e.g., Normal())\n\nConstructors\n\nMapReferenceDensity(): Use standard normal distribution (default).\nMapReferenceDensity(densitytype::Distributions.UnivariateDistribution): Specify reference distribution.\n\n\n\n\n\n","category":"type"},{"location":"api/densities/#Distributions.pdf","page":"Reference and Target Densities","title":"Distributions.pdf","text":"pdf(density::AbstractMapDensity, x)\n\nEvaluate the probability density at point(s) x.\n\nArguments\n\ndensity::AbstractMapDensity: Target or reference density\nx: Point (vector) or multiple points (matrix, rows are samples) at which to evaluate\n\nReturns\n\nScalar density value for vector input, or vector of densities for matrix input\n\nNote\n\nThis computes exp(logpdf(density, x)). For numerical stability, prefer using logpdf when possible.\n\n\n\n\n\n","category":"function"},{"location":"api/densities/#Distributions.logpdf","page":"Reference and Target Densities","title":"Distributions.logpdf","text":"logpdf(density::AbstractMapDensity, x)\n\nEvaluate the log-density at point(s) x.\n\nArguments\n\ndensity::AbstractMapDensity: Target or reference density\nx: Point (vector) or multiple points (matrix, rows are samples) at which to evaluate\n\nReturns\n\nScalar log-density value for vector input, or vector of log-densities for matrix input\n\n\n\n\n\n","category":"function"},{"location":"api/densities/#TransportMaps.grad_logpdf","page":"Reference and Target Densities","title":"TransportMaps.grad_logpdf","text":"grad_logpdf(density::AbstractMapDensity, x)\n\nEvaluate the gradient of log-density at point(s) x.\n\nArguments\n\ndensity::AbstractMapDensity: Target or reference density\nx: Point (vector) or multiple points (matrix, rows are samples) at which to evaluate\n\nReturns\n\nGradient vector for vector input, or matrix of gradients (one per row) for matrix input\n\n\n\n\n\n","category":"function"},{"location":"Manuals/conditional_densities/#Conditional-Densities","page":"Conditional Densities and Samples","title":"Conditional Densities","text":"When constructing a transport map, the triangular structure of the Knothe-Rosenblatt rearrangement provides a systematic way to factorize the joint density into a product of conditional densities:\n\npi(boldsymbolx) = pi(x_1) pi(x_2  x_1) pi(x_3  x_1 x_2) cdots pi(x_k  x_1 dots x_k-1)\n\nIn Refs. [1] and [2], the authors show that these conditional densities are obtained sequentially by inverting the map components after one another. We define a transport map as given in Getting Started with TransportMaps.jl.\n\nThe conditional density for x_k given x_1 dots x_k-1 is defined as:\n\npi(x_k  x_1 dots x_k-1) = rho(z_k) left fracpartial T_k(z_k)partial z_k right^-1\n\nwhere z_k is obtained by inverting the first k components of the map, i.e., z_k = T^k(x_1dotsx_k)^-1. Here, rho(z_k) represents the reference density, and T_k is the k-th component of the triangular map.\n\nSimilarly, this allows us to sample from a conditional density pi(x_k  x_1 dots x_k-1) by first inverting the map to get z_k, and then evaluating x_k = T_k(z_1 dots z_k).","category":"section"},{"location":"Manuals/conditional_densities/#Setting-up-a-Transport-Map","page":"Conditional Densities and Samples","title":"Setting up a Transport Map","text":"For this example, we'll use the banana distribution as our target density, which is also used in Getting Started with TransportMaps.jl and Banana: Map from Density. The banana distribution is defined as a:\n\npi(x_1 x_2) = phi(x_1) cdot phi(x_2 - x_1^2)\n\nwhere phi is the standard normal PDF.\n\nWe load the packaged and define the banana density function and create a target density object:\n\nusing TransportMaps\nusing Plots\nusing Distributions\n\nbanana_density(x) = logpdf(Normal(), x[1]) + logpdf(Normal(), x[2] - x[1]^2)\ntarget = MapTargetDensity(banana_density, :auto_diff)\nnothing #hide\n\nDefine the map and quadrature; and optimize the map:\n\nM = PolynomialMap(2, 2, :normal, Softplus(), HermiteBasis())\nquadrature = GaussHermiteWeights(10, 2)\n\n# Optimize the map:\noptimize!(M, target, quadrature)\nnothing #hide","category":"section"},{"location":"Manuals/conditional_densities/#Conditional-Density-Evaluation","page":"Conditional Densities and Samples","title":"Conditional Density Evaluation","text":"Now we can compute conditional densities. For simplicity, we look at a two-dimensional example, so we are interested in the conditional density pi(x_2  x_1 = 05).\n\nDefine the conditioning variable and the variable to evaluate the conditional density:\n\nx₁ = 0.5\nx₂ = 0.8\ndensity = conditional_density(M, x₂, x₁)\nprintln(\"Conditional density π(x₂=$x₂ | x₁=$x₁) = $density\")\n\nEvaluate conditional density for multiple values:\n\nx₂_values = range(-3, 3, length=100)\ndensities = conditional_density(M, x₂_values, x₁)\nnothing #hide\n\nPlot the conditional density:\n\nplot(x₂_values, densities,\n    xlabel=\"x₂\", ylabel=\"π(x₂ | x₁=$x₁)\",\n    title=\"Conditional Density π(x₂ | x₁=$x₁)\",\n    linewidth=2, label=\"Conditional Density\")\nsavefig(\"conditional-density.svg\"); nothing # hide\n\n(Image: Conditional Density)","category":"section"},{"location":"Manuals/conditional_densities/#Conditional-Sampling","page":"Conditional Densities and Samples","title":"Conditional Sampling","text":"We can also sample from the conditional density pi(x_2  x_1). First, we sample z_2 in the standard normal space and then evaluate the conditional map:\n\nSingle value sampling:\n\nz₂ = randn()\ncond_sample = conditional_sample(M, x₁, z₂)\nprintln(\"Conditional sample for z₂=$z₂: x₂=$cond_sample\")\n\nMultiple samples:\n\nz₂_values = randn(10_000)\ncond_samples = conditional_sample(M, x₁, z₂_values)\nnothing #hide\n\nCreate a histogram of the conditional samples and overlay the analytical density:\n\nhistogram(cond_samples, bins=50, normalize=:pdf, alpha=0.7,\n    label=\"Conditional Samples\", xlabel=\"x₂\", ylabel=\"Density\")\nplot!(x₂_values, densities, linewidth=2,\n    label=\"Conditional Density\")\nsavefig(\"conditional-sampling.svg\"); nothing # hide\n\n(Image: Conditional Sampling)","category":"section"},{"location":"Manuals/conditional_densities/#Comparison-with-True-Conditional-Density","page":"Conditional Densities and Samples","title":"Comparison with True Conditional Density","text":"Let's compare our transport map's conditional density with the true conditional density of the target distribution:\n\nTrue conditional density for the banana distribution:\n\nfunction true_banana_conditional_density(x₂, x₁)\n    # For the banana distribution π(x₁, x₂) = N(x₁; 0, 1) * N(x₂ - x₁²; 0, 1)\n    # The conditional density π(x₂|x₁) = N(x₂; x₁², 1)\n    # This is a normal distribution centered at x₁² with variance 1\n    μ_cond = x₁^2\n    σ_cond = 1.0\n    return pdf(Normal(μ_cond, σ_cond), x₂)\nend\nnothing #hide\n\nCompute true conditional densities:\n\ntrue_densities = [true_banana_conditional_density(x₂, x₁) for x₂ in x₂_values]\nnothing #hide\n\nPlot comparison:\n\nplot(x₂_values, densities, linewidth=2, label=\"TM Conditional\",\n    xlabel=\"x₂\", ylabel=\"π(x₂ | x₁=$x₁)\")\nplot!(x₂_values, true_densities, linewidth=2, linestyle=:dash,\n    label=\"True Conditional\")\ntitle!(\"Transport Map vs True Conditional Density\")\nsavefig(\"conditional-comparison.svg\"); nothing # hide\n\n(Image: Conditional Comparison)","category":"section"},{"location":"Manuals/conditional_densities/#Multiple-Conditioning-Scenarios","page":"Conditional Densities and Samples","title":"Multiple Conditioning Scenarios","text":"Let's explore how the conditional density π(x₂ | x₁) changes as we vary x₁. This shows the nonlinear structure of the banana distribution:\n\nx₁_values = [-0.6, 0.0, 1.0, 2.0]\n\np = plot(xlabel=\"x₂\", ylabel=\"π(x₂ | x₁)\",\n    title=\"Conditional Densities for Different x₁ Values\")\n\nfor (i, x₁_val) in enumerate(x₁_values)\n    densities_cond = conditional_density(M, x₂_values, x₁_val)\n    true_densities_cond = [true_banana_conditional_density(x₂, x₁_val) for x₂ in x₂_values]\n\n    plot!(p, x₂_values, densities_cond, linewidth=2,\n        label=\"TM: x₁=$x₁_val\", color=i)\n    plot!(p, x₂_values, true_densities_cond, linewidth=2, linestyle=:dash,\n        label=\"True: x₁=$x₁_val\", color=i)\nend\n\nplot!(p)\nsavefig(\"multiple-conditioning.svg\"); nothing # hide\n\n(Image: Multiple Conditioning)\n\nnote: Note\nFor a more comprehensive example with real-world applications, see the Bayesian Inference: Biochemical Oxygen Demand (BOD) Example.\n\n\n\nThis page was generated using Literate.jl.","category":"section"}]
}
