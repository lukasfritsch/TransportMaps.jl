var documenterSearchIndex = {"docs":
[{"location":"Manuals/quadrature_methods/#Quadrature-Methods","page":"Quadrature Methods","title":"Quadrature Methods","text":"A crucial part of transport-map applications is selecting a suitable quadrature method. Quadrature is used in map optimization, in particular when evaluating the Kullback–Leibler divergence\n\nmathcalD_mathrmKLleft(T_ rho  piright)=int Biglog rho(boldsymbolz)-log pi(T(boldsymbola boldsymbolz))-log operatornamedet nabla T(boldsymbola boldsymbolz) Big rho(boldsymbolz)  mathrmd boldsymbolz\n\nHere, boldsymbolz denotes the variable in the reference space with density rho(boldsymbolz), pi(boldsymbolx) is the target density, and T(boldsymbola boldsymbolz) is the transport map parameterized by boldsymbola.\n\nIn practice we approximate the integral by a quadrature sum:\n\nsum_i=1^N w_qiBig-logpibigl(T(boldsymbolaboldsymbolz_qi)bigr)-log detnabla T(boldsymbolaboldsymbolz_qi) Big\n\nThe quadrature points boldsymbolz_qi and weights w_qi must be chosen so the sum approximates expectations with respect to the reference measure rho(boldsymbolz), which is typically the standard Gaussian.\n\nEspecially in Bayesian inference, where evaluating the target density pi(boldsymbolx) can be expensive, using efficient quadrature methods is important to reduce the number of target evaluations [4].","category":"section"},{"location":"Manuals/quadrature_methods/#Monte-Carlo","page":"Quadrature Methods","title":"Monte Carlo","text":"Monte Carlo estimates an expectation Ef(Z) by sampling Z_i sim mathcalN(0 mathbfI) and forming the sample average. It is simple and robust; weights are uniform. Convergence is stochastic (O(n^-12)) but independent of dimension.\n\nWe start by loading the necessary packages:\n\nusing TransportMaps\nusing Plots\n\nusing Random # hide\nRandom.seed!(42) # hide\nmc = MonteCarloWeights(500, 2)\np_mc = scatter(mc.points[:, 1], mc.points[:, 2], ms=3,\n    label=\"MC samples\", title=\"Monte Carlo (500 pts)\", aspect_ratio=1)\nsavefig(\"quadrature_mc.svg\"); nothing # hide\n\n(Image: Monte Carlo samples)","category":"section"},{"location":"Manuals/quadrature_methods/#Latin-Hypercube-Sampling-(LHS)","page":"Quadrature Methods","title":"Latin Hypercube Sampling (LHS)","text":"Latin Hypercube is a stratified sampling design that improves space-filling over plain Monte Carlo; weights remain uniform. It often reduces variance for low to moderate dimensions.\n\nlhs = LatinHypercubeWeights(500, 2)\np_lhs = scatter(lhs.points[:, 1], lhs.points[:, 2], ms=3,\n    label=\"LHS samples\", title=\"Latin Hypercube (500 pts)\", aspect_ratio=1)\nsavefig(\"quadrature_lhs.svg\"); nothing # hide\n\n(Image: Latin Hypercube samples)","category":"section"},{"location":"Manuals/quadrature_methods/#Tensor-product-Gauss–Hermite","page":"Quadrature Methods","title":"Tensor-product Gauss–Hermite","text":"Gauss–Hermite quadrature provides nodes boldsymbolz_q i and weights w_q i such that for smooth integrands the integral with respect to the Gaussian density is approximated very accurately. Tensor-product rules take the 1D rule in each coordinate and form all combinations, producing N=n^d nodes for n points per dimension.\n\nhermite = GaussHermiteWeights(5, 2)\np_hermite = scatter(hermite.points[:, 1], hermite.points[:, 2],\n    ms=4, label=\"Gauss–Hermite\", title=\"Tensor Gauss–Hermite (5 × 5)\", aspect_ratio=1)\nsavefig(\"quadrature_hermite.svg\"); nothing # hide\n\n(Image: Gauss-Hermite tensor product sample)","category":"section"},{"location":"Manuals/quadrature_methods/#Sparse-Smolyak-Gauss–Hermite","page":"Quadrature Methods","title":"Sparse Smolyak Gauss–Hermite","text":"Sparse Smolyak grids combine 1D quadrature rules across dimensions with carefully chosen coefficients to cancel redundant high-order interactions. The result is substantially fewer nodes than the full tensor product while retaining high accuracy for mixed smooth functions. Note that Smolyak quadrature can produce negative weights; this is expected for higher-order correction terms.\n\nsparse = SparseSmolyakWeights(2, 2)\np_sparse = scatter(sparse.points[:, 1], sparse.points[:, 2], ms=6,\n    label=\"Smolyak\", title=\"Sparse Smolyak (level 2)\", aspect_ratio=1)\nsavefig(\"quadrature_smolyak.svg\"); nothing # hide\n\n(Image: Sparse Smolyak sample)","category":"section"},{"location":"Manuals/quadrature_methods/#Quick-usage-notes","page":"Quadrature Methods","title":"Quick usage notes","text":"Use Monte Carlo or LHS for high-dimensional problems where deterministic quadrature is infeasible.\nUse tensor Gauss–Hermite in very low dimensions when the integrand is smooth and high accuracy is required.\nUse Sparse Smolyak for intermediate dimensions (e.g. d leq 6) to get higher accuracy at much lower cost than the full tensor rule.\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"references/#References","page":"References","title":"References","text":"Y. Marzouk, T. Moselhy, M. Parno and A. Spantini. Sampling via Measure Transport: An Introduction. In: Handbook of Uncertainty Quantification, edited by R. Ghanem, D. Higdon and H. Owhadi (Springer International Publishing, Cham, 2016); pp. 1–41.\n\n\n\nM. Ramgraber, D. Sharp, M. L. Provost and Y. Marzouk. A Friendly Introduction to Triangular Transport (Mar 2025), arXiv:2503.21673 [stat].\n\n\n\nR. Baptista, Y. Marzouk and O. Zahm. On the Representation and Learning of Monotone Triangular Transport Maps. Foundations of Computational Mathematics (2023).\n\n\n\nJ. Grashorn, M. Broggi, L. Chamoin and M. Beer. Efficiency Comparison of MCMC and Transport Map Bayesian Posterior Estimation for Structural Health Monitoring. Mechanical Systems and Signal Processing 216, 111440 (2024).\n\n\n\nM. Rosenblatt. Remarks on a multivariate transformation. The annals of mathematical statistics 23, 470–472 (1952).\n\n\n\nH. Knothe. Contributions to the theory of convex bodies. The Michigan Mathematical Journal 4, 39–52 (1957).\n\n\n\nM. Parno, P.-B. Rubio, D. Sharp, M. Brennan, R. Baptista, H. Bonart and Y. Marzouk. MParT: Monotone Parameterization Toolkit. Journal of Open Source Software 7, 4843 (2022).\n\n\n\nB. Zanger, O. Zahm, T. Cui and M. Schreiber. Sequential Transport Maps Using SoS Density Estimation and alpha``-Divergences (Oct 2024), arXiv:2402.17943 [stat].\n\n\n\nB. Sudret. Global Sensitivity Analysis Using Polynomial Chaos Expansions. Reliability Engineering & System Safety 93, 964–979 (2008).\n\n\n\nD. Xiu and G. E. Karniadakis. The Wiener–Askey Polynomial Chaos for Stochastic Differential Equations. SIAM Journal on Scientific Computing 24, 619–644 (2002).\n\n\n\nN. Lüthen, S. Marelli and B. Sudret. Sparse Polynomial Chaos Expansions: Literature Survey and Benchmark. SIAM/ASA Journal on Uncertainty Quantification 9, 593–649 (2021).\n\n\n\nA. B. Sullivan, D. M. Snyder and S. A. Rounds. Controls on biochemical oxygen demand in the upper Klamath River, Oregon. Chemical Geology 269, 12–21 (2010).\n\n\n\n","category":"section"},{"location":"Manuals/optimization/#Optimization-of-the-Map-Coefficients","page":"Optimization","title":"Optimization of the Map Coefficients","text":"A crucial step in constructing transport maps is the optimization of the map coefficients, which determine how well the map represents the target distribution. This process can be approached in two distinct ways, depending on the available information about the target distribution [1].","category":"section"},{"location":"Manuals/optimization/#Map-from-density","page":"Optimization","title":"Map-from-density","text":"One way to construct a transport map is to directly optimize its parameters based on the (unnormalized) target density, as shown in Banana: Map from Density. This approach requires access to the target density function and uses quadrature schemes to approximate integrals, as introduced in Quadrature Methods.\n\nFormally, we define the following optimization problem to determine the coefficients boldsymbola of the parameterized map T:\n\nmin_boldsymbola sum_i=1^N w_qiBig-logpibigl(T(boldsymbolaboldsymbolz_qi)bigr)-log detnabla T(boldsymbolaboldsymbolz_qi) Big\n\nAs noted by [1], this optimization problem is generally non-convex. Specifically, it is only convex when the target density pi(boldsymbolx) is log-concave. Especially in Bayesian inference, where the target density represents the posterior density, the function is not log-concave, resulting in a non-convex optimization problem.\n\nIn this package, map optimization is performed with the help of Optim.jl, and support a wide range of optimizers and options (such as convergence criteria and printing preferences). Specifically, we can pass our optimize! function the desired optimizer and options. For a full overview of available options, see the Optim.jl configuration documentation.\n\nTo perform the optimization of the map coefficients, we call:\n\noptimize!(M::PolynomialMap, target_density::Function, quadrature::AbstractQuadratureWeights;\n  optimizer::Optim.AbstractOptimizer = LBFGS(), options::Optim.Options = Optim.Options())\n\nWe have to provide the polynomial map M, the target density function, and a quadrature scheme. Optionally, we can specify the optimizer (default is LBFGS()) and options.\n\nnote: Set initial coefficients\nAs the starting point of the optimization, the map coefficients can be set using setcoefficients!(M, coeffs), where coeffs is a vector of coefficients.","category":"section"},{"location":"Manuals/optimization/#Usage","page":"Optimization","title":"Usage","text":"First we load the packages:\n\nusing TransportMaps\nusing Optim\nusing Distributions\nusing Plots\n\nThen, define the target density and quadrature scheme. Here, we use the same banana-shaped density as in Banana: Map from Density:\n\nbanana_density(x) = pdf(Normal(), x[1]) * pdf(Normal(), x[2] - x[1]^2)\ntarget = MapTargetDensity(banana_density, :auto_diff)\nquadrature = GaussHermiteWeights(10, 2)\nnothing #hide\n\nSet optimization options to print the trace every 20 iterations:\n\nopts_trace = Optim.Options(iterations=200, show_trace=true, show_every=20, store_trace=true)\n\nWe will try the following optimizers from Optim.jl, ordered from simplest to most sophisticated:","category":"section"},{"location":"Manuals/optimization/#Gradient-Descent","page":"Optimization","title":"Gradient Descent","text":"The most basic optimization algorithm, Gradient Descent iteratively moves in the direction of the negative gradient. It is simple and robust, but can be slow to converge, especially for ill-conditioned problems.\n\nM_gd = PolynomialMap(2, 2)\nres_gd = optimize!(M_gd, target, quadrature; optimizer=GradientDescent(), options=opts_trace)\nprintln(res_gd)","category":"section"},{"location":"Manuals/optimization/#Conjugate-Gradient","page":"Optimization","title":"Conjugate Gradient","text":"Conjugate Gradient improves upon basic gradient descent by using conjugate directions, which can accelerate convergence for large-scale or quadratic problems. It requires gradient information but not the Hessian.\n\nM_cg = PolynomialMap(2, 2)\nres_cg = optimize!(M_cg, target, quadrature; optimizer=ConjugateGradient(), options=opts_trace)\nprintln(res_cg)","category":"section"},{"location":"Manuals/optimization/#Nelder-Mead","page":"Optimization","title":"Nelder-Mead","text":"Nelder-Mead is a derivative-free optimizer that uses a simplex of points to search for the minimum. It is useful when gradients are unavailable or unreliable, but may be less efficient for high-dimensional or smooth problems.\n\nM_nm = PolynomialMap(2, 2)\nres_nm = optimize!(M_nm, target, quadrature; optimizer=NelderMead(), options=opts_trace)\nprintln(res_nm)","category":"section"},{"location":"Manuals/optimization/#BFGS","page":"Optimization","title":"BFGS","text":"BFGS is a quasi-Newton method that builds up an approximation to the Hessian matrix using gradient evaluations. It is generally faster and more robust than gradient descent and conjugate gradient for smooth problems.\n\nM_bfgs = PolynomialMap(2, 2)\nres_bfgs = optimize!(M_bfgs, target, quadrature; optimizer=BFGS(), options=opts_trace)\nprintln(res_bfgs)","category":"section"},{"location":"Manuals/optimization/#LBFGS","page":"Optimization","title":"LBFGS","text":"LBFGS is a limited-memory version of BFGS, making it suitable for large-scale problems where storing the full Hessian approximation is impractical. It is the default optimizer in many scientific computing packages due to its efficiency and reliability.\n\nM_lbfgs = PolynomialMap(2, 2)\nres_lbfgs = optimize!(M_lbfgs, target, quadrature; optimizer=LBFGS(), options=opts_trace)\nprintln(res_lbfgs)\n\nFinally, we can compare the results by means of variance diagnostic:\n\nsamples_z = randn(1000, 2)\nv_gd = variance_diagnostic(M_gd, target, samples_z)\nv_cg = variance_diagnostic(M_cg, target, samples_z)\nv_nm = variance_diagnostic(M_nm, target, samples_z)\nv_bfgs = variance_diagnostic(M_bfgs, target, samples_z)\nv_lbfgs = variance_diagnostic(M_lbfgs, target, samples_z)\n\nprintln(\"Variance diagnostic GradientDescent:   \", v_gd)\nprintln(\"Variance diagnostic ConjugateGradient: \", v_cg)\nprintln(\"Variance diagnostic NelderMead:        \", v_nm)\nprintln(\"Variance diagnostic BFGS:              \", v_bfgs)\nprintln(\"Variance diagnostic LBFGS:             \", v_lbfgs)\n\nWe can visualize the convergence of all optimizers:\n\nplot([res_gd.trace[i].iteration for i in 1:length(res_gd.trace)], lw=2,\n    [res_gd.trace[i].g_norm for i in 1:length(res_gd.trace)], label=\"GradientDescent\")\nplot!([res_cg.trace[i].iteration for i in 1:length(res_cg.trace)], lw=2,\n    [res_cg.trace[i].g_norm for i in 1:length(res_cg.trace)], label=\"ConjugateGradient\")\nplot!([res_nm.trace[i].iteration for i in 1:length(res_nm.trace)], lw=2,\n    [res_nm.trace[i].g_norm for i in 1:length(res_nm.trace)], label=\"NelderMead\")\nplot!([res_bfgs.trace[i].iteration for i in 1:length(res_bfgs.trace)], lw=2,\n    [res_bfgs.trace[i].g_norm for i in 1:length(res_bfgs.trace)], label=\"BFGS\")\nplot!([res_lbfgs.trace[i].iteration for i in 1:length(res_lbfgs.trace)], lw=2,\n    [res_lbfgs.trace[i].g_norm for i in 1:length(res_lbfgs.trace)], label=\"LBFGS\")\nplot!(xaxis=:log, yaxis=:log, xlabel=\"Iteration\", ylabel=\"Gradient norm\",\n    title=\"Convergence of different optimizers\", xlims=(1, 200),\n    legend=:bottomleft)\nsavefig(\"optimization-conv.svg\"); nothing # hide\n\n(Image: Optimization Convergence)\n\nIt becomes clear, that LBFGS and BFGS are the most efficient optimizers in this case, while Nelder-Mead struggles to keep up.","category":"section"},{"location":"Manuals/optimization/#Map-from-samples","page":"Optimization","title":"Map-from-samples","text":"Another strategy of constructing a transport map is to use samples of the target density, as seen in Banana: Map from Samples. The formulation of transport map estimation in this way has the benefit to transform the problem into a convex optimization problem, when reference density is log-concave [1]. Since we can choose the reference density, we can leverage this property to simplify the optimization process.\n\nWhen the map is constructed from samples, the optimization problem is formulated by minimizing the Kullback-Leibler divergence between the pushforward of the reference density and the empirical distribution of the samples. We denote the transport map by S, which pushes forward the target distribution to the reference distribution. This leads to the following optimization problem:\n\nmin_boldsymbola -frac1M sum_i=1^M log rholeft(S(boldsymbola boldsymbolx_i)right) - log leftdet nabla S(boldsymbola boldsymbolx_i)right\n\nwhere boldsymbolx_i_i=1^M are samples from the target distribution, and rho(cdot) is the density of the reference distribution.\n\nTo perform the optimization, we can use the same optimize! function as before, but now we pass samples instead of a target density and quadrature scheme. Similarly, we can specify the optimizer and options:\n\noptimize!(M::PolynomialMap, samples::AbstractArray{<:Real};\n  optimizer::Optim.AbstractOptimizer = LBFGS(), options::Optim.Options = Optim.Options())\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"Manuals/basis_functions/#Basis-Functions","page":"Basis Functions","title":"Basis Functions","text":"In order to construct a parameterized map, we first need to define a suitable basis.\n\nThis manual describes the different one-dimensional basis families currently implemented in TransportMaps.jl. We focus on variants of the (probabilists') Hermite polynomials and recently proposed edge-controlled versions [3], [2].","category":"section"},{"location":"Manuals/basis_functions/#Probabilistic-Hermite-Basis","page":"Basis Functions","title":"Probabilistic Hermite Basis","text":"The probabilistic Hermite polynomials form an orthonormal basis with respect to the standard normal density\n\nphi(z) = frac1sqrt2 pi expleft(-fracz^22right)\n\nThey satisfy the three-term recurrence operatornameHe_n+1(z)=z operatornameHe_n(z)-n operatornameHe_n-1(z)  with (operatornameHe_0(z)=1) and (operatornameHe_1(z)=z).\n\nOrthonormal polynomial bases such as the Hermite family are useful for several reasons. Orthogonality with respect to a reference measure makes coefficient estimation stable: projections onto the basis are simple inner products and a truncated series gives the best L^2-approximation under that measure. These properties are the same reasons orthogonal polynomials are used in Polynomial Chaos Expansions (PCE) for surrogate modelling and uncertainty quantification (see Sudret [9] for a concise introduction). In the Gaussian case the Hermite polynomials are the natural choice (Wiener–Askey scheme, see [10]).\n\nWe want to visualize the Hermite polynomials, first we load the necessary packages:\n\nusing Distributions\nusing Plots\nusing TransportMaps\n\nThen we construct the basis via HermiteBasis() or HermiteBasis(:none) (:none means no edge control):\n\nbasis = HermiteBasis()\nz = -3:0.01:3\n\np1 = plot(xlabel=\"z\", ylabel=\"Basis function\", title=\"Standard Hermite Basis\")\nfor degree in 0:4\n    plot!(p1, z, map(x -> basisfunction(basis, degree, x), z), label=\"degree $degree\")\nend\nsavefig(\"hermite_basis_standard.svg\"); nothing # hide\n\n(Image: Standard Hermite Basis)\n\nIf we zoom out, we can see that the tails grow quickly for large z:\n\nz = -7:0.1:7\n\np2 = plot(xlabel=\"z\", ylabel=\"Basis function\", title=\"Standard Hermite Basis\")\nfor degree in 0:4\n    plot!(p2, z, map(x -> basisfunction(basis, degree, x), z), label=\"degree $degree\")\nend\nsavefig(\"hermite_basis_standard_zoom.svg\"); nothing # hide\n\n(Image: Standard Hermite Basis)","category":"section"},{"location":"Manuals/basis_functions/#Linearized-Hermite-Basis","page":"Basis Functions","title":"Linearized Hermite Basis","text":"Linearized Hermite polynomials (edge-linearized basis) were introduced in [3] to control growth for large z by replacing the polynomial with a tangent line outside data-dependent bounds z^lz^u:\n\nmathcalH^mathrmLin_j(z)=frac1sqrtZ_alpha_j\nbegincases\nmathrmHe_j(z^l)+mathrmHe_j(z^l)(z-z^l)  z z^l \nmathrmHe_j(z)  z^lle z le z^u \nmathrmHe_j(z^u)+mathrmHe_j(z^u)(z-z^u)  z z^u\nendcases\n\nThe bounds are chosen here as the 0.01 and 0.99 empirical quantiles of (reference) samples. Alternatively, they can be chosen as the quantiles of the respective reference density. The normalization constant Z_alpha_j follows the definition in the paper: Z_alpha_j=alpha_j for jk and Z_alpha_k=(alpha_k+1).\n\nbasis = LinearizedHermiteBasis(Normal(), 4, 1)\nprintln(\"Linearization bounds: \", basis.linearizationbounds)\n\np3 = plot(xlabel=\"z\", ylabel=\"Basis function\", title=\"Linearized Hermite Basis\")\nfor degree in 0:4\n    plot!(p3, z, map(x -> basisfunction(basis, degree, x), z), label=\"degree $degree\")\nend\nsavefig(\"hermite_basis_linearized.svg\"); nothing # hide\n\n(Image: Linearized Hermite Basis)","category":"section"},{"location":"Manuals/basis_functions/#Edge-Controlled-(Weighted)-Hermite-Basis:-Gaussian-Weight","page":"Basis Functions","title":"Edge-Controlled (Weighted) Hermite Basis: Gaussian Weight","text":"Edge control modifies each Hermite polynomial with a decaying weight to reduce growth in the tails [2]. Using a Gaussian weight gives:\n\nmathcalH_j^textGauss(z)=mathrmHe_j(z)expleft(-tfracz^24right)\n\nbasis = GaussianWeightedHermiteBasis()\n\np4 = plot(xlabel=\"z\", ylabel=\"Basis function\", title=\"Gaussian-Weighted Hermite Basis\")\nfor degree in 0:4\n    plot!(p4, z, map(x -> basisfunction(basis, degree, x), z), label=\"degree $degree\")\nend\nsavefig(\"hermite_basis_gaussian.svg\"); nothing # hide\n\n(Image: Gaussian Weighted Hermite Basis)\n\nnote: Note\nIn order to preserve some extrapolation properties, the weights are only applied to polynomials of degree j geq 2, as noted in [2].","category":"section"},{"location":"Manuals/basis_functions/#Edge-Controlled-Hermite-Basis:-Cubic-Spline-Weight","page":"Basis Functions","title":"Edge-Controlled Hermite Basis: Cubic Spline Weight","text":"A cubic spline weight smoothly damps the polynomials outside a radius r, also introduced in [2]. In this implementation, we define r based on the 0.01 and 0.99 quantile values z^l z^u:\n\nmathcalH_j^mathrmCub(z)=operatornameHe_j(z)left(2 u^3-3 u^2+1right)qquad u=minleft(1fraczrright) r=2max(z^lz^u)\n\nbasis = CubicSplineHermiteBasis(Normal())\n\np5 = plot(xlabel=\"z\", ylabel=\"Basis function\", title=\"Cubic Spline Weighted Hermite Basis\")\nfor degree in 0:4\n    plot!(p5, z, map(x -> basisfunction(basis, degree, x), z), label=\"degree $degree\")\nend\nsavefig(\"hermite_basis_cubic.svg\"); nothing # hide\n\n(Image: Cubic Spline Weighted Hermite Basis)","category":"section"},{"location":"Manuals/basis_functions/#Summary","page":"Basis Functions","title":"Summary","text":"We showcased four basis variants:\n\nStandard (orthonormal) Hermite\nLinearized Hermite (piecewise linear tails)\nGaussian-weighted Hermite (exponential damping)\nCubic-spline-weighted Hermite (compact-style smooth damping)\n\nThese can be supplied when constructing polynomial transport maps to tune stability, tail behavior, and sparsity characteristics.\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"Examples/banana_adaptive/#Banana:-Adaptive-Transport-Map-from-Samples","page":"Banana: Adaptive Transport Map from Samples","title":"Banana: Adaptive Transport Map from Samples","text":"This implements the ATM algorithm from [3] to approximate the banana distribution using an adaptive polynomial transport map.\n\nAs an example, we once again consider the banana distribution defined by the density:\n\np(x) = phi(x_1) cdot phi(x_2 - x_1^2)\n\nwhere phi is the standard normal PDF.\n\nusing Random # hide\nRandom.seed!(123) # hide\nnothing # hide\n\nWe start with the necessary packages:\n\nusing TransportMaps\nusing Distributions\nusing LinearAlgebra\nusing Plots","category":"section"},{"location":"Examples/banana_adaptive/#Generating-Target-Samples","page":"Banana: Adaptive Transport Map from Samples","title":"Generating Target Samples","text":"The generation of samples from the banana distribution is done in the same way as in the example Banana: Map from Samples. Here, we use N = 500 samples for the ATM learning.\n\nbanana_density(x) = pdf(Normal(), x[1]) * pdf(Normal(), x[2] - x[1]^2)\n\nnum_samples = 500\n\nfunction generate_banana_samples(n_samples::Int)\n    samples = Matrix{Float64}(undef, n_samples, 2)\n\n    count = 0\n    while count < n_samples\n        x1 = randn() * 2\n        x2 = randn() * 3 + x1^2\n\n        if rand() < banana_density([x1, x2]) / 0.4\n            count += 1\n            samples[count, :] = [x1, x2]\n        end\n    end\n\n    return samples\nend\nnothing #hide\n\nprintln(\"Generating samples from banana distribution...\")\ntarget_samples = generate_banana_samples(num_samples)\nprintln(\"Generated $(size(target_samples, 1)) samples\")","category":"section"},{"location":"Examples/banana_adaptive/#Creating-the-Transport-Map","page":"Banana: Adaptive Transport Map from Samples","title":"Creating the Transport Map","text":"First, create a linear transport map as a starting point. This is also the default behavior of the optimize_adaptive_transportmap function, which standardizes the samples using a linear map based on their marginal mean and standard deviation when no explicit linear map is provided.\n\nL = LinearMap(target_samples)\n\nThen, we perform adaptive transport map learning with k-fold cross-validation. We use the setup as in [3]: We select a Hermite basis with linearization, use the modified Softplus rectifier with parameter beta = 2 and k = 5 folds. We set the maximum number of terms to 3 and 6 for the two components, respectively.\n\nCalling the optimize_adaptive_transportmap function performs the adaptive learning of map terms and returns the learned map, optimization results, selected terms, and selected folds. We can provide the initial linear map L to standardize the samples before learning the ATM, and also specify options, such as the rectifier, basis and options for the optimization process. The function returns the ComposedMap of the linear map and the learned adaptive transport map.\n\nM, results, selected_terms, selected_folds = optimize_adaptive_transportmap(\n    target_samples, [3, 6], 5, L, Softplus(2.))\nnothing # hide\n\nThe adaptive map learning prints out information about the learning process, including the selected terms and the training and test objectives for each fold.\n\nWe see, that in the first component, the ATM selected 2 terms (out of the maximum 3), and in the second component, it selected all 5 out of the maximum 6 terms. The number of selected terms is chosen based on the test objective across the folds.\n\nTo visualized which terms were selected, we can plot the multi-index sets of the learned ATM:\n\nind_atm = getmultiindexsets(M.polynomialmap[2])\n\ndim = scatter(ind_atm[:, 1], ind_atm[:, 2], ms=30, legend=false)\nplot!(xlims=(-0.5, maximum(ind_atm[:, 1]) + 0.5), ylims=(-0.5, maximum(ind_atm[:, 2]) + 0.5),\n    aspect_ratio=1, xlabel=\"Multi-index α₁\", ylabel=\"Multi-index α₂\")\nxticks!(0:maximum(ind_atm[:, 1]))\nyticks!(0:maximum(ind_atm[:, 2]))\nsavefig(\"atm-indices.svg\"); nothing # hide\n\n(Image: Learned Multi-Index) We see that the ATM algorithm selected terms in an L-shape pattern, indicating that lower-order interactions were prioritized.","category":"section"},{"location":"Examples/banana_adaptive/#Testing-the-Map","page":"Banana: Adaptive Transport Map from Samples","title":"Testing the Map","text":"new_samples = generate_banana_samples(1000)\nnorm_samples = randn(1000, 2)\nnothing #hide\n\nmapped_banana_samples = inverse(M, norm_samples)\nnothing #hide","category":"section"},{"location":"Examples/banana_adaptive/#Visualizing-Results","page":"Banana: Adaptive Transport Map from Samples","title":"Visualizing Results","text":"Let's create a scatter plot comparing the original samples with the mapped samples to see how well our transport map learned the distribution:\n\np11 = scatter(new_samples[:, 1], new_samples[:, 2],\n    label=\"Original Samples\", alpha=0.5, color=1,\n    title=\"Original Banana Distribution Samples\",\n    xlabel=\"x₁\", ylabel=\"x₂\")\n\nscatter!(p11, mapped_banana_samples[:, 1], mapped_banana_samples[:, 2],\n    label=\"Mapped Samples\", alpha=0.5, color=2,\n    title=\"Transport Map Generated Samples\",\n    xlabel=\"x₁\", ylabel=\"x₂\")\n\nplot(p11, size=(600, 400))\nsavefig(\"atm-comparison-target.svg\"); nothing # hide\n\n(Image: Sample Comparison)","category":"section"},{"location":"Examples/banana_adaptive/#Density-Comparison","page":"Banana: Adaptive Transport Map from Samples","title":"Density Comparison","text":"Similarly, we can compare the true banana density with the learned density obtained via the pullback of the composed map:\n\nx₁ = range(-3, 3, length=100)\nx₂ = range(-2.5, 4.0, length=100)\n\ntrue_density = [banana_density([x1, x2]) for x2 in x₂, x1 in x₁]\nlearned_density = [pullback(M, [x1, x2]) for x2 in x₂, x1 in x₁]\n\np3 = contour(x₁, x₂, true_density,\n    title=\"True Banana Density\",\n    xlabel=\"x₁\", ylabel=\"x₂\",\n    colormap=:viridis, levels=10)\n\np4 = contour(x₁, x₂, learned_density,\n    title=\"Learned Density (Pullback)\",\n    xlabel=\"x₁\", ylabel=\"x₂\",\n    colormap=:viridis, levels=10)\n\nplot(p3, p4, layout=(1, 2), size=(800, 400))\nsavefig(\"atm-density-comparison.svg\"); nothing # hide\n\n(Image: Density Comparison)","category":"section"},{"location":"Examples/banana_adaptive/#Plot-iterations","page":"Banana: Adaptive Transport Map from Samples","title":"Plot iterations","text":"Additionally, we can visualize the selected terms and optimization objectives over the iterations for the second component of the ATM. First, we retrieve the best fold based on the selected folds during learning:\n\nmap_index = 2  # Choose 2nd component\nbest_fold = selected_folds[map_index]\nres_best = results[map_index][best_fold]\nnothing #hide\n\nThen, we can plot the selected terms at each iteration:\n\nmax_1 = maximum(res_best.terms[end][:, 1])\nmax_2 = maximum(res_best.terms[end][:, 2])\n\np = plot(layout=(2, 3), xlims=(-0.5, max_1 + 0.5), ylims=(-0.5, max_2 + 0.5),\n    aspect_ratio=1, xlabel=\"Multi-index α₁\", ylabel=\"Multi-index α₂\", legend=false,)\n\nfor (i, term) in enumerate(res_best.terms)\n    scatter!(p, term[:, 1], term[:, 2], ms=20, title=\"Iteration $i\", subplot=i)\nend\n\nxticks!(0:max_1)\nyticks!(0:max_2)\nplot!(p, size=(800, 600))\nsavefig(\"iterations.svg\"); nothing # hide\n\n(Image: Iterations)\n\nWe can see that in the last iteration, the interaction term (1 1) was added. However, in the end, this term was not selected in the final map based on the cross-validation. To better understand the selection, we can also plot the training and test objectives over the iterations:\n\nCompare optimization objectives\n\nplot(res_best.train_objectives, label=\"Train Objective\", lw=2, ls=:dash, marker=:o)\nplot!(res_best.test_objectives, label=\"Test Objective\", lw=2, ls=:dash, marker=:o)\nplot!(xlabel=\"Iteration\", ylabel=\"Objective Value\",\n    title=\"Training and Test Objectives over Iterations\")\nplot!(size=(600, 400))\nsavefig(\"objectives.svg\"); nothing # hide\n\n(Image: Objectives)\n\nHere, we see that the test objective increased in the last iteration, indicating overfitting, which is why the last added term was not selected in the final adaptive transport map.\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"Examples/bod_bayesianinference/#Biochemical-Oxygen-Demand-(BOD)-Example","page":"Bayesian Inference: BOD","title":"Biochemical Oxygen Demand (BOD) Example","text":"This example demonstrates Bayesian parameter estimation for a biochemical oxygen demand model using transport maps. The problem comes from environmental engineering and was originally presented in [12] and later used as a benchmark in transport map applications [1].\n\nThe model describes the evolution of biochemical oxygen demand (BOD) in a river system using an exponential growth model with two uncertain parameters controlling growth and decay rates.\n\nusing TransportMaps\nusing Plots\nusing Distributions","category":"section"},{"location":"Examples/bod_bayesianinference/#The-Forward-Model","page":"Bayesian Inference: BOD","title":"The Forward Model","text":"The BOD model is given by:\n\nmathcalB(t) = A(1-exp(-Bt))+ varepsilon\n\nwhere the parameters A and B unknown material parameters and varepsilon sim mathcalN(0 10^-3) represents measurement noise.\n\nThe parameters A and B follow the prior distributions\n\nA sim mathcalU(04 12) quad B sim mathcalU(001 031)\n\nIn order to describe the input parameters in an unbounded space, we transform them into a space with standard normal prior distributions p(theta_i) sim mathcalN(0 1). We write A and B as functions of the new variables theta_1 and theta_2 with the help of a density transformation with the standard normal CDF Phi(theta_i):\n\nbeginaligned\nA = 04 + (12 - 04) cdot Phi(theta_1) \nB = 001 + (031 - 001) cdot Phi(theta_2)\nendaligned\n\nusing Random # hide\nRandom.seed!(123) # hide\nnothing # hide\n\nWe define the forward model as a function of theta and t in Julia:\n\nfunction forward_model(t, θ)\n    A = 0.4 + (1.2 - 0.4) * cdf(Normal(), θ[1])\n    B = 0.01 + (0.31 - 0.01) * cdf(Normal(), θ[2])\n    return A * (1 - exp(-B * t))\nend\nnothing #hide","category":"section"},{"location":"Examples/bod_bayesianinference/#Experimental-Data","page":"Bayesian Inference: BOD","title":"Experimental Data","text":"We have BOD measurements at five time points:\n\nt = [1, 2, 3, 4, 5]\nD = [0.18, 0.32, 0.42, 0.49, 0.54]\nσ = sqrt(1e-3)\nnothing #hide\n\nLet's visualize the data along with model predictions for different parameter values:\n\ns = scatter(t, D, label=\"Data\", xlabel=\"Time (t)\", ylabel=\"Biochemical Oxygen Demand (D)\",\n    size=(600, 400), legend=:topleft)\n# Plot model output for some parameter values\nt_values = range(0, 5, length=100)\nfor θ₁ in [-0.5, 0, 0.5]\n    for θ₂ in [-0.5, 0, 0.5]\n        plot!(t_values, [forward_model(ti, [θ₁, θ₂]) for ti in t_values],\n            label=\"(θ₁ = $θ₁, θ₂ = $θ₂)\", linestyle=:dash)\n    end\nend\nsavefig(\"realizations-bod.svg\"); nothing # hide\n\n(Image: BOD Realizations)","category":"section"},{"location":"Examples/bod_bayesianinference/#Bayesian-Inference-Setup","page":"Bayesian Inference: BOD","title":"Bayesian Inference Setup","text":"We define the posterior distribution using a standard normal prior on both parameters and a Gaussian likelihood for the observations:\n\npi(mathbfyboldsymboltheta) = prod_t=1^5 frac1sqrt2pisigma^2expleft(-frac12sigma^2(y_t - mathcalB(t))^2right)\n\nfunction posterior(θ)\n    # Calculate the likelihood\n    likelihood = prod([pdf(Normal(forward_model(t[k], θ), σ), D[k]) for k in 1:5])\n    # Calculate the prior\n    prior = pdf(Normal(), θ[1]) * pdf(Normal(), θ[2])\n    return prior * likelihood\nend\n\ntarget = MapTargetDensity(posterior, :auto_diff)","category":"section"},{"location":"Examples/bod_bayesianinference/#Creating-and-Optimizing-the-Transport-Map","page":"Bayesian Inference: BOD","title":"Creating and Optimizing the Transport Map","text":"We use a 2-dimensional polynomial transport map with degree 3 and Softplus rectifier:\n\nM = PolynomialMap(2, 3, :normal, Softplus(), LinearizedHermiteBasis())\n\nSet up Gauss-Hermite quadrature for optimization:\n\nquadrature = GaussHermiteWeights(10, 2)\n\nOptimize the map coefficients:\n\nres = optimize!(M, target, quadrature)\nprintln(\"Optimization result: \", res)","category":"section"},{"location":"Examples/bod_bayesianinference/#Generating-Posterior-Samples","page":"Bayesian Inference: BOD","title":"Generating Posterior Samples","text":"Generate samples from the standard normal distribution and map them to the posterior:\n\nsamples_z = randn(1000, 2)\n\nMap the samples through our transport map:\n\nmapped_samples = evaluate(M, samples_z)","category":"section"},{"location":"Examples/bod_bayesianinference/#Quality-Assessment","page":"Bayesian Inference: BOD","title":"Quality Assessment","text":"Compute the variance diagnostic to assess the quality of our approximation:\n\nvar_diag = variance_diagnostic(M, target, samples_z)\nprintln(\"Variance Diagnostic: \", var_diag)","category":"section"},{"location":"Examples/bod_bayesianinference/#Visualization","page":"Bayesian Inference: BOD","title":"Visualization","text":"Plot the mapped samples along with contours of the true posterior density:\n\nθ₁ = range(-0.5, 1.5, length=100)\nθ₂ = range(-0.5, 3, length=100)\n\nposterior_values = [posterior([θ₁, θ₂]) for θ₂ in θ₂, θ₁ in θ₁]\n\nscatter(mapped_samples[:, 1], mapped_samples[:, 2],\n    label=\"Mapped Samples\", alpha=0.5, color=1,\n    xlabel=\"θ₁\", ylabel=\"θ₂\", title=\"Posterior Density and Mapped Samples\")\ncontour!(θ₁, θ₂, posterior_values, colormap=:viridis, label=\"Posterior Density\")\nsavefig(\"samples-bod.svg\"); nothing # hide\n\n(Image: BOD Samples)\n\nFinally, we can compute the pullback density to visualize how well our transport map approximates the posterior:\n\nposterior_pullback = [pullback(M, [θ₁, θ₂]) for θ₂ in θ₂, θ₁ in θ₁]\n\ncontour(θ₁, θ₂, posterior_values ./ maximum(posterior_values);\n    levels=5, colormap=:viridis, colorbar=false,\n    label=\"Target\", xlabel=\"θ₁\", ylabel=\"θ₂\")\ncontour!(θ₁, θ₂, posterior_pullback ./ maximum(posterior_pullback);\n    levels=5, colormap=:viridis, linestyle=:dash,\n    label=\"Pullback\")\nsavefig(\"pullback-bod.svg\"); nothing # hide\n\n(Image: BOD Pullback Density)\n\nWe can also visually observe a good agreement between the true posterior and the TM approximation.","category":"section"},{"location":"Examples/bod_bayesianinference/#Conditional-Density-and-Samples","page":"Bayesian Inference: BOD","title":"Conditional Density and Samples","text":"We can compute conditional densities and generate conditional samples using the transport map. Therefore, we make use of the factorization of the structure of triangular maps given by the Knothe-Rosenblatt rearrangement [1], [2]:\n\npi(mathrmx)=underbracepileft(x_1right)_T_1^-1left(z_1right) underbracepileft(x_2 mid x_1right)_T_2^-1left(z_2  x_1right) underbracepileft(x_3 mid x_1 x_2right)_T_3^-1left(z_3  x_1 x_2right) cdots underbracepileft(x_K mid x_1 ldots x_K-1right)_T_K^-1left(z_K  x_1 ldots x_K-1right)\n\nThis allows us to compute the conditional density pi(theta_2  theta_1) and generate samples from this conditional distribution efficiently. We only need to invert the second component of the map to obtain theta_2 given a fixed value of theta_1 and then push forward samples from the reference distribution.\n\nWe can use the conditional_sample function to generate samples from the conditional distribution. Therefore, we samples from the standard normal distribution for z_2 and push them through the conditional map. We use the previously generated samples for z_2 and fix theta_1.\n\nθ₁ = 0.\nconditional_samples = conditional_sample(M, θ₁, randn(10_000))\nnothing #hide\n\nThen, we compute the conditional density of theta_2 given theta_1 first analytically by integrating out theta_1 from the joint posterior. We use numerical integration for this purpose and evaluate the conditional density on a grid.\n\nθ_range = 0:0.01:2\nint_analytical = gaussquadrature(ξ -> posterior([θ₁, ξ]), 1000, -10., 10.)\nposterior_conditional(θ₂) = posterior([θ₁, θ₂]) / int_analytical\nconditional_analytical = posterior_conditional.(θ_range)\nnothing #hide\n\nThen we compute the conditional density using the transport map, which is given as:\n\npi(theta_2  theta_1) = rho_2left(T^2(theta_1 theta_2)^-1right) leftfracpartial T^2(theta_1 theta_2)^-1partial theta_2right\n\nconditional_mapped = conditional_density(M, θ_range, θ₁)\nnothing #hide\n\nFinally, we plot the results:\n\nhistogram(conditional_samples, bins=50, normalize=:pdf, α=0.5,\n    label=\"Conditional Samples\", xlabel=\"θ₂\", ylabel=\"π(θ₂ | θ₁=$θ₁)\")\nplot!(θ_range, conditional_analytical, lw=2, label=\"Analytical Conditional PDF\")\nplot!(θ_range, conditional_mapped, lw=2, label=\"TM Conditional PDF\")\nsavefig(\"conditional-bod.svg\"); nothing # hide\n\n(Image: BOD Conditional Density)\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"Examples/banana_mapfromsamples/#Banana:-Map-from-Samples","page":"Banana: Map from Samples","title":"Banana: Map from Samples","text":"This example demonstrates how to use TransportMaps.jl to approximate a \"banana\" distribution using polynomial transport maps when only samples from the target distribution are available.\n\nUnlike the density-based approach, this method learns the transport map directly from sample data using optimization techniques. This is particularly useful when the target density is unknown or difficult to evaluate [1].\n\nWe start with the necessary packages:\n\nusing TransportMaps\nusing Distributions\nusing LinearAlgebra\nusing Plots\n\nusing Random # hide\nRandom.seed!(123) # hide\nnothing # hide","category":"section"},{"location":"Examples/banana_mapfromsamples/#Generating-Target-Samples","page":"Banana: Map from Samples","title":"Generating Target Samples","text":"The banana distribution has the density:\n\np(x) = phi(x_1) cdot phi(x_2 - x_1^2)\n\nwhere phi is the standard normal PDF.\n\nbanana_density(x) = pdf(Normal(), x[1]) * pdf(Normal(), x[2] - x[1]^2)\n\nSet up the log-target function for sampling:\n\nnum_samples = 1000\nnothing #hide\n\nGenerate samples using rejection sampling (no external dependencies)\n\nfunction generate_banana_samples(n_samples::Int)\n    samples = Matrix{Float64}(undef, n_samples, 2)\n\n    count = 0\n    while count < n_samples\n        x1 = randn() * 2\n        x2 = randn() * 3 + x1^2\n\n        if rand() < banana_density([x1, x2]) / 0.4\n            count += 1\n            samples[count, :] = [x1, x2]\n        end\n    end\n\n    return samples\nend\n\nprintln(\"Generating samples from banana distribution...\")\ntarget_samples = generate_banana_samples(num_samples)\nprintln(\"Generated $(size(target_samples, 1)) samples\")","category":"section"},{"location":"Examples/banana_mapfromsamples/#Creating-the-Transport-Map","page":"Banana: Map from Samples","title":"Creating the Transport Map","text":"First, we create a linear map to standardize the samples:\n\nL = LinearMap(target_samples)\n\nWe create a 2-dimensional polynomial transport map with degree 2. For sample-based optimization, we typically start with lower degrees and can increase complexity as needed.\n\nM = PolynomialMap(2, 2, :normal, Softplus())","category":"section"},{"location":"Examples/banana_mapfromsamples/#Optimizing-from-Samples","page":"Banana: Map from Samples","title":"Optimizing from Samples","text":"The key difference from density-based optimization is that we optimize directly from the sample data without requiring the density function. Inside the optimization the map is arranged s.t. the \"forward\" direction is from the (unknown) target distribution to the standard normal distribution. Also, we give the linear map to standardize the samples before optimization.\n\nres = optimize!(M, target_samples, L)\nnothing # hide\n\nWe can check the optimization results of the first component:\n\nres.optimization_results[1]\n\nWe can also check the optimization results of the second component:\n\nres.optimization_results[2]\n\nFinally, we construct a composed map that combines the linear and polynomial maps:\n\nC = ComposedMap(L, M)","category":"section"},{"location":"Examples/banana_mapfromsamples/#Testing-the-Map","page":"Banana: Map from Samples","title":"Testing the Map","text":"Let's generate new samples from the banana density and standard normal samples and map them through our optimized transport map to verify the learned distribution:\n\nnew_samples = generate_banana_samples(1000)\nnorm_samples = randn(1000, 2)\nnothing #hide\n\nMap the samples through our transport map. Note that evaluate now transports from reference to target, i.e. mapped_samples should be standard normal samples:\n\nmapped_samples = evaluate(C, new_samples)\nnothing #hide\n\nwhile pushing from the standard normal samples to the target distribution generates new samples from the banana distribution:\n\nmapped_banana_samples = inverse(C, norm_samples)\nnothing #hide","category":"section"},{"location":"Examples/banana_mapfromsamples/#Visualizing-Results","page":"Banana: Map from Samples","title":"Visualizing Results","text":"Let's create a scatter plot comparing the original samples with the mapped samples to see how well our transport map learned the distribution:\n\np11 = scatter(new_samples[:, 1], new_samples[:, 2],\n    label=\"Original Samples\", alpha=0.5, color=1,\n    title=\"Original Banana Distribution Samples\",\n    xlabel=\"x₁\", ylabel=\"x₂\")\n\nscatter!(p11, mapped_banana_samples[:, 1], mapped_banana_samples[:, 2],\n    label=\"Mapped Samples\", alpha=0.5, color=2,\n    title=\"Transport Map Generated Samples\",\n    xlabel=\"x₁\", ylabel=\"x₂\")\n\nplot(p11, size=(600, 400))\nsavefig(\"samples-comparison-target.svg\"); nothing # hide\n\n(Image: Sample Comparison)\n\nand the resulting samples in standard normal space:\n\np12 = scatter(norm_samples[:, 1], norm_samples[:, 2],\n    label=\"Original Samples\", alpha=0.5, color=1,\n    title=\"Original Banana Distribution Samples\",\n    xlabel=\"x₁\", ylabel=\"x₂\")\n\nscatter!(p12, mapped_samples[:, 1], mapped_samples[:, 2],\n    label=\"Mapped Samples\", alpha=0.5, color=2,\n    title=\"Transport Map Generated Samples\",\n    xlabel=\"x₁\", ylabel=\"x₂\")\n\nplot(p12, size=(600, 400), aspect_ratio=1)\nsavefig(\"samples-comparison-reference.svg\"); nothing # hide\n\n(Image: Sample Comparison)","category":"section"},{"location":"Examples/banana_mapfromsamples/#Density-Comparison","page":"Banana: Map from Samples","title":"Density Comparison","text":"We can also compare the learned density (via pullback) with the true density:\n\nx₁ = range(-3, 3, length=100)\nx₂ = range(-2.5, 4.0, length=100)\n\nTrue banana density values:\n\ntrue_density = [banana_density([x1, x2]) for x2 in x₂, x1 in x₁]\nnothing # hide\n\nLearned density via pullback through the transport map. Note that \"pullback\" computes the density of the mapped samples in the standard normal space:\n\nlearned_density = [pullback(C, [x1, x2]) for x2 in x₂, x1 in x₁]\nnothing # hide\n\nCreate contour plots for comparison:\n\np3 = contour(x₁, x₂, true_density,\n    title=\"True Banana Density\",\n    xlabel=\"x₁\", ylabel=\"x₂\",\n    colormap=:viridis, levels=10)\n\np4 = contour(x₁, x₂, learned_density,\n    title=\"Learned Density (Pullback)\",\n    xlabel=\"x₁\", ylabel=\"x₂\",\n    colormap=:viridis, levels=10)\n\nplot(p3, p4, layout=(1, 2), size=(800, 400))\nsavefig(\"density-comparison.svg\"); nothing # hide\n\n(Image: Density Comparison)","category":"section"},{"location":"Examples/banana_mapfromsamples/#Combined-Visualization","page":"Banana: Map from Samples","title":"Combined Visualization","text":"Finally, let's create a combined plot showing both the original samples and the density contours:\n\nscatter(target_samples[:, 1], target_samples[:, 2],\n    label=\"Original Samples\", alpha=0.3, color=1,\n    xlabel=\"x₁\", ylabel=\"x₂\",\n    title=\"Banana Distribution: Samples and Learned Density\")\n\ncontour!(x₁, x₂, learned_density ./ maximum(learned_density),\n    levels=5, colormap=:viridis, alpha=0.8,\n    label=\"Learned Density Contours\")\n\nxlims!(-3, 3)\nylims!(-2.5, 4.0)\nsavefig(\"combined-result.svg\"); nothing # hide\n\n(Image: Combined Result)","category":"section"},{"location":"Examples/banana_mapfromsamples/#Quality-Assessment","page":"Banana: Map from Samples","title":"Quality Assessment","text":"We can assess the quality of our sample-based approximation by comparing statistics of the original and mapped samples:\n\nprintln(\"Sample Statistics Comparison:\")\nprintln(\"Original samples - Mean: \", Distributions.mean(target_samples, dims=1))\nprintln(\"Original samples - Std:  \", Distributions.std(target_samples, dims=1))\nprintln(\"Mapped samples - Mean:   \", Distributions.mean(mapped_banana_samples, dims=1))\nprintln(\"Mapped samples - Std:    \", Distributions.std(mapped_banana_samples, dims=1))","category":"section"},{"location":"Examples/banana_mapfromsamples/#Interpretation","page":"Banana: Map from Samples","title":"Interpretation","text":"The sample-based approach learns the transport map by fitting to the empirical distribution of the samples. This method is particularly useful when:\n\nThe target density is unknown or expensive to evaluate\nOnly sample data is available from experiments or simulations\nThe distribution is complex and difficult to express analytically\n\nThe quality of the approximation depends on:\n\nThe number and quality of the original samples\nThe polynomial degree of the transport map\nThe optimization algorithm and convergence criteria","category":"section"},{"location":"Examples/banana_mapfromsamples/#Further-Experiments","page":"Banana: Map from Samples","title":"Further Experiments","text":"You can experiment with:\n\nDifferent polynomial degrees for more complex distributions\nDifferent rectifier functions (Softplus(), ShiftedELU())\nMore sophisticated MCMC sampling strategies\nCross-validation techniques to assess generalization\nDifferent sample sizes to study convergence behavior\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"Manuals/map_parameterization/#Choosing-a-Map-Parameterization","page":"Map Parameterization","title":"Choosing a Map Parameterization","text":"A crucial aspect of constructing transport maps is the choice of map parameterization. In addition to the choice of the univariate basis functions, the construction of the multi-index set that defines the multi-dimensional polynomial expansion for each component can significantly influence the performance of the map.\n\nThe k-th component of a triangular transport map is defined as T^k mathbbR^k to mathbbR [3]\n\nT^k(z_1 z_2dotsz_k boldsymbola) = f(z_1 z_2dotsz_k-10boldsymbola) + int_0^z_k g(partial_k f(z_1 z_2dotsz_k-1xiboldsymbola))  mathrmd xi\n\nHere, g is a monotone increasing function that ensures the monotonicity of the map, which is a necessary property for transport maps. A common choice for g is the softplus function, defined as g(x) = log(1 + e^x).\n\nFurther, f is a multivariate polynomial function parameterized by coefficients boldsymbola. The polynomial function f can be expressed as a linear combination of multivariate basis functions:\n\nf(z_1 z_2dotsz_k boldsymbola) = sum_alpha in mathcalA_k a_alpha Psi_alpha(z_1 z_2dotsz_k)\n\nHere, Psi_alpha(z_1 z_2dotsz_k) = prod_i=1^k psi_alpha_i(z_i) are multivariate basis functions constructed from univariate basis functions psi_alpha_i(z_i), and mathcalA_k is the multi-index set that determines which polynomial terms are included in the expansion.\n\nBy selecting an appropriate multi-index set mathcalA_k, the structure of the map itself can be tailored to the problem at hand. This includes the ability to create sparse maps, which can be particularly beneficial in high-dimensional settings or when the underlying relationships are known to be simpler. This feature is similar to the concept of sparse polynomial chaos expansions used in (forward) uncertainty propagation [11].\n\nIn TransportMaps.jl, three types of map structures outlined in [1] are implemented:\n\nTotal Order Map: Includes all terms up to a specified total degree p for d dimensions. In order to maintain the triangular structure, the k-th component of the map only depends on the first k variables. Thus, the multivariate basis for the k-th component is constructed from the first k univariate bases. The multi-index set alpha satisfies:\n\nmathcalA_k^TO = boldsymbolalpha boldsymbolalpha_1  leq p  wedge  alpha_i = 0  forall  i  k\n\nNo-Mixed Terms Map: Excludes mixed terms, retaining only pure terms for each variable. The multi-index set alpha satisfies:\n\nmathcalA_k^NM = boldsymbolalpha boldsymbolalpha_1  leq p  wedge  alpha_i alpha_j = 0   forall  i neq j  wedge  alpha_i = 0   forall  i  k\n\nDiagonal Map: Includes only diagonal terms, where each variable is independent of the others. The multi-index set alpha for the k-th component satisfies:\n\nmathcalA_k^D = boldsymbolalpha boldsymbolalpha_1  leq p  wedge  alpha_i = 0  forall  i neq k\n\nFor an better overview, the multi-index sets for each map type will be visualized later.","category":"section"},{"location":"Manuals/map_parameterization/#Constructing-Maps-with-Different-Parameterizations","page":"Map Parameterization","title":"Constructing Maps with Different Parameterizations","text":"We will now demonstrate how to construct and compare the three types of map parameterizations using TransportMaps.jl. We start by importing the necessary packages:\n\nusing TransportMaps\nusing Plots\nusing Distributions\n\nThen we define the three types of (sparse) maps. We consider a two-dimensional map with polynomial degree p=3, softplus as the rectifier function, and Hermite polynomials as the univariate basis functions.","category":"section"},{"location":"Manuals/map_parameterization/#Total-Order-Map","page":"Map Parameterization","title":"Total Order Map","text":"M_to = PolynomialMap(2, 3, :normal, Softplus(), HermiteBasis())\n# alternative: PolynomialMap(2, 3, :normal, Softplus(), HermiteBasis(), :total)","category":"section"},{"location":"Manuals/map_parameterization/#No-mixed-terms-Map","page":"Map Parameterization","title":"No-mixed-terms Map","text":"We can use the NoMixedMap constructor for this map type:\n\nM_nm = NoMixedMap(2, 3, :normal, Softplus(), HermiteBasis())\n# alternative: PolynomialMap(2, 3, :normal, Softplus(), HermiteBasis(), :no_mixed)","category":"section"},{"location":"Manuals/map_parameterization/#Diagonal-Map","page":"Map Parameterization","title":"Diagonal Map","text":"For the diagonal map, we can use the DiagonalMap constructor:\n\nM_d = DiagonalMap(2, 3, :normal, Softplus(), HermiteBasis())\n# alternative: PolynomialMap(2, 3, :normal, Softplus(), HermiteBasis(), :diagonal)","category":"section"},{"location":"Manuals/map_parameterization/#Visualizing-Multi-Index-Sets","page":"Map Parameterization","title":"Visualizing Multi-Index Sets","text":"The multi-index set determines the terms included in the polynomial expansion. We extract the multi-index sets of the second component of each map for visualization:\n\nExtract the multi-index sets of the second component of each map for visualization:\n\nind_to = getmultiindexsets(M_to.components[2])\nind_nm = getmultiindexsets(M_nm.components[2])\nind_d = getmultiindexsets(M_d.components[2])\nnothing #hide\n\nFinally, we plot the multi-index sets for each map type. This allows us to reproduce the comparison of multi-index sets as shown in Figure 1 in [1]:\n\nscatter(ind_to[:, 1], ind_to[:, 2], ms=20, label=\"Total Order\",)\nscatter!(ind_nm[:, 1], ind_nm[:, 2], ms=12, label=\"No Mixed\")\nscatter!(ind_d[:, 1], ind_d[:, 2], ms=6, label=\"Diagonal\")\nscatter!(xlim=(-0.5, 3.5), ylim=(-0.5, 3.5), aspect_ratio=1, legend=:topright,\n    xlabel=\"Multi-index α₁\", ylabel=\"Multi-index α₂\",\n    title=\"Multi-index Set of Map Component M²\")\nsavefig(\"multi_indices.svg\"); nothing # hide\n\n(Image: Multi Index Sets)","category":"section"},{"location":"Manuals/map_parameterization/#Example:-Comparing-Map-Parameterizations","page":"Map Parameterization","title":"Example: Comparing Map Parameterizations","text":"We optimize the coefficients of each map to match a cubic target density. The target density is defined as:\n\np(x) = phi(x_1) cdot phi(x_2 - x_1^3 - x_1^2)\n\nwhere phi is the standard normal PDF.\n\nDefine the cubic target density and visualize it:\n\ncubic_density(x) = pdf(Normal(), x[1]) * pdf(Normal(), x[2] - x[1]^3 - x[1]^2)\n\nx₁ = range(-3, 3, length=1000)\nx₂ = range(-10, 20, length=1000)\n\ntrue_density = [cubic_density([x1, x2]) for x2 in x₂, x1 in x₁]\ncontour(x₁, x₂, true_density;\n    label=\"x₁\", ylabel=\"x₂\", colormap=:viridis, levels=10)\n\nWe create the MapTargetDensity and quadrature weights for optimization:\n\ntarget = MapTargetDensity(cubic_density, :auto_diff)\nquadrature = SparseSmolyakWeights(3, 2)\nnothing #hide\n\nGenerate samples in the standard normal space boldsymbolZ for the variance diagnostic:\n\nsamples_z = randn(1000, 2)\nnothing #hide\n\nNow, we optimize each map type, compute the variance diagnostic and visualize the mapped samples.","category":"section"},{"location":"Manuals/map_parameterization/#Total-Order-Map-2","page":"Map Parameterization","title":"Total Order Map","text":"We start with the total order map:\n\noptimize!(M_to, target, quadrature)\n\nmapped_samples = evaluate(M_to, samples_z)\nvar_diag = variance_diagnostic(M_to, target, samples_z)\nprintln(\"Variance Diagnostic: \", var_diag)\n\nscatter(mapped_samples[:, 1], mapped_samples[:, 2],\n    ms=4, label=nothing, c=1, title=\"Total Order\",\n    xlabel=\"x₁\", ylabel=\"x₂\")\nsavefig(\"total_order.svg\"); nothing # hide\n\n(Image: Total Order)","category":"section"},{"location":"Manuals/map_parameterization/#No-Mixed-Terms-Map","page":"Map Parameterization","title":"No Mixed Terms Map","text":"Next, we optimize the no-mixed-terms map:\n\noptimize!(M_nm, target, quadrature)\n\nmapped_samples_no_mixed = evaluate(M_nm, samples_z)\nvar_diag_no_mixed = variance_diagnostic(M_nm, target, samples_z)\nprintln(\"Variance Diagnostic: \", var_diag_no_mixed)\n\nscatter(mapped_samples_no_mixed[:, 1], mapped_samples_no_mixed[:, 2],\n    ms=4, label=nothing, c=2, title=\"No Mixed\",\n    xlabel=\"x₁\", ylabel=\"x₂\")\nsavefig(\"no_mixed.svg\"); nothing # hide\n\n(Image: No Mixed)","category":"section"},{"location":"Manuals/map_parameterization/#Diagonal-Map-2","page":"Map Parameterization","title":"Diagonal Map","text":"Finally, we optimize the diagonal map:\n\noptimize!(M_d, target, quadrature)\n\nmapped_samples_diagonal = evaluate(M_d, samples_z)\nvar_diag_diagonal = variance_diagnostic(M_d, target, samples_z)\nprintln(\"Variance Diagnostic: \", var_diag_diagonal)\n\nscatter(mapped_samples_diagonal[:, 1], mapped_samples_diagonal[:, 2],\n    ms=4, label=nothing, c=3, title=\"Diagonal\",\n    xlabel=\"x₁\", ylabel=\"x₂\")\nsavefig(\"diagonal.svg\"); nothing # hide\n\n(Image: Diagonal)\n\nWe observe that the total order and no-mixed-terms maps achieve similar variance diagnostics, while the diagonal map performs significantly worse due to its inability to capture dependencies between variables.\n\nnote: Adaptive Map Construction\nThe choice of map parameterization can significantly impact the performance of transport maps. In practice, one might start with a simpler map structure, such as the diagonal or no-mixed-terms map, and then adaptively enrich the map based on the observed performance, as discussed in [3]. This adaptive approach allows for a balance between computational efficiency and approximation accuracy. For more information see the Adaptive Transport Maps manual and [3].\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"api/#API-Reference","page":"API","title":"API Reference","text":"This page provides a comprehensive reference for all exported functions and types in TransportMaps.jl.","category":"section"},{"location":"api/#Transport-Maps","page":"API","title":"Transport Maps","text":"This will be added later.","category":"section"},{"location":"Examples/banana_mapfromdensity/#Banana:-Map-from-Density","page":"Banana: Map from Density","title":"Banana: Map from Density","text":"This example demonstrates how to use TransportMaps.jl to approximate a \"banana\" distribution using polynomial transport maps.\n\nThe banana distribution is a common test case in transport map literature [1], defined as a standard normal in the first dimension and a normal distribution centered at x_1^2 in the second dimension. This example showcases the effectiveness of triangular transport maps for capturing nonlinear dependencies [3].\n\nWe start with the necessary packages:\n\nusing TransportMaps\nusing Distributions\nusing Plots\n\nusing Random # hide\nRandom.seed!(123) # hide\nnothing # hide","category":"section"},{"location":"Examples/banana_mapfromdensity/#Creating-the-Transport-Map","page":"Banana: Map from Density","title":"Creating the Transport Map","text":"We start by creating a 2-dimensional polynomial transport map with degree 2 and a Softplus rectifier function.\n\nM = PolynomialMap(2, 2, Normal(), Softplus())","category":"section"},{"location":"Examples/banana_mapfromdensity/#Setting-up-Quadrature","page":"Banana: Map from Density","title":"Setting up Quadrature","text":"For optimization, we need to specify quadrature weights. Here we use a sparse Smolyak grid with level 2:\n\nquadrature = SparseSmolyakWeights(2, 2)","category":"section"},{"location":"Examples/banana_mapfromdensity/#Defining-the-Target-Density","page":"Banana: Map from Density","title":"Defining the Target Density","text":"The banana distribution has the density:\n\np(x) = phi(x_1) cdot phi(x_2 - x_1^2)\n\nwhere phi is the standard normal PDF.\n\ntarget_density(x) = pdf(Normal(), x[1]) * pdf(Normal(), x[2] - x[1]^2)\nnothing #hide\n\nCreate a MapTargetDensity object for optimization\n\ntarget = MapTargetDensity(target_density, :auto_diff)","category":"section"},{"location":"Examples/banana_mapfromdensity/#Optimizing-the-Map","page":"Banana: Map from Density","title":"Optimizing the Map","text":"Now we optimize the map coefficients to approximate the target density:\n\nres = optimize!(M, target, quadrature)\nprintln(\"Optimization result: \", res)","category":"section"},{"location":"Examples/banana_mapfromdensity/#Testing-the-Map","page":"Banana: Map from Density","title":"Testing the Map","text":"Let's generate some samples from the standard normal distribution and map them through our optimized transport map:\n\nsamples_z = randn(1000, 2)\n\nMap the samples through our transport map:\n\nmapped_samples = evaluate(M, samples_z)","category":"section"},{"location":"Examples/banana_mapfromdensity/#Visualizing-Results","page":"Banana: Map from Density","title":"Visualizing Results","text":"Let's create a scatter plot of the mapped samples to see how well our transport map approximates the banana distribution:\n\nscatter(mapped_samples[:, 1], mapped_samples[:, 2],\n    label=\"Mapped Samples\", alpha=0.5, color=2,\n    title=\"Transport Map Approximation of Banana Distribution\",\n    xlabel=\"x₁\", ylabel=\"x₂\")\nsavefig(\"samples-banana.svg\"); nothing # hide\n\n(Image: Banana Samples)","category":"section"},{"location":"Examples/banana_mapfromdensity/#Quality-Assessment","page":"Banana: Map from Density","title":"Quality Assessment","text":"We can assess the quality of our approximation using the variance diagnostic:\n\nvar_diag = variance_diagnostic(M, target, samples_z)\nprintln(\"Variance Diagnostic: \", var_diag)","category":"section"},{"location":"Examples/banana_mapfromdensity/#Interpretation","page":"Banana: Map from Density","title":"Interpretation","text":"The variance diagnostic provides a measure of how well the transport map approximates the target distribution. Lower values indicate better approximation.\n\nThe scatter plot should show the characteristic \"banana\" shape, with samples curved according to the relationship x_2 propto x_1^2.","category":"section"},{"location":"Examples/banana_mapfromdensity/#Further-Experiments","page":"Banana: Map from Density","title":"Further Experiments","text":"You can experiment with:\n\nDifferent polynomial degrees (see [3] for monotone map theory)\nDifferent rectifier functions (IdentityRectifier(), ShiftedELU())\nDifferent quadrature methods (MonteCarloWeights, LatinHypercubeWeights, GaussHermiteWeights)\nMore quadrature points for higher accuracy\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"Manuals/adaptive_transport_map/#Adaptive-Transport-Maps","page":"Adaptive Transport Maps","title":"Adaptive Transport Maps","text":"A key challenge in constructing transport maps is choosing the appropriate parameterization, specifically the multi-index set that defines which polynomial terms to include in the expansion. While fixed parameterizations like total order, no-mixed terms, or diagonal maps (see Choosing a Map Parameterization) can work well in many cases, they may not be optimal for all target distributions.\n\nAdaptive transport maps address this limitation by automatically selecting the most relevant polynomial terms through a greedy enrichment strategy. This approach is particularly useful when:\n\nThe structure of the target distribution is unknown a priori\nComputational resources are limited and a sparse representation is desired\nHigh-dimensional problems require careful selection of interaction terms","category":"section"},{"location":"Manuals/adaptive_transport_map/#Theory","page":"Adaptive Transport Maps","title":"Theory","text":"The adaptive transport map (ATM) algorithm was introduced by [3] and provides a principled approach to construct sparse, triangular transport maps by adaptively enriching the multi-index set based on gradient information.","category":"section"},{"location":"Manuals/adaptive_transport_map/#Greedy-Multi-Index-Selection","page":"Adaptive Transport Maps","title":"Greedy Multi-Index Selection","text":"Given a triangular transport map with components T^k mathbbR^k to mathbbR for k=1ldotsd, each component is parameterized by a polynomial expansion:\n\nT^k(z_1 ldots z_k boldsymbola) = f(z_1 ldots z_k-1 0 boldsymbola) + int_0^z_k gleft(partial_k f(z_1 ldots z_k-1 xi boldsymbola)right) dxi\n\nwhere f is a multivariate polynomial:\n\nf(z_1 ldots z_k boldsymbola) = sum_alpha in Lambda_k a_alpha Psi_alpha(z_1 ldots z_k)\n\nHere, Lambda_k is the multi-index set that determines which terms are included.\n\nThe ATM algorithm starts with a minimal multi-index set (typically containing only the constant term) and iteratively adds terms that maximize the improvement in the objective function. At each iteration t, given the current multi-index set Lambda_t, the algorithm:\n\nIdentifies candidate terms from the reduced margin of Lambda_t:\n\nmathcalRM(Lambda_t) = alpha notin Lambda_t  alpha - e_i in Lambda_t text for all  i text with  alpha_i  0\n\nwhere e_i is the i-th standard basis vector.\n\nFor each candidate alpha in mathcalRM(Lambda_t), evaluates the gradient of the objective with respect to the coefficient a_alpha (initialized to zero).\nSelects the candidate with the largest absolute gradient value:\n\nalpha^+ = argmax_alpha in mathcalRM(Lambda_t) leftfracpartial Jpartial a_alpharight\n\nUpdates the multi-index set: Lambda_t+1 = Lambda_t cup alpha^+ and optimizes all coefficients.\n\nThis greedy selection strategy ensures that at each iteration, the term most likely to improve the objective function is added, leading to sparse and efficient representations.","category":"section"},{"location":"Manuals/adaptive_transport_map/#Cross-Validation-for-Model-Selection","page":"Adaptive Transport Maps","title":"Cross-Validation for Model Selection","text":"A critical question when using adaptive transport maps is: how many terms should be included? Including too few terms may result in underfitting, while including too many can lead to overfitting.\n\nTo address this, the ATM implementation supports k-fold cross-validation. The algorithm:\n\nSplits the data into k folds\nFor each fold, trains the map on k-1 folds and validates on the remaining fold\nTracks both training and validation objectives at each iteration\nSelects the number of terms that minimizes the average validation objective across folds\n\nThis approach provides a data-driven way to balance model complexity and generalization performance.","category":"section"},{"location":"Manuals/adaptive_transport_map/#Usage-in-TransportMaps.jl","page":"Adaptive Transport Maps","title":"Usage in TransportMaps.jl","text":"The optimize_adaptive_transportmap function provides two main interfaces for constructing adaptive transport maps.\n\nThe simplest approach uses a fixed train-test split to monitor overfitting:\n\nM, histories = optimize_adaptive_transportmap(\n    samples,             # Matrix of samples (n_samples × d)\n    maxterms,            # Vector of maximum terms per component\n    lm,                  # Linear map for standardization (default: LinearMap(samples))\n    rectifier,           # Rectifier function (default: Softplus())\n    basis;               # Polynomial basis (default: LinearizedHermiteBasis())\n    optimizer = LBFGS(),\n    options = Optim.Options(),\n    test_fraction = 0.2  # Fraction of data for validation\n)\n\nFor automatic model selection, use k-fold cross-validation. This implementation is based on the original algorithm proposed in [3]:\n\nM, fold_histories, selected_terms, selected_folds = optimize_adaptive_transportmap(\n    samples,             # Matrix of samples (n_samples × d)\n    maxterms,            # Vector of maximum terms per component\n    k_folds,             # Number of folds for cross-validation\n    lm,                  # Linear map for standardization (default: LinearMap(samples))\n    rectifier,           # Rectifier function (default: Softplus())\n    basis;               # Polynomial basis (default: LinearizedHermiteBasis())\n    optimizer = LBFGS(),\n    options = Optim.Options()\n)\n\nThe k-fold version returns:\n\nM: The final composed transport map trained on all data with the selected number of terms\nfold_histories: Optimization histories for each component and fold\nselected_terms: Number of terms selected for each component based on cross-validation\nselected_folds: Which fold had the best performance for each component\n\nexample: Example\nThe usage is demonstrated in the example Banana: Adaptive Transport Map from Samples.","category":"section"},{"location":"Manuals/adaptive_transport_map/#References","page":"Adaptive Transport Maps","title":"References","text":"The implementation of adaptive transport maps is based on the work by Baptista et al.:\n\nBaptista, R., Marzouk, Y., & Zahm, O. (2023). On the Representation and Learning of Monotone Triangular Transport Maps. Foundations of Computational Mathematics. https://doi.org/10.1007/s10208-023-09630-x\nMatlab implementation of the original ATM algorithm: https://github.com/baptistar/ATM\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"Manuals/getting_started/#Getting-Started-with-TransportMaps.jl","page":"Getting Started","title":"Getting Started with TransportMaps.jl","text":"This guide will help you get started with TransportMaps.jl for constructing and using transport maps.","category":"section"},{"location":"Manuals/getting_started/#Basic-Concepts","page":"Getting Started","title":"Basic Concepts","text":"","category":"section"},{"location":"Manuals/getting_started/#What-is-a-Transport-Map?","page":"Getting Started","title":"What is a Transport Map?","text":"A transport map T boldsymbolZ mapsto boldsymbolX is a mapping from reference space boldsymbolZ sim rho(boldsymbolz) to the target space boldsymbolX sim pi(boldsymbolx) [1]. Hence, the inverse map T^-1 boldsymbolX mapsto boldsymbolZ maps from the target to the reference space.","category":"section"},{"location":"Manuals/getting_started/#Triangular-Maps","page":"Getting Started","title":"Triangular Maps","text":"TransportMaps.jl focuses on triangular transport maps [3], following the Knothe-Rosenblatt rearrangement [6]. This structure ensures that the map is invertible and the Jacobian determinant is easy to compute. A triangular map in n dimensions has the form:\n\nT(boldsymbolz) =\nleft(beginarrayc\nT_1(z_1) \nT_2(z_1 z_2) \nT_3(z_1 z_2 z_3) \nvdots \nT_n(z_1 z_2 dots z_n)\nendarray\nright)\n\nThe inverse map T^-1 can be computed sequentially by inverting each component.","category":"section"},{"location":"Manuals/getting_started/#First-Example:-A-Simple-2D-Transport-Map","page":"Getting Started","title":"First Example: A Simple 2D Transport Map","text":"using TransportMaps\nusing Distributions\nusing Random\nusing Plots\nusing LinearAlgebra\n\nLet's create a simple 2D transport map:\n\nSet random seed for reproducibility\n\nRandom.seed!(1234)\nnothing #hide\n\nCreate a 2D polynomial map with degree 2\n\nM = PolynomialMap(2, 2, Normal(), Softplus())\n\nThe map is initially identity (coefficients are zero)\n\nprintln(\"Initial coefficients: \", getcoefficients(M))","category":"section"},{"location":"Manuals/getting_started/#Defining-a-Target-Distribution","page":"Getting Started","title":"Defining a Target Distribution","text":"For optimization, you need to define your target probability density. Let's start with a simple correlated Gaussian:\n\nExample: Correlated Gaussian\n\nfunction correlated_gaussian(x; ρ=0.8)\n    Σ = [1.0 ρ; ρ 1.0]\n    return pdf(MvNormal(zeros(2), Σ), x)\nend\nnothing #hide\n\nCreate a MapTargetDensity object for optimization\n\ntarget_density = MapTargetDensity(correlated_gaussian, :auto_diff)","category":"section"},{"location":"Manuals/getting_started/#Setting-up-Quadrature","page":"Getting Started","title":"Setting up Quadrature","text":"Choose an appropriate quadrature scheme for map optimization:\n\nGauss-Hermite quadrature (good for Gaussian-like targets)\n\nquadrature = GaussHermiteWeights(5, 2)  # 5 points per dimension, 2D\n# alternative options:\n# quadrature = MonteCarloWeights(1000, 2)  # 1000 samples, 2D\n# quadrature = LatinHypercubeWeights(1000, 2)\n# quadrature = SparseSmolyakWeights(3, 2)  # Level 3, 2D","category":"section"},{"location":"Manuals/getting_started/#Optimizing-the-Map","page":"Getting Started","title":"Optimizing the Map","text":"Fit the transport map to your target distribution:\n\nresult = optimize!(M, target_density, quadrature)","category":"section"},{"location":"Manuals/getting_started/#Generating-Samples","page":"Getting Started","title":"Generating Samples","text":"Once optimized, use the map to generate samples:\n\nGenerate reference samples (standard Gaussian)\n\nn_samples = 1000\nreference_samples = randn(n_samples, 2)\n\nTransform to target distribution\n\ntarget_samples = evaluate(M, reference_samples)","category":"section"},{"location":"Manuals/getting_started/#Visualizing-Results","page":"Getting Started","title":"Visualizing Results","text":"Let's plot both the reference and target samples:\n\np1 = scatter(reference_samples[:, 1], reference_samples[:, 2],\n    alpha=0.6, title=\"Reference Samples\",\n    xlabel=\"Z₁\", ylabel=\"Z₂\", legend=false, aspect_ratio=:equal)\n\np2 = scatter(target_samples[:, 1], target_samples[:, 2],\n    alpha=0.6, title=\"Target Samples\",\n    xlabel=\"X₁\", ylabel=\"X₂\", legend=false, aspect_ratio=:equal)\n\nplot(p1, p2, layout=(1, 2), size=(800, 400))\nsavefig(\"samples.svg\"); nothing # hide\n\n(Image: Transport Map Samples)","category":"section"},{"location":"Manuals/getting_started/#Evaluating-Map-Quality","page":"Getting Started","title":"Evaluating Map Quality","text":"Check how well your map approximates the target:\n\nVariance diagnostic (should be close to 1 for good maps)\n\nvar_diag = variance_diagnostic(M, target_density, reference_samples)\nprintln(\"Variance diagnostic: \", var_diag)\n\nYou can also check the Jacobian determinant\n\nsample_point = [0.0, 0.0]\njac = jacobian(M, sample_point)\ndet_jac = det(jac)\nprintln(\"Jacobian determinant at origin: \", det_jac)","category":"section"},{"location":"Manuals/getting_started/#Working-with-Different-Rectifiers","page":"Getting Started","title":"Working with Different Rectifiers","text":"The rectifier function affects the map's behavior. Let's compare different options:\n\nShiftedELU rectifier\n\nM_elu = PolynomialMap(2, 2, Normal(), ShiftedELU())\nresult_elu = optimize!(M_elu, target_density, quadrature)\nvar_diag_elu = variance_diagnostic(M_elu, target_density, reference_samples)\n\nprintln(\"Variance diagnostics:\")\nprintln(\"  Softplus: \", var_diag)\nprintln(\"  ShiftedELU: \", var_diag_elu)","category":"section"},{"location":"Manuals/getting_started/#More-Complex-Example:-Banana-Distribution","page":"Getting Started","title":"More Complex Example: Banana Distribution","text":"Now let's try a more challenging target - the banana distribution:\n\nDefine banana density\n\nbanana_density(x) = pdf(Normal(), x[1]) * pdf(Normal(), x[2] - x[1]^2)\ntarget_density_banana = MapTargetDensity(banana_density, :auto_diff)\n\nCreate a new map for this target and optimize:\n\nM_banana = PolynomialMap(2, 2, Normal(), Softplus())\nresult_banana = optimize!(M_banana, target_density_banana, quadrature)\n\nGenerate samples\n\nbanana_samples = evaluate(M_banana, reference_samples)\n\nVisualize the banana distribution\n\nx1_grid = range(-3, 3, length=100)\nx2_grid = range(-3, 6, length=100)\nposterior_values = [banana_density([x₁, x₂]) for x₂ in x2_grid, x₁ in x1_grid]\n\nscatter(banana_samples[:, 1], banana_samples[:, 2],\n    alpha=0.6, title=\"Banana Distribution Samples\",\n    xlabel=\"X₁\", ylabel=\"X₂\", legend=false, aspect_ratio=:equal)\ncontour!(x1_grid, x2_grid, posterior_values, colormap=:viridis, label=\"Posterior Density\")\nsavefig(\"banana_samples.svg\"); nothing # hide\n\n(Image: Banana Distribution Samples)\n\nCheck quality\n\nvar_diag_banana = variance_diagnostic(M_banana, target_density_banana, reference_samples)\nprintln(\"Banana distribution variance diagnostic: \", var_diag_banana)\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"#TransportMaps.jl","page":"Home","title":"TransportMaps.jl","text":"This is an implementation of triangular transport maps in Julia based on the description in [1]. For a comprehensive introduction to transport maps, see [2]. The theoretical foundations for monotone triangular transport maps are detailed in [1], [3]. For practical applications in structural health monitoring and Bayesian inference, see [4].","category":"section"},{"location":"#What-are-Transport-Maps?","page":"Home","title":"What are Transport Maps?","text":"Transport maps are smooth, invertible functions that can transform one probability distribution into another [1]. The mathematical foundation builds on the Rosenblatt transformation [5] and the Knothe-Rosenblatt rearrangement [6]. They are particularly useful for:\n\nSampling: Generate samples from complex distributions by transforming samples from simple distributions\nVariational inference: Approximate complex posterior distributions in Bayesian updating problems [4]\nDensity estimation: Learn the structure of complex probability distributions","category":"section"},{"location":"#Key-Features","page":"Home","title":"Key Features","text":"Polynomial Maps: Triangular polynomial transport maps\nAdaptive Construction: Automatic selection of polynomial terms for efficient approximation\nMultiple Rectifiers: Support for different activation functions (Softplus, ShiftedELU, Identity)\nQuadrature Integration: Multiple quadrature schemes for map optimization\nOptimization: Built-in optimization routines for fitting maps to target densities\nMultithreaded evaluation for processing multiple points efficiently","category":"section"},{"location":"#Installation","page":"Home","title":"Installation","text":"using Pkg\nPkg.add(\"https://github.com/lukasfritsch/TransportMaps.jl\")","category":"section"},{"location":"#Quick-Start-Example","page":"Home","title":"Quick Start Example","text":"Here's a simple example showing how to construct a transport map for a \"banana\" distribution:\n\nusing TransportMaps\nusing Distributions\n\n# Create a 2D polynomial map with degree 2 and Softplus rectifier\nM = PolynomialMap(2, 2, Normal(), Softplus())\n\n# Set up quadrature for optimization\nquadrature = GaussHermiteWeights(3, 2)\n\n# Define target density (banana distribution)\ntarget_density(x) = pdf(Normal(), x[1]) * pdf(Normal(), x[2] - x[1]^2)\ntarget = MapTargetDensity(target_density, :auto_diff)\n\n# Optimize the map coefficients\nresult = optimize!(M, target, quadrature)\n\n# Generate samples by mapping standard Gaussian samples\nsamples_z = randn(1000, 2)\n# Matrix input automatically uses multithreading for better performance\nmapped_samples = evaluate(M, samples_z)\n\n# Evaluate map quality\nvariance_diag = variance_diagnostic(M, target, samples_z)","category":"section"},{"location":"#Package-Architecture","page":"Home","title":"Package Architecture","text":"The package is organized around several key components:","category":"section"},{"location":"#Map-Components","page":"Home","title":"Map Components","text":"PolynomialMapComponent: Individual polynomial components of triangular maps\nHermiteBasis: Hermite polynomial basis functions\nMultivariateBasis: Multivariate polynomial basis construction","category":"section"},{"location":"#Transport-Maps","page":"Home","title":"Transport Maps","text":"PolynomialMap: Main triangular polynomial transport map implementation","category":"section"},{"location":"#Rectifier-Functions","page":"Home","title":"Rectifier Functions","text":"IdentityRectifier: No transformation (linear)\nSoftplus: Smooth positive transformation\nShiftedELU: Exponential linear unit variant","category":"section"},{"location":"#Quadrature-and-Optimization","page":"Home","title":"Quadrature and Optimization","text":"GaussHermiteWeights: Gauss-Hermite quadrature points and weights\nMonteCarloWeights: Monte Carlo integration\nLatinHypercubeWeights: Latin hypercube sampling","category":"section"},{"location":"#API-Reference","page":"Home","title":"API Reference","text":"Pages = [\"api.md\"]","category":"section"},{"location":"#Authors","page":"Home","title":"Authors","text":"Lukas Fritsch, Institute for Risk and Reliability, Leibniz University Hannover\nJan Grashorn, Chair for Engineering Materials and Building Preservation, Helmut-Schmidt-University Hamburg","category":"section"},{"location":"#Related-Implementation","page":"Home","title":"Related Implementation","text":"ATM: Matlab code for adaptive transport maps [3]\nMParT: C++-based library for transport maps [7]\nTransportBasedInference.jl: Julia implementation of adaptive transport maps (ATM) and Kalman filters\nSequentialMeasureTransport.jl: Julia implementation of transport maps from sum-of-squares densities [8]\nTriangular-Transport-Toolbox: Python code for the triangular transport tutorial paper [2]","category":"section"},{"location":"Manuals/conditional_densities/#Conditional-Densities","page":"Conditional Densities and Samples","title":"Conditional Densities","text":"When constructing a transport map, the triangular structure of the Knothe-Rosenblatt rearrangement provides a systematic way to factorize the joint density into a product of conditional densities:\n\npi(boldsymbolx) = pi(x_1) pi(x_2  x_1) pi(x_3  x_1 x_2) cdots pi(x_k  x_1 dots x_k-1)\n\nIn Refs. [1] and [2], the authors show that these conditional densities are obtained sequentially by inverting the map components after one another. We define a transport map as given in Getting Started with TransportMaps.jl.\n\nThe conditional density for x_k given x_1 dots x_k-1 is defined as:\n\npi(x_k  x_1 dots x_k-1) = rho(z_k) left fracpartial T_k(z_k)partial z_k right^-1\n\nwhere z_k is obtained by inverting the first k components of the map, i.e., z_k = T^k(x_1dotsx_k)^-1. Here, rho(z_k) represents the reference density, and T_k is the k-th component of the triangular map.\n\nSimilarly, this allows us to sample from a conditional density pi(x_k  x_1 dots x_k-1) by first inverting the map to get z_k, and then evaluating x_k = T_k(z_1 dots z_k).","category":"section"},{"location":"Manuals/conditional_densities/#Setting-up-a-Transport-Map","page":"Conditional Densities and Samples","title":"Setting up a Transport Map","text":"For this example, we'll use the banana distribution as our target density, which is also used in Getting Started with TransportMaps.jl and Banana: Map from Density. The banana distribution is defined as a:\n\npi(x_1 x_2) = phi(x_1) cdot phi(x_2 - x_1^2)\n\nwhere phi is the standard normal PDF.\n\nWe load the packaged and define the banana density function and create a target density object:\n\nusing TransportMaps\nusing Plots\nusing Distributions\n\nbanana_density(x) = pdf(Normal(), x[1]) * pdf(Normal(), x[2] - x[1]^2)\ntarget = MapTargetDensity(banana_density, :auto_diff)\nnothing #hide\n\nDefine the map and quadrature; and optimize the map:\n\nM = PolynomialMap(2, 2, :normal, Softplus(), HermiteBasis())\nquadrature = GaussHermiteWeights(10, 2)\n\n# Optimize the map:\noptimize!(M, target, quadrature)\nnothing #hide","category":"section"},{"location":"Manuals/conditional_densities/#Conditional-Density-Evaluation","page":"Conditional Densities and Samples","title":"Conditional Density Evaluation","text":"Now we can compute conditional densities. For simplicity, we look at a two-dimensional example, so we are interested in the conditional density pi(x_2  x_1 = 05).\n\nDefine the conditioning variable and the variable to evaluate the conditional density:\n\nx₁ = 0.5\nx₂ = 0.8\ndensity = conditional_density(M, x₂, x₁)\nprintln(\"Conditional density π(x₂=$x₂ | x₁=$x₁) = $density\")\n\nEvaluate conditional density for multiple values:\n\nx₂_values = range(-3, 3, length=100)\ndensities = conditional_density(M, x₂_values, x₁)\nnothing #hide\n\nPlot the conditional density:\n\nplot(x₂_values, densities,\n    xlabel=\"x₂\", ylabel=\"π(x₂ | x₁=$x₁)\",\n    title=\"Conditional Density π(x₂ | x₁=$x₁)\",\n    linewidth=2, label=\"Conditional Density\")\nsavefig(\"conditional-density.svg\"); nothing # hide\n\n(Image: Conditional Density)","category":"section"},{"location":"Manuals/conditional_densities/#Conditional-Sampling","page":"Conditional Densities and Samples","title":"Conditional Sampling","text":"We can also sample from the conditional density pi(x_2  x_1). First, we sample z_2 in the standard normal space and then evaluate the conditional map:\n\nSingle value sampling:\n\nz₂ = randn()\ncond_sample = conditional_sample(M, x₁, z₂)\nprintln(\"Conditional sample for z₂=$z₂: x₂=$cond_sample\")\n\nMultiple samples:\n\nz₂_values = randn(10_000)\ncond_samples = conditional_sample(M, x₁, z₂_values)\nnothing #hide\n\nCreate a histogram of the conditional samples and overlay the analytical density:\n\nhistogram(cond_samples, bins=50, normalize=:pdf, alpha=0.7,\n    label=\"Conditional Samples\", xlabel=\"x₂\", ylabel=\"Density\")\nplot!(x₂_values, densities, linewidth=2,\n    label=\"Conditional Density\")\nsavefig(\"conditional-sampling.svg\"); nothing # hide\n\n(Image: Conditional Sampling)","category":"section"},{"location":"Manuals/conditional_densities/#Comparison-with-True-Conditional-Density","page":"Conditional Densities and Samples","title":"Comparison with True Conditional Density","text":"Let's compare our transport map's conditional density with the true conditional density of the target distribution:\n\nTrue conditional density for the banana distribution:\n\nfunction true_banana_conditional_density(x₂, x₁)\n    # For the banana distribution π(x₁, x₂) = N(x₁; 0, 1) * N(x₂ - x₁²; 0, 1)\n    # The conditional density π(x₂|x₁) = N(x₂; x₁², 1)\n    # This is a normal distribution centered at x₁² with variance 1\n    μ_cond = x₁^2\n    σ_cond = 1.0\n    return pdf(Normal(μ_cond, σ_cond), x₂)\nend\nnothing #hide\n\nCompute true conditional densities:\n\ntrue_densities = [true_banana_conditional_density(x₂, x₁) for x₂ in x₂_values]\nnothing #hide\n\nPlot comparison:\n\nplot(x₂_values, densities, linewidth=2, label=\"TM Conditional\",\n    xlabel=\"x₂\", ylabel=\"π(x₂ | x₁=$x₁)\")\nplot!(x₂_values, true_densities, linewidth=2, linestyle=:dash,\n    label=\"True Conditional\")\ntitle!(\"Transport Map vs True Conditional Density\")\nsavefig(\"conditional-comparison.svg\"); nothing # hide\n\n(Image: Conditional Comparison)","category":"section"},{"location":"Manuals/conditional_densities/#Multiple-Conditioning-Scenarios","page":"Conditional Densities and Samples","title":"Multiple Conditioning Scenarios","text":"Let's explore how the conditional density π(x₂ | x₁) changes as we vary x₁. This shows the nonlinear structure of the banana distribution:\n\nx₁_values = [-0.6, 0.0, 1.0, 2.0]\n\np = plot(xlabel=\"x₂\", ylabel=\"π(x₂ | x₁)\",\n    title=\"Conditional Densities for Different x₁ Values\")\n\nfor (i, x₁_val) in enumerate(x₁_values)\n    densities_cond = conditional_density(M, x₂_values, x₁_val)\n    true_densities_cond = [true_banana_conditional_density(x₂, x₁_val) for x₂ in x₂_values]\n\n    plot!(p, x₂_values, densities_cond, linewidth=2,\n        label=\"TM: x₁=$x₁_val\", color=i)\n    plot!(p, x₂_values, true_densities_cond, linewidth=2, linestyle=:dash,\n        label=\"True: x₁=$x₁_val\", color=i)\nend\n\nplot!(p)\nsavefig(\"multiple-conditioning.svg\"); nothing # hide\n\n(Image: Multiple Conditioning)\n\nnote: Note\nFor a more comprehensive example with real-world applications, see the Biochemical Oxygen Demand (BOD) Example.\n\n\n\nThis page was generated using Literate.jl.","category":"section"}]
}
